{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eG8T3V8lxqE",
        "outputId": "d6422029-411a-4e57-8ddd-ab87175d5194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (2.179.0)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymupdf4llm\n",
            "  Downloading pymupdf4llm-0.0.27-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Collecting pymupdf>=1.26.3 (from pymupdf4llm)\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.8.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf4llm-0.0.27-py3-none-any.whl (30 kB)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib pdfplumber pymupdf4llm tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiFm3MgRnZsj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx0szEJWl-nR"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf4llm pdfplumber pandas requests\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start the ollama serve process in the background\n",
        "# We redirect its output to a log file to keep our notebook clean\n",
        "server_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    stdout=open(\"ollama_server.log\", \"w\"),\n",
        "    stderr=subprocess.STDOUT\n",
        ")\n",
        "\n",
        "print(\"✅ Ollama server started in the background.\")\n",
        "print(\"You can now run other cells!\")\n",
        "\n",
        "# Give the server a few seconds to start up before you run other commands\n",
        "time.sleep(5)\n",
        "! ollama pull llama3.2:3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERILxBEPBXPd",
        "outputId": "b13dc24d-8015-49d0-c0b4-f67015a50c95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 PDF Text Extractor for Pharmaceutical Documents\n",
            "======================================================================\n",
            "🔍 Looking for PDF files in: drive/MyDrive/pdf\n",
            "📁 Found 341 PDF files to process\n",
            "======================================================================\n",
            "\n",
            "📄 Processing: bula_1755192077396.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 12,011 characters\n",
            "📊 Found 40 headers and 40 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755192077396_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755192097944.pdf\n",
            "✅ Extracted 16 pages using pdfplumber\n",
            "📝 Extracted 47,853 characters\n",
            "📊 Found 86 headers and 86 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755192097944_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195358088.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 29,326 characters\n",
            "📊 Found 45 headers and 45 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195358088_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195361693.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 22,964 characters\n",
            "📊 Found 96 headers and 96 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195361693_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195365369.pdf\n",
            "✅ Extracted 20 pages using pdfplumber\n",
            "📝 Extracted 50,502 characters\n",
            "📊 Found 143 headers and 143 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195365369_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195368936.pdf\n",
            "✅ Extracted 19 pages using pdfplumber\n",
            "📝 Extracted 51,676 characters\n",
            "📊 Found 219 headers and 219 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195368936_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195372611.pdf\n",
            "✅ Extracted 22 pages using pdfplumber\n",
            "📝 Extracted 76,653 characters\n",
            "📊 Found 230 headers and 230 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195372611_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195376229.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 40,082 characters\n",
            "📊 Found 178 headers and 178 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195376229_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195379863.pdf\n",
            "✅ Extracted 26 pages using pdfplumber\n",
            "📝 Extracted 105,360 characters\n",
            "📊 Found 304 headers and 304 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195379863_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195383507.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 16,817 characters\n",
            "📊 Found 32 headers and 32 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195383507_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195387070.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 23,547 characters\n",
            "📊 Found 38 headers and 38 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195387070_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195390739.pdf\n",
            "✅ Extracted 24 pages using pdfplumber\n",
            "📝 Extracted 35,085 characters\n",
            "📊 Found 161 headers and 161 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195390739_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195394453.pdf\n",
            "✅ Extracted 29 pages using pdfplumber\n",
            "📝 Extracted 45,991 characters\n",
            "📊 Found 147 headers and 147 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195394453_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195397987.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 13,958 characters\n",
            "📊 Found 52 headers and 52 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195397987_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195401687.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 21,611 characters\n",
            "📊 Found 52 headers and 52 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195401687_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195405310.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 39,078 characters\n",
            "📊 Found 131 headers and 131 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195405310_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195409003.pdf\n",
            "✅ Extracted 27 pages using pdfplumber\n",
            "📝 Extracted 73,495 characters\n",
            "📊 Found 180 headers and 180 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195409003_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195412736.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 19,010 characters\n",
            "📊 Found 51 headers and 51 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195412736_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195416341.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 24,181 characters\n",
            "📊 Found 68 headers and 68 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195416341_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195419989.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 17,388 characters\n",
            "📊 Found 48 headers and 48 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195419989_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195423699.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 15,122 characters\n",
            "📊 Found 46 headers and 46 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195423699_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195729672.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 23,859 characters\n",
            "📊 Found 44 headers and 44 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195729672_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195733362.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 29,326 characters\n",
            "📊 Found 45 headers and 45 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195733362_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195737054.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 22,964 characters\n",
            "📊 Found 96 headers and 96 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195737054_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195740597.pdf\n",
            "✅ Extracted 20 pages using pdfplumber\n",
            "📝 Extracted 50,502 characters\n",
            "📊 Found 143 headers and 143 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195740597_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195744229.pdf\n",
            "✅ Extracted 19 pages using pdfplumber\n",
            "📝 Extracted 51,676 characters\n",
            "📊 Found 219 headers and 219 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195744229_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195747817.pdf\n",
            "✅ Extracted 22 pages using pdfplumber\n",
            "📝 Extracted 76,653 characters\n",
            "📊 Found 230 headers and 230 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195747817_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195751498.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 40,082 characters\n",
            "📊 Found 178 headers and 178 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195751498_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195755219.pdf\n",
            "✅ Extracted 26 pages using pdfplumber\n",
            "📝 Extracted 105,360 characters\n",
            "📊 Found 304 headers and 304 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195755219_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195758941.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 16,817 characters\n",
            "📊 Found 32 headers and 32 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195758941_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195762579.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 23,547 characters\n",
            "📊 Found 38 headers and 38 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195762579_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195766330.pdf\n",
            "✅ Extracted 24 pages using pdfplumber\n",
            "📝 Extracted 35,085 characters\n",
            "📊 Found 161 headers and 161 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195766330_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195769913.pdf\n",
            "✅ Extracted 29 pages using pdfplumber\n",
            "📝 Extracted 45,991 characters\n",
            "📊 Found 147 headers and 147 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195769913_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195773534.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 13,958 characters\n",
            "📊 Found 52 headers and 52 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195773534_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195777223.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 21,611 characters\n",
            "📊 Found 52 headers and 52 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195777223_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195780956.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 39,078 characters\n",
            "📊 Found 131 headers and 131 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195780956_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195784592.pdf\n",
            "✅ Extracted 27 pages using pdfplumber\n",
            "📝 Extracted 73,495 characters\n",
            "📊 Found 180 headers and 180 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195784592_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195788408.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 19,010 characters\n",
            "📊 Found 51 headers and 51 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195788408_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195791970.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 24,181 characters\n",
            "📊 Found 68 headers and 68 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195791970_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195795622.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 17,388 characters\n",
            "📊 Found 48 headers and 48 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195795622_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195799334.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 15,122 characters\n",
            "📊 Found 46 headers and 46 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195799334_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195808231.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 18,446 characters\n",
            "📊 Found 41 headers and 41 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195808231_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195812155.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 17,373 characters\n",
            "📊 Found 39 headers and 39 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195812155_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195815637.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 18,646 characters\n",
            "📊 Found 54 headers and 54 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195815637_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195819271.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 27,850 characters\n",
            "📊 Found 60 headers and 60 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195819271_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195822983.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 19,438 characters\n",
            "📊 Found 72 headers and 72 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195822983_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195826652.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 28,523 characters\n",
            "📊 Found 74 headers and 74 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195826652_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195830424.pdf\n",
            "✅ Extracted 19 pages using pdfplumber\n",
            "📝 Extracted 36,160 characters\n",
            "📊 Found 192 headers and 192 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195830424_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195834023.pdf\n",
            "✅ Extracted 19 pages using pdfplumber\n",
            "📝 Extracted 36,388 characters\n",
            "📊 Found 216 headers and 216 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195834023_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195837662.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 24,800 characters\n",
            "📊 Found 54 headers and 54 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195837662_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195841330.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 37,991 characters\n",
            "📊 Found 61 headers and 61 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195841330_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195844947.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 31,532 characters\n",
            "📊 Found 67 headers and 67 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195844947_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195848661.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 41,387 characters\n",
            "📊 Found 88 headers and 88 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195848661_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195852295.pdf\n",
            "✅ Extracted 45 pages using pdfplumber\n",
            "📝 Extracted 72,678 characters\n",
            "📊 Found 164 headers and 164 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195852295_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195856038.pdf\n",
            "✅ Extracted 99 pages using pdfplumber\n",
            "📝 Extracted 275,477 characters\n",
            "📊 Found 330 headers and 330 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195856038_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195859710.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 12,787 characters\n",
            "📊 Found 70 headers and 70 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195859710_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195863386.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 12,131 characters\n",
            "📊 Found 71 headers and 71 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195863386_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195866995.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 21,296 characters\n",
            "📊 Found 43 headers and 43 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195866995_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195870698.pdf\n",
            "✅ Extracted 14 pages using pdfplumber\n",
            "📝 Extracted 42,087 characters\n",
            "📊 Found 55 headers and 55 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195870698_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195874373.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 14,998 characters\n",
            "📊 Found 73 headers and 73 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195874373_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195878000.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 17,791 characters\n",
            "📊 Found 73 headers and 73 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195878000_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195886813.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 9,309 characters\n",
            "📊 Found 42 headers and 42 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195886813_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195890561.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 8,817 characters\n",
            "📊 Found 45 headers and 45 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195890561_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195894025.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 21,617 characters\n",
            "📊 Found 79 headers and 79 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195894025_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195897725.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 20,054 characters\n",
            "📊 Found 79 headers and 79 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195897725_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195901363.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 16,137 characters\n",
            "📊 Found 66 headers and 66 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195901363_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195905053.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 35,000 characters\n",
            "📊 Found 79 headers and 79 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195905053_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195908569.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 30,334 characters\n",
            "📊 Found 118 headers and 118 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195908569_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195912244.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 44,656 characters\n",
            "📊 Found 152 headers and 152 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195912244_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195915842.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 17,436 characters\n",
            "📊 Found 41 headers and 41 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195915842_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195919502.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 18,714 characters\n",
            "📊 Found 42 headers and 42 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195919502_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195923154.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 18,876 characters\n",
            "📊 Found 89 headers and 89 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195923154_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195926888.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 13,760 characters\n",
            "📊 Found 54 headers and 54 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195926888_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195930639.pdf\n",
            "✅ Extracted 23 pages using pdfplumber\n",
            "📝 Extracted 44,248 characters\n",
            "📊 Found 216 headers and 216 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195930639_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195934295.pdf\n",
            "✅ Extracted 19 pages using pdfplumber\n",
            "📝 Extracted 42,778 characters\n",
            "📊 Found 173 headers and 173 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195934295_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195937988.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 8,752 characters\n",
            "📊 Found 44 headers and 44 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195937988_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195941678.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 14,867 characters\n",
            "📊 Found 57 headers and 57 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195941678_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195945432.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 12,217 characters\n",
            "📊 Found 33 headers and 33 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195945432_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195948972.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 11,047 characters\n",
            "📊 Found 37 headers and 37 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195948972_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195952790.pdf\n",
            "✅ Extracted 72 pages using pdfplumber\n",
            "📝 Extracted 146,688 characters\n",
            "📊 Found 403 headers and 403 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195952790_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195956681.pdf\n",
            "✅ Extracted 93 pages using pdfplumber\n",
            "📝 Extracted 218,913 characters\n",
            "📊 Found 381 headers and 381 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195956681_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195965312.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 17,303 characters\n",
            "📊 Found 56 headers and 56 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195965312_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195968986.pdf\n",
            "✅ Extracted 19 pages using pdfplumber\n",
            "📝 Extracted 40,373 characters\n",
            "📊 Found 71 headers and 71 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195968986_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195972626.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 15,731 characters\n",
            "📊 Found 50 headers and 50 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195972626_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195976304.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 20,402 characters\n",
            "📊 Found 64 headers and 64 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195976304_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195979910.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 6,838 characters\n",
            "📊 Found 27 headers and 27 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195979910_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195983635.pdf\n",
            "✅ Extracted 4 pages using pdfplumber\n",
            "📝 Extracted 6,439 characters\n",
            "📊 Found 28 headers and 28 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195983635_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195987374.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 10,535 characters\n",
            "📊 Found 49 headers and 49 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195987374_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195991000.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 10,537 characters\n",
            "📊 Found 50 headers and 50 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195991000_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195994605.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 20,272 characters\n",
            "📊 Found 131 headers and 131 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195994605_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755195998304.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 55,881 characters\n",
            "📊 Found 124 headers and 124 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755195998304_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196001985.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 10,877 characters\n",
            "📊 Found 81 headers and 81 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196001985_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196005660.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 14,087 characters\n",
            "📊 Found 86 headers and 86 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196005660_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196009360.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 11,430 characters\n",
            "📊 Found 52 headers and 52 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196009360_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196012987.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 14,705 characters\n",
            "📊 Found 65 headers and 65 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196012987_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196016741.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 14,887 characters\n",
            "📊 Found 75 headers and 75 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196016741_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196020390.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 33,836 characters\n",
            "📊 Found 73 headers and 73 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196020390_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196024048.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 38,421 characters\n",
            "📊 Found 105 headers and 105 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196024048_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196027815.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 49,042 characters\n",
            "📊 Found 121 headers and 121 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196027815_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196031350.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 7,546 characters\n",
            "📊 Found 35 headers and 35 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196031350_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196035021.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 9,710 characters\n",
            "📊 Found 42 headers and 42 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196035021_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196043875.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 17,298 characters\n",
            "📊 Found 102 headers and 102 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196043875_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196047675.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 19,337 characters\n",
            "📊 Found 78 headers and 78 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196047675_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196051279.pdf\n",
            "✅ Extracted 14 pages using pdfplumber\n",
            "📝 Extracted 23,875 characters\n",
            "📊 Found 150 headers and 150 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196051279_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196054931.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 19,024 characters\n",
            "📊 Found 119 headers and 119 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196054931_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196058631.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 11,806 characters\n",
            "📊 Found 43 headers and 43 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196058631_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196062299.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 13,185 characters\n",
            "📊 Found 52 headers and 52 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196062299_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196065865.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 9,503 characters\n",
            "📊 Found 26 headers and 26 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196065865_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196069590.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 12,879 characters\n",
            "📊 Found 26 headers and 26 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196069590_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196073282.pdf\n",
            "✅ Extracted 14 pages using pdfplumber\n",
            "📝 Extracted 27,350 characters\n",
            "📊 Found 48 headers and 48 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196073282_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196076997.pdf\n",
            "✅ Extracted 21 pages using pdfplumber\n",
            "📝 Extracted 52,727 characters\n",
            "📊 Found 108 headers and 108 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196076997_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196080658.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 16,974 characters\n",
            "📊 Found 124 headers and 124 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196080658_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196084343.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 21,094 characters\n",
            "📊 Found 130 headers and 130 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196084343_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196088146.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 14,868 characters\n",
            "📊 Found 97 headers and 97 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196088146_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196091743.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 19,025 characters\n",
            "📊 Found 97 headers and 97 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196091743_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196095366.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 11,669 characters\n",
            "📊 Found 39 headers and 39 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196095366_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196098984.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 10,887 characters\n",
            "📊 Found 39 headers and 39 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196098984_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196102797.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 19,581 characters\n",
            "📊 Found 33 headers and 33 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196102797_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196106479.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 29,566 characters\n",
            "📊 Found 41 headers and 41 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196106479_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196110283.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 8,688 characters\n",
            "📊 Found 35 headers and 35 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196110283_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196113823.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 14,195 characters\n",
            "📊 Found 41 headers and 41 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196113823_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196122723.pdf\n",
            "✅ Extracted 32 pages using pdfplumber\n",
            "📝 Extracted 67,986 characters\n",
            "📊 Found 96 headers and 96 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196122723_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196126404.pdf\n",
            "✅ Extracted 38 pages using pdfplumber\n",
            "📝 Extracted 111,076 characters\n",
            "📊 Found 133 headers and 133 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196126404_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196129954.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 21,624 characters\n",
            "📊 Found 48 headers and 48 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196129954_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196133671.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 41,140 characters\n",
            "📊 Found 80 headers and 80 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196133671_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196137272.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 16,912 characters\n",
            "📊 Found 66 headers and 66 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196137272_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196141026.pdf\n",
            "✅ Extracted 16 pages using pdfplumber\n",
            "📝 Extracted 44,639 characters\n",
            "📊 Found 96 headers and 96 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196141026_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196144730.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 13,199 characters\n",
            "📊 Found 40 headers and 40 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196144730_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196148379.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 15,353 characters\n",
            "📊 Found 43 headers and 43 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196148379_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196151976.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 18,866 characters\n",
            "📊 Found 37 headers and 37 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196151976_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196155652.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 23,226 characters\n",
            "📊 Found 45 headers and 45 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196155652_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196159516.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 34,238 characters\n",
            "📊 Found 170 headers and 170 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196159516_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196163027.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 44,350 characters\n",
            "📊 Found 150 headers and 150 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196163027_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196166653.pdf\n",
            "✅ Extracted 17 pages using pdfplumber\n",
            "📝 Extracted 43,564 characters\n",
            "📊 Found 126 headers and 126 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196166653_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196170467.pdf\n",
            "✅ Extracted 23 pages using pdfplumber\n",
            "📝 Extracted 69,580 characters\n",
            "📊 Found 152 headers and 152 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196170467_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196173988.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 19,866 characters\n",
            "📊 Found 96 headers and 96 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196173988_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196177684.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 43,127 characters\n",
            "📊 Found 150 headers and 150 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196177684_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196181317.pdf\n",
            "✅ Extracted 14 pages using pdfplumber\n",
            "📝 Extracted 32,194 characters\n",
            "📊 Found 94 headers and 94 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196181317_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196184984.pdf\n",
            "✅ Extracted 19 pages using pdfplumber\n",
            "📝 Extracted 46,787 characters\n",
            "📊 Found 107 headers and 107 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196184984_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196188627.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 13,193 characters\n",
            "📊 Found 40 headers and 40 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196188627_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196192396.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 18,949 characters\n",
            "📊 Found 42 headers and 42 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196192396_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196201270.pdf\n",
            "✅ Extracted 26 pages using pdfplumber\n",
            "📝 Extracted 50,056 characters\n",
            "📊 Found 320 headers and 320 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196201270_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196204919.pdf\n",
            "✅ Extracted 29 pages using pdfplumber\n",
            "📝 Extracted 94,549 characters\n",
            "📊 Found 341 headers and 341 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196204919_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196208612.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 18,956 characters\n",
            "📊 Found 74 headers and 74 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196208612_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196212172.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 25,330 characters\n",
            "📊 Found 69 headers and 69 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196212172_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196215981.pdf\n",
            "✅ Extracted 14 pages using pdfplumber\n",
            "📝 Extracted 31,854 characters\n",
            "📊 Found 100 headers and 100 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196215981_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196219569.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 40,390 characters\n",
            "📊 Found 106 headers and 106 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196219569_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196223668.pdf\n",
            "✅ Extracted 25 pages using pdfplumber\n",
            "📝 Extracted 39,265 characters\n",
            "📊 Found 247 headers and 247 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196223668_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196227271.pdf\n",
            "✅ Extracted 35 pages using pdfplumber\n",
            "📝 Extracted 60,721 characters\n",
            "📊 Found 327 headers and 327 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196227271_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196230592.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 15,198 characters\n",
            "📊 Found 75 headers and 75 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196230592_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196234198.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 20,455 characters\n",
            "📊 Found 78 headers and 78 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196234198_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196237909.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 19,058 characters\n",
            "📊 Found 81 headers and 81 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196237909_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196241738.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 31,376 characters\n",
            "📊 Found 86 headers and 86 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196241738_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196245408.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 29,031 characters\n",
            "📊 Found 104 headers and 104 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196245408_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196249093.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 31,662 characters\n",
            "📊 Found 115 headers and 115 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196249093_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196252780.pdf\n",
            "✅ Extracted 16 pages using pdfplumber\n",
            "📝 Extracted 32,247 characters\n",
            "📊 Found 218 headers and 218 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196252780_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196256382.pdf\n",
            "✅ Extracted 17 pages using pdfplumber\n",
            "📝 Extracted 46,850 characters\n",
            "📊 Found 235 headers and 235 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196256382_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196260206.pdf\n",
            "✅ Extracted 16 pages using pdfplumber\n",
            "📝 Extracted 35,657 characters\n",
            "📊 Found 167 headers and 167 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196260206_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196263840.pdf\n",
            "✅ Extracted 16 pages using pdfplumber\n",
            "📝 Extracted 43,114 characters\n",
            "📊 Found 172 headers and 172 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196263840_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196267531.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 22,430 characters\n",
            "📊 Found 62 headers and 62 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196267531_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196271163.pdf\n",
            "✅ Extracted 14 pages using pdfplumber\n",
            "📝 Extracted 30,731 characters\n",
            "📊 Found 61 headers and 61 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196271163_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196279812.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 14,326 characters\n",
            "📊 Found 43 headers and 43 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196279812_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196283532.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 15,683 characters\n",
            "📊 Found 48 headers and 48 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196283532_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196287099.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 14,427 characters\n",
            "📊 Found 38 headers and 38 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196287099_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196290776.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 15,834 characters\n",
            "📊 Found 39 headers and 39 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196290776_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196294506.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 19,343 characters\n",
            "📊 Found 49 headers and 49 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196294506_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196298172.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 23,075 characters\n",
            "📊 Found 63 headers and 63 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196298172_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196301898.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 10,392 characters\n",
            "📊 Found 31 headers and 31 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196301898_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196305631.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 10,761 characters\n",
            "📊 Found 35 headers and 35 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196305631_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196309404.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 31,078 characters\n",
            "📊 Found 78 headers and 78 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196309404_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196313093.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 42,278 characters\n",
            "📊 Found 98 headers and 98 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196313093_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196316664.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 25,247 characters\n",
            "📊 Found 88 headers and 88 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196316664_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196320331.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 34,594 characters\n",
            "📊 Found 105 headers and 105 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196320331_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196324039.pdf\n",
            "✅ Extracted 28 pages using pdfplumber\n",
            "📝 Extracted 42,516 characters\n",
            "📊 Found 168 headers and 168 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196324039_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196327830.pdf\n",
            "✅ Extracted 42 pages using pdfplumber\n",
            "📝 Extracted 86,377 characters\n",
            "📊 Found 200 headers and 200 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196327830_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196331322.pdf\n",
            "✅ Extracted 4 pages using pdfplumber\n",
            "📝 Extracted 7,802 characters\n",
            "📊 Found 26 headers and 26 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196331322_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196335027.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 9,514 characters\n",
            "📊 Found 25 headers and 25 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196335027_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196339051.pdf\n",
            "✅ Extracted 65 pages using pdfplumber\n",
            "📝 Extracted 125,224 characters\n",
            "📊 Found 396 headers and 396 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196339051_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196342715.pdf\n",
            "✅ Extracted 89 pages using pdfplumber\n",
            "📝 Extracted 225,440 characters\n",
            "📊 Found 497 headers and 497 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196342715_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196346143.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 21,467 characters\n",
            "📊 Found 86 headers and 86 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196346143_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196350010.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 39,145 characters\n",
            "📊 Found 104 headers and 104 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196350010_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196358774.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 11,480 characters\n",
            "📊 Found 37 headers and 37 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196358774_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196362442.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 10,309 characters\n",
            "📊 Found 35 headers and 35 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196362442_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196365994.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 8,823 characters\n",
            "📊 Found 32 headers and 32 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196365994_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196369782.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 8,192 characters\n",
            "📊 Found 28 headers and 28 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196369782_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196373486.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 12,853 characters\n",
            "📊 Found 35 headers and 35 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196373486_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196377131.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 16,364 characters\n",
            "📊 Found 50 headers and 50 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196377131_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196380909.pdf\n",
            "✅ Extracted 23 pages using pdfplumber\n",
            "📝 Extracted 61,981 characters\n",
            "📊 Found 168 headers and 168 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196380909_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196384608.pdf\n",
            "✅ Extracted 29 pages using pdfplumber\n",
            "📝 Extracted 118,125 characters\n",
            "📊 Found 230 headers and 230 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196384608_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196388199.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 20,679 characters\n",
            "📊 Found 64 headers and 64 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196388199_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196391884.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 39,692 characters\n",
            "📊 Found 87 headers and 87 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196391884_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196395558.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 25,746 characters\n",
            "📊 Found 145 headers and 145 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196395558_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196399356.pdf\n",
            "✅ Extracted 14 pages using pdfplumber\n",
            "📝 Extracted 42,543 characters\n",
            "📊 Found 135 headers and 135 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196399356_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196402907.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 18,576 characters\n",
            "📊 Found 62 headers and 62 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196402907_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196406557.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 22,943 characters\n",
            "📊 Found 63 headers and 63 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196406557_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196410324.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 11,713 characters\n",
            "📊 Found 49 headers and 49 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196410324_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196413998.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 19,451 characters\n",
            "📊 Found 63 headers and 63 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196413998_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196417548.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 7,886 characters\n",
            "📊 Found 32 headers and 32 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196417548_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196421359.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 8,736 characters\n",
            "📊 Found 36 headers and 36 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196421359_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196425030.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 23,109 characters\n",
            "📊 Found 147 headers and 147 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196425030_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196428746.pdf\n",
            "✅ Extracted 28 pages using pdfplumber\n",
            "📝 Extracted 92,134 characters\n",
            "📊 Found 161 headers and 161 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196428746_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196437624.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 21,107 characters\n",
            "📊 Found 45 headers and 45 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196437624_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196441295.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 19,785 characters\n",
            "📊 Found 50 headers and 50 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196441295_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196444893.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 24,834 characters\n",
            "📊 Found 52 headers and 52 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196444893_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196448688.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 28,100 characters\n",
            "📊 Found 53 headers and 53 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196448688_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196452326.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 30,981 characters\n",
            "📊 Found 68 headers and 68 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196452326_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196456181.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 42,103 characters\n",
            "📊 Found 89 headers and 89 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196456181_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196459872.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 27,369 characters\n",
            "📊 Found 56 headers and 56 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196459872_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196463599.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 34,698 characters\n",
            "📊 Found 71 headers and 71 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196463599_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196467138.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 9,980 characters\n",
            "📊 Found 48 headers and 48 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196467138_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196470812.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 13,538 characters\n",
            "📊 Found 54 headers and 54 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196470812_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196474503.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 9,249 characters\n",
            "📊 Found 36 headers and 36 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196474503_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196478580.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 15,453 characters\n",
            "📊 Found 44 headers and 44 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196478580_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196482116.pdf\n",
            "✅ Extracted 4 pages using pdfplumber\n",
            "📝 Extracted 5,356 characters\n",
            "📊 Found 27 headers and 27 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196482116_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196485658.pdf\n",
            "✅ Extracted 4 pages using pdfplumber\n",
            "📝 Extracted 5,356 characters\n",
            "📊 Found 27 headers and 27 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196485658_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196489361.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 10,752 characters\n",
            "📊 Found 44 headers and 44 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196489361_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196493091.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 10,752 characters\n",
            "📊 Found 44 headers and 44 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196493091_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196496898.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 14,189 characters\n",
            "📊 Found 29 headers and 29 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196496898_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196500600.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 22,184 characters\n",
            "📊 Found 32 headers and 32 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196500600_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196504206.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 19,556 characters\n",
            "📊 Found 50 headers and 50 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196504206_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196507887.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 22,558 characters\n",
            "📊 Found 54 headers and 54 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196507887_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196516767.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 22,347 characters\n",
            "📊 Found 184 headers and 184 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196516767_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196520593.pdf\n",
            "✅ Extracted 20 pages using pdfplumber\n",
            "📝 Extracted 72,262 characters\n",
            "📊 Found 204 headers and 204 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196520593_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196524243.pdf\n",
            "✅ Extracted 14 pages using pdfplumber\n",
            "📝 Extracted 22,044 characters\n",
            "📊 Found 45 headers and 45 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196524243_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196527907.pdf\n",
            "✅ Extracted 23 pages using pdfplumber\n",
            "📝 Extracted 54,298 characters\n",
            "📊 Found 64 headers and 64 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196527907_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196528494.pdf\n",
            "✅ Extracted 19 pages using pdfplumber\n",
            "📝 Extracted 51,676 characters\n",
            "📊 Found 219 headers and 219 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196528494_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196529929.pdf\n",
            "✅ Extracted 22 pages using pdfplumber\n",
            "📝 Extracted 76,653 characters\n",
            "📊 Found 230 headers and 230 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196529929_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196531503.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 13,145 characters\n",
            "📊 Found 78 headers and 78 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196531503_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196535281.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 16,356 characters\n",
            "📊 Found 87 headers and 87 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196535281_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196538845.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 26,865 characters\n",
            "📊 Found 47 headers and 47 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196538845_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196542531.pdf\n",
            "✅ Extracted 19 pages using pdfplumber\n",
            "📝 Extracted 62,930 characters\n",
            "📊 Found 80 headers and 80 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196542531_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196546202.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 10,480 characters\n",
            "📊 Found 31 headers and 31 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196546202_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196549856.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 11,511 characters\n",
            "📊 Found 38 headers and 38 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196549856_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196553752.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 52,499 characters\n",
            "📊 Found 103 headers and 103 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196553752_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196557533.pdf\n",
            "✅ Extracted 17 pages using pdfplumber\n",
            "📝 Extracted 73,383 characters\n",
            "📊 Found 94 headers and 94 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196557533_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196561098.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 15,312 characters\n",
            "📊 Found 38 headers and 38 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196561098_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196564780.pdf\n",
            "✅ Extracted 17 pages using pdfplumber\n",
            "📝 Extracted 40,878 characters\n",
            "📊 Found 46 headers and 46 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196564780_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196568293.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 22,703 characters\n",
            "📊 Found 78 headers and 78 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196568293_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196571965.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 32,640 characters\n",
            "📊 Found 92 headers and 92 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196571965_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196575597.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 28,230 characters\n",
            "📊 Found 50 headers and 50 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196575597_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196579287.pdf\n",
            "✅ Extracted 16 pages using pdfplumber\n",
            "📝 Extracted 35,533 characters\n",
            "📊 Found 58 headers and 58 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196579287_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196582919.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 15,489 characters\n",
            "📊 Found 47 headers and 47 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196582919_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196586629.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 26,752 characters\n",
            "📊 Found 57 headers and 57 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196586629_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196595598.pdf\n",
            "✅ Extracted 37 pages using pdfplumber\n",
            "📝 Extracted 88,830 characters\n",
            "📊 Found 220 headers and 220 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196595598_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196599309.pdf\n",
            "✅ Extracted 84 pages using pdfplumber\n",
            "📝 Extracted 256,501 characters\n",
            "📊 Found 365 headers and 365 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196599309_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196602919.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 9,358 characters\n",
            "📊 Found 34 headers and 34 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196602919_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196606455.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 12,393 characters\n",
            "📊 Found 35 headers and 35 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196606455_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196610218.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 25,433 characters\n",
            "📊 Found 117 headers and 117 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196610218_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196613833.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 42,333 characters\n",
            "📊 Found 133 headers and 133 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196613833_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196617449.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 25,567 characters\n",
            "📊 Found 359 headers and 359 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196617449_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196621127.pdf\n",
            "✅ Extracted 21 pages using pdfplumber\n",
            "📝 Extracted 56,825 characters\n",
            "📊 Found 330 headers and 330 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196621127_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196624874.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 9,136 characters\n",
            "📊 Found 32 headers and 32 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196624874_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196628577.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 12,244 characters\n",
            "📊 Found 35 headers and 35 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196628577_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196632095.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 7,967 characters\n",
            "📊 Found 38 headers and 38 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196632095_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196639503.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 26,015 characters\n",
            "📊 Found 62 headers and 62 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196639503_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196643162.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 32,502 characters\n",
            "📊 Found 85 headers and 85 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196643162_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196646753.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 37,494 characters\n",
            "📊 Found 81 headers and 81 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196646753_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196650361.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 31,393 characters\n",
            "📊 Found 60 headers and 60 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196650361_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196653991.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 36,561 characters\n",
            "📊 Found 85 headers and 85 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196653991_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196657667.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 38,463 characters\n",
            "📊 Found 92 headers and 92 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196657667_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196661361.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 16,917 characters\n",
            "📊 Found 45 headers and 45 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196661361_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196664981.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 16,653 characters\n",
            "📊 Found 44 headers and 44 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196664981_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196673815.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 24,630 characters\n",
            "📊 Found 56 headers and 56 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196673815_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196677468.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 48,745 characters\n",
            "📊 Found 78 headers and 78 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196677468_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196681179.pdf\n",
            "✅ Extracted 20 pages using pdfplumber\n",
            "📝 Extracted 69,312 characters\n",
            "📊 Found 90 headers and 90 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196681179_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196684660.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 33,935 characters\n",
            "📊 Found 90 headers and 90 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196684660_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196688385.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 23,905 characters\n",
            "📊 Found 58 headers and 58 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196688385_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196691957.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 33,059 characters\n",
            "📊 Found 79 headers and 79 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196691957_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196695634.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 11,890 characters\n",
            "📊 Found 51 headers and 51 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196695634_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196699183.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 21,038 characters\n",
            "📊 Found 48 headers and 48 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196699183_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196702731.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 17,228 characters\n",
            "📊 Found 48 headers and 48 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196702731_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196706420.pdf\n",
            "✅ Extracted 17 pages using pdfplumber\n",
            "📝 Extracted 34,481 characters\n",
            "📊 Found 46 headers and 46 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196706420_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196709952.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 8,576 characters\n",
            "📊 Found 41 headers and 41 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196709952_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196713576.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 13,053 characters\n",
            "📊 Found 46 headers and 46 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196713576_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196717321.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 19,539 characters\n",
            "📊 Found 87 headers and 87 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196717321_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196720897.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 36,934 characters\n",
            "📊 Found 91 headers and 91 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196720897_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196724676.pdf\n",
            "✅ Extracted 18 pages using pdfplumber\n",
            "📝 Extracted 45,768 characters\n",
            "📊 Found 82 headers and 82 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196724676_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196728215.pdf\n",
            "✅ Extracted 22 pages using pdfplumber\n",
            "📝 Extracted 58,108 characters\n",
            "📊 Found 141 headers and 141 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196728215_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196731779.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 16,577 characters\n",
            "📊 Found 37 headers and 37 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196731779_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196735357.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 15,926 characters\n",
            "📊 Found 37 headers and 37 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196735357_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196739041.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 21,180 characters\n",
            "📊 Found 110 headers and 110 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196739041_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196742737.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 23,024 characters\n",
            "📊 Found 110 headers and 110 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196742737_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196751518.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 16,306 characters\n",
            "📊 Found 46 headers and 46 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196751518_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196755157.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 34,379 characters\n",
            "📊 Found 66 headers and 66 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196755157_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196758819.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 25,174 characters\n",
            "📊 Found 62 headers and 62 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196758819_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196762588.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 27,623 characters\n",
            "📊 Found 58 headers and 58 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196762588_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196765986.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 12,997 characters\n",
            "📊 Found 51 headers and 51 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196765986_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196769625.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 15,307 characters\n",
            "📊 Found 51 headers and 51 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196769625_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196773342.pdf\n",
            "✅ Extracted 19 pages using pdfplumber\n",
            "📝 Extracted 32,100 characters\n",
            "📊 Found 137 headers and 137 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196773342_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196777019.pdf\n",
            "✅ Extracted 21 pages using pdfplumber\n",
            "📝 Extracted 45,475 characters\n",
            "📊 Found 144 headers and 144 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196777019_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196780608.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 16,013 characters\n",
            "📊 Found 51 headers and 51 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196780608_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196784240.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 19,346 characters\n",
            "📊 Found 43 headers and 43 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196784240_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196787949.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 23,566 characters\n",
            "📊 Found 46 headers and 46 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196787949_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196791590.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 30,353 characters\n",
            "📊 Found 56 headers and 56 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196791590_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196795381.pdf\n",
            "✅ Extracted 16 pages using pdfplumber\n",
            "📝 Extracted 21,937 characters\n",
            "📊 Found 77 headers and 77 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196795381_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196798953.pdf\n",
            "✅ Extracted 14 pages using pdfplumber\n",
            "📝 Extracted 19,630 characters\n",
            "📊 Found 76 headers and 76 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196798953_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196802456.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 15,057 characters\n",
            "📊 Found 100 headers and 100 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196802456_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196806421.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 23,816 characters\n",
            "📊 Found 97 headers and 97 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196806421_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196809620.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 9,694 characters\n",
            "📊 Found 82 headers and 82 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196809620_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196813207.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 14,790 characters\n",
            "📊 Found 69 headers and 69 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196813207_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196817074.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 24,835 characters\n",
            "📊 Found 92 headers and 92 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196817074_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196820707.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 37,251 characters\n",
            "📊 Found 115 headers and 115 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196820707_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196829318.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 10,039 characters\n",
            "📊 Found 36 headers and 36 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196829318_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196832970.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 9,038 characters\n",
            "📊 Found 32 headers and 32 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196832970_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196838404.pdf\n",
            "❌ Error processing bula_1755196838404.pdf: All extraction methods failed\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196838404_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196841053.pdf\n",
            "❌ Error processing bula_1755196841053.pdf: All extraction methods failed\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196841053_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196844034.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 20,847 characters\n",
            "📊 Found 65 headers and 65 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196844034_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196847655.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 25,201 characters\n",
            "📊 Found 77 headers and 77 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196847655_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196851201.pdf\n",
            "✅ Extracted 6 pages using pdfplumber\n",
            "📝 Extracted 10,593 characters\n",
            "📊 Found 41 headers and 41 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196851201_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196854854.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 9,767 characters\n",
            "📊 Found 40 headers and 40 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196854854_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196858646.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 13,899 characters\n",
            "📊 Found 55 headers and 55 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196858646_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196862262.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 15,734 characters\n",
            "📊 Found 55 headers and 55 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196862262_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196865859.pdf\n",
            "✅ Extracted 20 pages using pdfplumber\n",
            "📝 Extracted 41,027 characters\n",
            "📊 Found 131 headers and 131 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196865859_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196869538.pdf\n",
            "✅ Extracted 17 pages using pdfplumber\n",
            "📝 Extracted 54,449 characters\n",
            "📊 Found 129 headers and 129 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196869538_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196873201.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 29,829 characters\n",
            "📊 Found 88 headers and 88 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196873201_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196876907.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 33,811 characters\n",
            "📊 Found 100 headers and 100 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196876907_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196880438.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 28,567 characters\n",
            "📊 Found 68 headers and 68 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196880438_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196884089.pdf\n",
            "✅ Extracted 14 pages using pdfplumber\n",
            "📝 Extracted 52,842 characters\n",
            "📊 Found 102 headers and 102 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196884089_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196887789.pdf\n",
            "✅ Extracted 15 pages using pdfplumber\n",
            "📝 Extracted 35,115 characters\n",
            "📊 Found 107 headers and 107 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196887789_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196891410.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 28,673 characters\n",
            "📊 Found 72 headers and 72 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196891410_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196895087.pdf\n",
            "✅ Extracted 26 pages using pdfplumber\n",
            "📝 Extracted 42,596 characters\n",
            "📊 Found 150 headers and 150 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196895087_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196898733.pdf\n",
            "✅ Extracted 26 pages using pdfplumber\n",
            "📝 Extracted 38,028 characters\n",
            "📊 Found 274 headers and 274 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196898733_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196907511.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 10,544 characters\n",
            "📊 Found 31 headers and 31 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196907511_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196911205.pdf\n",
            "✅ Extracted 5 pages using pdfplumber\n",
            "📝 Extracted 11,706 characters\n",
            "📊 Found 32 headers and 32 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196911205_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196914926.pdf\n",
            "✅ Extracted 20 pages using pdfplumber\n",
            "📝 Extracted 39,324 characters\n",
            "📊 Found 211 headers and 211 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196914926_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196918531.pdf\n",
            "✅ Extracted 21 pages using pdfplumber\n",
            "📝 Extracted 50,960 characters\n",
            "📊 Found 205 headers and 205 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196918531_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196922134.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 15,247 characters\n",
            "📊 Found 52 headers and 52 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196922134_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196925769.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 17,280 characters\n",
            "📊 Found 52 headers and 52 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196925769_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196929564.pdf\n",
            "✅ Extracted 13 pages using pdfplumber\n",
            "📝 Extracted 25,471 characters\n",
            "📊 Found 147 headers and 147 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196929564_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196933188.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 30,303 characters\n",
            "📊 Found 129 headers and 129 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196933188_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196936729.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 12,086 characters\n",
            "📊 Found 65 headers and 65 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196936729_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196940347.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 17,307 characters\n",
            "📊 Found 65 headers and 65 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196940347_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196944021.pdf\n",
            "✅ Extracted 12 pages using pdfplumber\n",
            "📝 Extracted 23,396 characters\n",
            "📊 Found 118 headers and 118 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196944021_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196947792.pdf\n",
            "✅ Extracted 11 pages using pdfplumber\n",
            "📝 Extracted 27,813 characters\n",
            "📊 Found 88 headers and 88 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196947792_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196951411.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 12,822 characters\n",
            "📊 Found 69 headers and 69 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196951411_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196954993.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 14,845 characters\n",
            "📊 Found 64 headers and 64 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196954993_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196958735.pdf\n",
            "✅ Extracted 8 pages using pdfplumber\n",
            "📝 Extracted 13,293 characters\n",
            "📊 Found 55 headers and 55 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196958735_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196962312.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 16,175 characters\n",
            "📊 Found 48 headers and 48 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196962312_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196966084.pdf\n",
            "✅ Extracted 9 pages using pdfplumber\n",
            "📝 Extracted 13,621 characters\n",
            "📊 Found 65 headers and 65 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196966084_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196969754.pdf\n",
            "✅ Extracted 10 pages using pdfplumber\n",
            "📝 Extracted 19,973 characters\n",
            "📊 Found 64 headers and 64 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196969754_extracted.json\n",
            "\n",
            "📄 Processing: bula_1755196973404.pdf\n",
            "✅ Extracted 7 pages using pdfplumber\n",
            "📝 Extracted 13,630 characters\n",
            "📊 Found 46 headers and 46 sections\n",
            "✅ Saved: drive/MyDrive/extracted_data/bula_1755196973404_extracted.json\n",
            "\n",
            "======================================================================\n",
            "📊 EXTRACTION SUMMARY\n",
            "======================================================================\n",
            "📁 Total files: 341\n",
            "✅ Successful: 339\n",
            "❌ Failed: 2\n",
            "📝 Total characters: 10,609,333\n",
            "💾 Summary saved: drive/MyDrive/extracted_data/extraction_summary.json\n",
            "======================================================================\n",
            "\n",
            "🎉 Extraction completed!\n",
            "📁 Output directory: drive/MyDrive/extracted_data\n",
            "📄 Files ready for LLM processing: 341\n",
            "\n",
            "📋 Next steps:\n",
            "1. Run the LLM processor on the extracted data\n",
            "2. Check the extraction_summary.json for detailed results\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "PDF Text Extractor for Pharmaceutical Documents\n",
        "Extracts text content from all PDF files in a directory and saves structured data\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "from datetime import datetime\n",
        "import pymupdf4llm\n",
        "import pdfplumber\n",
        "\n",
        "class PDFTextExtractor:\n",
        "    def __init__(self, pdf_directory: str = \"pdf\", output_directory: str = \"extracted_data\"):\n",
        "        \"\"\"\n",
        "        Initialize PDF text extractor\n",
        "\n",
        "        Args:\n",
        "            pdf_directory: Directory containing PDF files to process\n",
        "            output_directory: Directory to save extracted data\n",
        "        \"\"\"\n",
        "        self.pdf_directory = Path(pdf_directory)\n",
        "        self.output_directory = Path(output_directory)\n",
        "        self.extracted_files = []\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        self.output_directory.mkdir(exist_ok=True)\n",
        "\n",
        "    def extract_pdf_content(self, pdf_path: Path) -> str:\n",
        "        \"\"\"Extract text content from PDF using multiple methods\"\"\"\n",
        "        try:\n",
        "            # Try pdfplumber first - better for structured documents\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                all_text = []\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        # Add page marker for structure analysis\n",
        "                        all_text.append(f\"=== PAGE {page_num + 1} ===\\n{page_text}\")\n",
        "\n",
        "                if all_text:\n",
        "                    print(f\"✅ Extracted {len(all_text)} pages using pdfplumber\")\n",
        "                    return \"\\n\\n\".join(all_text)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ pdfplumber failed for {pdf_path.name}: {e}\")\n",
        "\n",
        "        try:\n",
        "            # Fallback to pymupdf4llm - better for complex layouts\n",
        "            content = pymupdf4llm.to_markdown(str(pdf_path))\n",
        "            if content:\n",
        "                print(f\"✅ Extracted content using pymupdf4llm\")\n",
        "                return content\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ pymupdf4llm failed for {pdf_path.name}: {e}\")\n",
        "\n",
        "        raise Exception(\"All extraction methods failed\")\n",
        "\n",
        "    def identify_document_sections(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Identify basic document sections using text patterns\n",
        "        This creates a foundation for the LLM to work with\n",
        "        \"\"\"\n",
        "        sections = {\n",
        "            \"headers\": [],\n",
        "            \"potential_sections\": [],\n",
        "            \"page_breaks\": [],\n",
        "            \"content_blocks\": []\n",
        "        }\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        current_section = None\n",
        "        content_block = []\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            stripped_line = line.strip()\n",
        "\n",
        "            # Detect page breaks\n",
        "            if \"=== PAGE\" in line:\n",
        "                sections[\"page_breaks\"].append({\n",
        "                    \"line_number\": i,\n",
        "                    \"page_marker\": stripped_line\n",
        "                })\n",
        "                if content_block:\n",
        "                    sections[\"content_blocks\"].append({\n",
        "                        \"start_line\": i - len(content_block),\n",
        "                        \"end_line\": i - 1,\n",
        "                        \"content\": \"\\n\".join(content_block),\n",
        "                        \"section\": current_section\n",
        "                    })\n",
        "                    content_block = []\n",
        "                continue\n",
        "\n",
        "            # Skip empty lines\n",
        "            if not stripped_line:\n",
        "                continue\n",
        "\n",
        "            # Detect potential headers (various patterns)\n",
        "            is_header = False\n",
        "\n",
        "            # Pattern 1: All caps lines\n",
        "            if stripped_line.isupper() and len(stripped_line) > 3:\n",
        "                is_header = True\n",
        "\n",
        "            # Pattern 2: Numbered sections (1., 2., etc.)\n",
        "            if len(stripped_line) < 100 and any(stripped_line.startswith(f\"{num}.\") for num in range(1, 20)):\n",
        "                is_header = True\n",
        "\n",
        "            # Pattern 3: Lines that end with colon\n",
        "            if stripped_line.endswith(':') and len(stripped_line) < 100:\n",
        "                is_header = True\n",
        "\n",
        "            # Pattern 4: Lines with specific pharmaceutical keywords\n",
        "            pharma_headers = [\n",
        "                'COMPOSIÇÃO', 'INDICAÇÕES', 'CONTRAINDICAÇÕES', 'POSOLOGIA',\n",
        "                'REAÇÕES ADVERSAS', 'INTERAÇÕES', 'PRECAUÇÕES', 'SUPERDOSE',\n",
        "                'ARMAZENAMENTO', 'APRESENTAÇÃO', 'REGISTRO', 'FABRICANTE'\n",
        "            ]\n",
        "            if any(keyword in stripped_line.upper() for keyword in pharma_headers):\n",
        "                is_header = True\n",
        "\n",
        "            if is_header:\n",
        "                sections[\"headers\"].append({\n",
        "                    \"line_number\": i,\n",
        "                    \"text\": stripped_line,\n",
        "                    \"confidence\": \"high\" if stripped_line.isupper() else \"medium\"\n",
        "                })\n",
        "\n",
        "                # Save previous content block\n",
        "                if content_block:\n",
        "                    sections[\"content_blocks\"].append({\n",
        "                        \"start_line\": i - len(content_block),\n",
        "                        \"end_line\": i - 1,\n",
        "                        \"content\": \"\\n\".join(content_block),\n",
        "                        \"section\": current_section\n",
        "                    })\n",
        "\n",
        "                current_section = stripped_line\n",
        "                content_block = []\n",
        "                sections[\"potential_sections\"].append({\n",
        "                    \"title\": stripped_line,\n",
        "                    \"start_line\": i,\n",
        "                    \"type\": \"inferred\"\n",
        "                })\n",
        "            else:\n",
        "                content_block.append(stripped_line)\n",
        "\n",
        "        # Add final content block\n",
        "        if content_block:\n",
        "            sections[\"content_blocks\"].append({\n",
        "                \"start_line\": len(lines) - len(content_block),\n",
        "                \"end_line\": len(lines) - 1,\n",
        "                \"content\": \"\\n\".join(content_block),\n",
        "                \"section\": current_section\n",
        "            })\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def process_single_pdf(self, pdf_path: Path) -> Dict[str, Any]:\n",
        "        \"\"\"Process a single PDF file\"\"\"\n",
        "        print(f\"\\n📄 Processing: {pdf_path.name}\")\n",
        "\n",
        "        try:\n",
        "            # Extract text content\n",
        "            raw_text = self.extract_pdf_content(pdf_path)\n",
        "\n",
        "            if not raw_text:\n",
        "                raise Exception(\"No text content extracted\")\n",
        "\n",
        "            print(f\"📝 Extracted {len(raw_text):,} characters\")\n",
        "\n",
        "            # Identify document sections\n",
        "            sections = self.identify_document_sections(raw_text)\n",
        "\n",
        "            # Create structured data\n",
        "            structured_data = {\n",
        "                \"metadata\": {\n",
        "                    \"file_name\": pdf_path.name,\n",
        "                    \"file_path\": str(pdf_path),\n",
        "                    \"file_size\": pdf_path.stat().st_size,\n",
        "                    \"extraction_date\": datetime.now().isoformat(),\n",
        "                    \"text_length\": len(raw_text),\n",
        "                    \"extraction_method\": \"multi_method\",\n",
        "                    \"processing_stage\": \"text_extraction\"\n",
        "                },\n",
        "                \"raw_content\": raw_text,\n",
        "                \"document_structure\": {\n",
        "                    \"total_headers\": len(sections[\"headers\"]),\n",
        "                    \"total_sections\": len(sections[\"potential_sections\"]),\n",
        "                    \"total_pages\": len(sections[\"page_breaks\"]),\n",
        "                    \"total_content_blocks\": len(sections[\"content_blocks\"]),\n",
        "                    \"headers\": sections[\"headers\"],\n",
        "                    \"sections\": sections[\"potential_sections\"],\n",
        "                    \"page_breaks\": sections[\"page_breaks\"],\n",
        "                    \"content_blocks\": sections[\"content_blocks\"]\n",
        "                },\n",
        "                \"extraction_statistics\": {\n",
        "                    \"characters_extracted\": len(raw_text),\n",
        "                    \"lines_processed\": len(raw_text.split('\\n')),\n",
        "                    \"sections_identified\": len(sections[\"potential_sections\"]),\n",
        "                    \"headers_found\": len(sections[\"headers\"]),\n",
        "                    \"extraction_quality\": \"good\" if len(raw_text) > 1000 else \"poor\"\n",
        "                }\n",
        "            }\n",
        "\n",
        "            print(f\"📊 Found {len(sections['headers'])} headers and {len(sections['potential_sections'])} sections\")\n",
        "            return structured_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {pdf_path.name}: {e}\")\n",
        "            return {\n",
        "                \"metadata\": {\n",
        "                    \"file_name\": pdf_path.name,\n",
        "                    \"file_path\": str(pdf_path),\n",
        "                    \"extraction_date\": datetime.now().isoformat(),\n",
        "                    \"processing_stage\": \"text_extraction\",\n",
        "                    \"error\": str(e)\n",
        "                },\n",
        "                \"raw_content\": \"\",\n",
        "                \"document_structure\": {},\n",
        "                \"extraction_statistics\": {\n",
        "                    \"extraction_success\": False,\n",
        "                    \"error_message\": str(e)\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def process_all_pdfs(self) -> Dict[str, str]:\n",
        "        \"\"\"Process all PDF files in the directory\"\"\"\n",
        "        print(f\"🔍 Looking for PDF files in: {self.pdf_directory}\")\n",
        "\n",
        "        if not self.pdf_directory.exists():\n",
        "            raise Exception(f\"PDF directory not found: {self.pdf_directory}\")\n",
        "\n",
        "        pdf_files = list(self.pdf_directory.glob(\"*.pdf\"))\n",
        "\n",
        "        if not pdf_files:\n",
        "            raise Exception(f\"No PDF files found in: {self.pdf_directory}\")\n",
        "\n",
        "        print(f\"📁 Found {len(pdf_files)} PDF files to process\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        output_files = {}\n",
        "        processing_summary = {\n",
        "            \"total_files\": len(pdf_files),\n",
        "            \"successful_extractions\": 0,\n",
        "            \"failed_extractions\": 0,\n",
        "            \"total_characters_extracted\": 0,\n",
        "            \"files_processed\": [],\n",
        "            \"processing_date\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            try:\n",
        "                # Process the PDF\n",
        "                structured_data = self.process_single_pdf(pdf_file)\n",
        "\n",
        "                # Create output filename\n",
        "                output_filename = f\"{pdf_file.stem}_extracted.json\"\n",
        "                output_path = self.output_directory / output_filename\n",
        "\n",
        "                # Save the structured data\n",
        "                with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(structured_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "                output_files[pdf_file.name] = str(output_path)\n",
        "                self.extracted_files.append(str(output_path))\n",
        "\n",
        "                # Update summary\n",
        "                if structured_data.get(\"raw_content\"):\n",
        "                    processing_summary[\"successful_extractions\"] += 1\n",
        "                    processing_summary[\"total_characters_extracted\"] += len(structured_data[\"raw_content\"])\n",
        "                else:\n",
        "                    processing_summary[\"failed_extractions\"] += 1\n",
        "\n",
        "                processing_summary[\"files_processed\"].append({\n",
        "                    \"input_file\": pdf_file.name,\n",
        "                    \"output_file\": output_filename,\n",
        "                    \"success\": bool(structured_data.get(\"raw_content\")),\n",
        "                    \"characters_extracted\": len(structured_data.get(\"raw_content\", \"\")),\n",
        "                    \"headers_found\": structured_data.get(\"document_structure\", {}).get(\"total_headers\", 0)\n",
        "                })\n",
        "\n",
        "                print(f\"✅ Saved: {output_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to process {pdf_file.name}: {e}\")\n",
        "                processing_summary[\"failed_extractions\"] += 1\n",
        "                processing_summary[\"files_processed\"].append({\n",
        "                    \"input_file\": pdf_file.name,\n",
        "                    \"output_file\": None,\n",
        "                    \"success\": False,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "\n",
        "        # Save processing summary\n",
        "        summary_path = self.output_directory / \"extraction_summary.json\"\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(processing_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"📊 EXTRACTION SUMMARY\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"📁 Total files: {processing_summary['total_files']}\")\n",
        "        print(f\"✅ Successful: {processing_summary['successful_extractions']}\")\n",
        "        print(f\"❌ Failed: {processing_summary['failed_extractions']}\")\n",
        "        print(f\"📝 Total characters: {processing_summary['total_characters_extracted']:,}\")\n",
        "        print(f\"💾 Summary saved: {summary_path}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        return output_files\n",
        "\n",
        "    def get_extracted_files(self) -> list:\n",
        "        \"\"\"Get list of successfully extracted files\"\"\"\n",
        "        return self.extracted_files\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main extraction process\"\"\"\n",
        "    print(\"📄 PDF Text Extractor for Pharmaceutical Documents\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Initialize extractor\n",
        "    extractor = PDFTextExtractor(pdf_directory=\"./drive/MyDrive/pdf\", output_directory=\"./drive/MyDrive/extracted_data\")\n",
        "\n",
        "    try:\n",
        "        # Process all PDFs\n",
        "        output_files = extractor.process_all_pdfs()\n",
        "\n",
        "        print(f\"\\n🎉 Extraction completed!\")\n",
        "        print(f\"📁 Output directory: {extractor.output_directory}\")\n",
        "        print(f\"📄 Files ready for LLM processing: {len(output_files)}\")\n",
        "\n",
        "        # Show next steps\n",
        "        print(\"\\n📋 Next steps:\")\n",
        "        print(\"1. Run the LLM processor on the extracted data\")\n",
        "        print(\"2. Check the extraction_summary.json for detailed results\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Extraction failed: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ZlupdoPBmHFA",
        "outputId": "1a16a292-1db1-430c-d99c-9f72f16aced9"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2891258971.py, line 808)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2891258971.py\"\u001b[0;36m, line \u001b[0;32m808\u001b[0m\n\u001b[0;31m    except Exception as e:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Batch Automated Pharmaceutical Document Parser with Google Drive Integration\n",
        "Processes multiple PDFs from a directory and saves results to Google Drive\n",
        "Optimized for Google Colab environment\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import re\n",
        "import subprocess\n",
        "import shlex\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from datetime import datetime\n",
        "import pymupdf4llm\n",
        "import pdfplumber\n",
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "class BatchAutomatedPharmaParser:\n",
        "    def __init__(self, model_name: str = \"llama3.2:3b\"):\n",
        "        \"\"\"\n",
        "        Initialize batch parser with Ollama integration for Google Colab\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.batch_results = {}\n",
        "        self.failed_files = []\n",
        "        self.processing_stats = {\n",
        "            'total_files': 0,\n",
        "            'successful': 0,\n",
        "            'failed': 0,\n",
        "            'start_time': None,\n",
        "            'end_time': None\n",
        "        }\n",
        "        self.setup_environment()\n",
        "\n",
        "    def setup_environment(self):\n",
        "        \"\"\"Setup Google Colab environment\"\"\"\n",
        "        print(\"🔧 Setting up Google Colab environment...\")\n",
        "\n",
        "        # Mount Google Drive\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"✅ Google Drive mounted successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Drive mount warning: {e}\")\n",
        "\n",
        "        # Setup Ollama\n",
        "        self.setup_ollama()\n",
        "\n",
        "    def setup_ollama(self):\n",
        "        \"\"\"Setup Ollama model automatically\"\"\"\n",
        "        print(f\"Setting up Ollama model: {self.model_name}\")\n",
        "\n",
        "        try:\n",
        "            # Check if ollama is available\n",
        "            subprocess.run([\"ollama\", \"--version\"], capture_output=True, check=True)\n",
        "            print(\"✅ Ollama CLI found\")\n",
        "        except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "            print(\"❌ Ollama CLI not found. Installing...\")\n",
        "            # Install Ollama in Colab\n",
        "            os.system(\"curl -fsSL https://ollama.ai/install.sh | sh\")\n",
        "            # Start Ollama service\n",
        "            os.system(\"ollama serve &\")\n",
        "            time.sleep(5)\n",
        "\n",
        "        try:\n",
        "            # Pull model if not available\n",
        "            print(f\"Pulling model {self.model_name}...\")\n",
        "            result = subprocess.run(\n",
        "                [\"ollama\", \"pull\", self.model_name],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=300\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(f\"✅ Model {self.model_name} ready\")\n",
        "            else:\n",
        "                print(f\"⚠️ Pull result: {result.stderr}\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"⚠️ Model pull timed out, but model might already be available\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error pulling model: {e}\")\n",
        "\n",
        "    def call_ollama_raw(self, prompt: str, extra_flags: str = \"\") -> str:\n",
        "        \"\"\"Call ollama with exact prompt - no modifications\"\"\"\n",
        "        cmd = [\"ollama\", \"run\", self.model_name]\n",
        "        if extra_flags:\n",
        "            cmd += shlex.split(extra_flags)\n",
        "\n",
        "        try:\n",
        "            proc = subprocess.run(\n",
        "                cmd,\n",
        "                input=prompt,\n",
        "                text=True,\n",
        "                capture_output=True,\n",
        "                timeout=120\n",
        "            )\n",
        "            output = proc.stdout.strip()\n",
        "            if not output:\n",
        "                output = proc.stderr.strip()\n",
        "            return output\n",
        "        except subprocess.TimeoutExpired:\n",
        "            raise RuntimeError(\"Ollama call timed out\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error calling ollama: {e}\")\n",
        "\n",
        "    def extract_pdf_content(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text content from PDF\"\"\"\n",
        "        try:\n",
        "            # Try pdfplumber first\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                all_text = []\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        all_text.append(page_text)\n",
        "\n",
        "                if all_text:\n",
        "                    return \"\\n\\n\".join(all_text)\n",
        "        except Exception as e:\n",
        "            print(f\"pdfplumber failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            # Fallback to pymupdf4llm\n",
        "            return pymupdf4llm.to_markdown(pdf_path)\n",
        "        except Exception as e:\n",
        "            print(f\"pymupdf4llm failed: {e}\")\n",
        "            raise Exception(\"All extraction methods failed\")\n",
        "\n",
        "    def create_entity_extraction_prompt(self, text: str, chunk_size: int = 3000) -> List[str]:\n",
        "        \"\"\"Create prompts for entity extraction from pharmaceutical text\"\"\"\n",
        "\n",
        "        # Base prompt for pharmaceutical entity extraction\n",
        "        base_prompt = \"\"\"System: You are a parser. For each Text below, extract entities, relation, value triples as a JSON array.\n",
        "Only output valid JSON. DO NOT include any extra text, commentary, or code fences. Output must be parseable by json.loads().\n",
        "\n",
        "Format:\n",
        "[\n",
        "  {\"entity\": \"...\", \"relation\": \"...\", \"value\": \"...\"},\n",
        "  ...\n",
        "]\n",
        "\n",
        "Focus on pharmaceutical information:\n",
        "- Medication names and active ingredients\n",
        "- Dosages, concentrations, and administration routes\n",
        "- Indications, contraindications, and side effects\n",
        "- Age groups, patient populations\n",
        "- Storage conditions and expiration\n",
        "- Manufacturer information\n",
        "\n",
        "Text: \"\"\"\n",
        "\n",
        "        # Split text into chunks if too long\n",
        "        text_chunks = []\n",
        "        if len(text) <= chunk_size:\n",
        "            text_chunks.append(text)\n",
        "        else:\n",
        "            words = text.split()\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "\n",
        "            for word in words:\n",
        "                if current_length + len(word) + 1 > chunk_size:\n",
        "                    if current_chunk:\n",
        "                        text_chunks.append(\" \".join(current_chunk))\n",
        "                        current_chunk = []\n",
        "                        current_length = 0\n",
        "\n",
        "                current_chunk.append(word)\n",
        "                current_length += len(word) + 1\n",
        "\n",
        "            if current_chunk:\n",
        "                text_chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "        # Create prompts for each chunk\n",
        "        prompts = []\n",
        "        for i, chunk in enumerate(text_chunks):\n",
        "            prompt = f\"{base_prompt}{chunk}\"\n",
        "            prompts.append(prompt)\n",
        "\n",
        "        return prompts\n",
        "\n",
        "    def create_structure_analysis_prompt(self, text: str) -> str:\n",
        "        \"\"\"Create prompt for document structure analysis\"\"\"\n",
        "\n",
        "        structure_prompt = f\"\"\"System: You are a pharmaceutical document analyzer. Analyze the document structure and create a JSON summary.\n",
        "Only output valid JSON. DO NOT include any extra text, commentary, or code fences.\n",
        "\n",
        "Format:\n",
        "{{\n",
        "  \"document_type\": \"...\",\n",
        "  \"main_sections\": [\n",
        "    {{\n",
        "      \"section_number\": \"...\",\n",
        "      \"section_title\": \"...\",\n",
        "      \"content_type\": \"...\",\n",
        "      \"key_points\": [\"...\", \"...\"]\n",
        "    }}\n",
        "  ],\n",
        "  \"medication_info\": {{\n",
        "    \"name\": \"...\",\n",
        "    \"active_ingredient\": \"...\",\n",
        "    \"forms\": [\"...\", \"...\"],\n",
        "    \"concentrations\": [\"...\", \"...\"]\n",
        "  }},\n",
        "  \"critical_information\": {{\n",
        "    \"contraindications\": [\"...\", \"...\"],\n",
        "    \"serious_warnings\": [\"...\", \"...\"],\n",
        "    \"storage_conditions\": \"...\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Text: {text[:4000]}\"\"\"\n",
        "\n",
        "        return structure_prompt\n",
        "\n",
        "    def parse_json_response(self, response: str) -> Any:\n",
        "        \"\"\"Parse JSON response, handling common formatting issues\"\"\"\n",
        "        # Clean up response\n",
        "        cleaned = response.strip()\n",
        "\n",
        "        # Remove code fences if present\n",
        "        if cleaned.startswith(\"```\"):\n",
        "            lines = cleaned.split('\\n')\n",
        "            cleaned = '\\n'.join(lines[1:-1] if lines[-1].strip() == \"```\" else lines[1:])\n",
        "\n",
        "        # Remove any leading/trailing text that's not JSON\n",
        "        start_idx = cleaned.find('[') if cleaned.find('[') != -1 else cleaned.find('{')\n",
        "        end_idx = cleaned.rfind(']') if cleaned.rfind(']') != -1 else cleaned.rfind('}')\n",
        "\n",
        "        if start_idx != -1 and end_idx != -1:\n",
        "            cleaned = cleaned[start_idx:end_idx+1]\n",
        "\n",
        "        try:\n",
        "            return json.loads(cleaned)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON parse error: {e}\")\n",
        "            print(f\"Problematic text: {cleaned[:200]}...\")\n",
        "            return None\n",
        "\n",
        "    def extract_entities_from_document(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Extract entities from entire document\"\"\"\n",
        "        print(\"🔍 Extracting entities using LLM...\")\n",
        "\n",
        "        prompts = self.create_entity_extraction_prompt(text)\n",
        "        all_entities = []\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Processing chunk {i+1}/{len(prompts)}...\")\n",
        "\n",
        "            try:\n",
        "                response = self.call_ollama_raw(prompt)\n",
        "                entities = self.parse_json_response(response)\n",
        "\n",
        "                if entities and isinstance(entities, list):\n",
        "                    all_entities.extend(entities)\n",
        "                    print(f\"  Extracted {len(entities)} entities from chunk {i+1}\")\n",
        "                else:\n",
        "                    print(f\"  No valid entities from chunk {i+1}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing chunk {i+1}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Remove duplicates\n",
        "        unique_entities = []\n",
        "        seen = set()\n",
        "        for entity in all_entities:\n",
        "            key = (entity.get('entity', ''), entity.get('relation', ''), entity.get('value', ''))\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                unique_entities.append(entity)\n",
        "\n",
        "        print(f\"✅ Total unique entities extracted: {len(unique_entities)}\")\n",
        "        return unique_entities\n",
        "\n",
        "    def analyze_document_structure(self, text: str) -> Dict:\n",
        "        \"\"\"Analyze document structure using LLM\"\"\"\n",
        "        print(\"📋 Analyzing document structure using LLM...\")\n",
        "\n",
        "        prompt = self.create_structure_analysis_prompt(text)\n",
        "\n",
        "        try:\n",
        "            response = self.call_ollama_raw(prompt)\n",
        "            structure = self.parse_json_response(response)\n",
        "\n",
        "            if structure and isinstance(structure, dict):\n",
        "                print(\"✅ Document structure analyzed successfully\")\n",
        "                return structure\n",
        "            else:\n",
        "                print(\"⚠️ Could not parse structure analysis\")\n",
        "                return {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error analyzing structure: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def create_summary_prompt(self, entities: List[Dict], structure: Dict) -> str:\n",
        "        \"\"\"Create prompt for generating document summary\"\"\"\n",
        "\n",
        "        entities_text = json.dumps(entities[:50], indent=2)  # Limit to first 50 entities\n",
        "        structure_text = json.dumps(structure, indent=2)\n",
        "\n",
        "        summary_prompt = f\"\"\"System: You are a pharmaceutical document summarizer. Based on the extracted entities and document structure, create a comprehensive summary.\n",
        "Only output valid JSON. DO NOT include any extra text, commentary, or code fences.\n",
        "\n",
        "Format:\n",
        "{{\n",
        "  \"executive_summary\": \"...\",\n",
        "  \"medication_details\": {{\n",
        "    \"name\": \"...\",\n",
        "    \"active_ingredients\": [\"...\", \"...\"],\n",
        "    \"therapeutic_class\": \"...\",\n",
        "    \"indications\": [\"...\", \"...\"],\n",
        "    \"dosage_forms\": [\"...\", \"...\"],\n",
        "    \"key_dosages\": [\"...\", \"...\"]\n",
        "  }},\n",
        "  \"safety_information\": {{\n",
        "    \"contraindications\": [\"...\", \"...\"],\n",
        "    \"warnings\": [\"...\", \"...\"],\n",
        "    \"common_side_effects\": [\"...\", \"...\"],\n",
        "    \"serious_reactions\": [\"...\", \"...\"]\n",
        "  }},\n",
        "  \"administration_info\": {{\n",
        "    \"routes\": [\"...\", \"...\"],\n",
        "    \"dosing_schedule\": \"...\",\n",
        "    \"special_populations\": {{\n",
        "      \"pediatric\": \"...\",\n",
        "      \"geriatric\": \"...\",\n",
        "      \"renal_impairment\": \"...\",\n",
        "      \"hepatic_impairment\": \"...\"\n",
        "    }}\n",
        "  }},\n",
        "  \"storage_and_handling\": \"...\",\n",
        "  \"manufacturer\": \"...\"\n",
        "}}\n",
        "\n",
        "Extracted Entities:\n",
        "{entities_text}\n",
        "\n",
        "Document Structure:\n",
        "{structure_text}\"\"\"\n",
        "\n",
        "        return summary_prompt\n",
        "\n",
        "    def generate_comprehensive_summary(self, entities: List[Dict], structure: Dict) -> Dict:\n",
        "        \"\"\"Generate comprehensive summary using LLM\"\"\"\n",
        "        print(\"📝 Generating comprehensive summary...\")\n",
        "\n",
        "        prompt = self.create_summary_prompt(entities, structure)\n",
        "\n",
        "        try:\n",
        "            response = self.call_ollama_raw(prompt, extra_flags=\"--temperature 0.1\")\n",
        "            summary = self.parse_json_response(response)\n",
        "\n",
        "            if summary and isinstance(summary, dict):\n",
        "                print(\"✅ Summary generated successfully\")\n",
        "                return summary\n",
        "            else:\n",
        "                print(\"⚠️ Could not parse summary\")\n",
        "                return {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error generating summary: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def process_single_document(self, pdf_path: str) -> Optional[Dict]:\n",
        "        \"\"\"Process a single document and return structured data\"\"\"\n",
        "        print(f\"\\n📄 Processing: {Path(pdf_path).name}\")\n",
        "\n",
        "        if not Path(pdf_path).exists():\n",
        "            print(f\"❌ File not found: {pdf_path}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Extract text\n",
        "            raw_content = self.extract_pdf_content(pdf_path)\n",
        "\n",
        "            if not raw_content:\n",
        "                print(\"❌ No text content extracted\")\n",
        "                return None\n",
        "\n",
        "            print(f\"✅ Extracted {len(raw_content)} characters\")\n",
        "\n",
        "            # Analyze structure\n",
        "            structure = self.analyze_document_structure(raw_content)\n",
        "\n",
        "            # Extract entities\n",
        "            entities = self.extract_entities_from_document(raw_content)\n",
        "\n",
        "            # Generate summary\n",
        "            summary = self.generate_comprehensive_summary(entities, structure)\n",
        "\n",
        "            # Compile final structure\n",
        "            structured_data = {\n",
        "                \"metadata\": {\n",
        "                    \"file_path\": pdf_path,\n",
        "                    \"file_name\": Path(pdf_path).name,\n",
        "                    \"processing_date\": datetime.now().isoformat(),\n",
        "                    \"total_text_length\": len(raw_content),\n",
        "                    \"total_entities\": len(entities),\n",
        "                    \"model_used\": self.model_name\n",
        "                },\n",
        "                \"document_structure\": structure,\n",
        "                \"extracted_entities\": entities,\n",
        "                \"comprehensive_summary\": summary,\n",
        "                \"processing_statistics\": {\n",
        "                    \"entities_by_type\": self._count_entities_by_type(entities),\n",
        "                    \"structure_sections\": len(structure.get('main_sections', [])),\n",
        "                    \"processing_method\": \"automated_llm_analysis\"\n",
        "                }\n",
        "            }\n",
        "\n",
        "            print(f\"✅ Successfully processed: {Path(pdf_path).name}\")\n",
        "            return structured_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {Path(pdf_path).name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_pdf_files(self, pdf_directory: str) -> List[str]:\n",
        "        \"\"\"Get all PDF files from directory\"\"\"\n",
        "        pdf_dir = Path(pdf_directory)\n",
        "\n",
        "        if not pdf_dir.exists():\n",
        "            raise FileNotFoundError(f\"Directory not found: {pdf_directory}\")\n",
        "\n",
        "        # Find all PDF files\n",
        "        pdf_files = []\n",
        "        for pattern in ['*.pdf', '*.PDF']:\n",
        "            pdf_files.extend(glob.glob(str(pdf_dir / pattern)))\n",
        "\n",
        "        pdf_files.sort()\n",
        "        print(f\"📁 Found {len(pdf_files)} PDF files in {pdf_directory}\")\n",
        "\n",
        "        return pdf_files\n",
        "\n",
        "    def process_batch(self,\n",
        "                     pdf_directory: str = \"/content/pdf\",\n",
        "                     output_drive_path: str = \"/content/drive/MyDrive/pharma_analysis\",\n",
        "                     batch_size: int = 5,\n",
        "                     save_individual: bool = True,\n",
        "                     save_combined: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Process multiple PDFs in batches\n",
        "\n",
        "        Args:\n",
        "            pdf_directory: Directory containing PDF files\n",
        "            output_drive_path: Google Drive path to save results\n",
        "            batch_size: Number of files to process before saving interim results\n",
        "            save_individual: Save individual analysis files\n",
        "            save_combined: Save combined results file\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"🚀 Starting batch processing...\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Setup\n",
        "        self.processing_stats['start_time'] = datetime.now()\n",
        "\n",
        "        # Create output directory\n",
        "        output_dir = Path(output_drive_path)\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Create subdirectories\n",
        "        individual_dir = output_dir / \"individual_analyses\"\n",
        "        individual_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Get PDF files\n",
        "        try:\n",
        "            pdf_files = self.get_pdf_files(pdf_directory)\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"❌ {e}\")\n",
        "            return {}\n",
        "\n",
        "        if not pdf_files:\n",
        "            print(\"❌ No PDF files found\")\n",
        "            return {}\n",
        "\n",
        "        self.processing_stats['total_files'] = len(pdf_files)\n",
        "\n",
        "        # Process files in batches\n",
        "        for i, pdf_path in enumerate(pdf_files):\n",
        "            print(f\"\\n📊 Progress: {i+1}/{len(pdf_files)} ({(i+1)/len(pdf_files)*100:.1f}%)\")\n",
        "\n",
        "            # Process single document\n",
        "            result = self.process_single_document(pdf_path)\n",
        "\n",
        "            if result:\n",
        "                file_key = Path(pdf_path).stem\n",
        "                self.batch_results[file_key] = result\n",
        "                self.processing_stats['successful'] += 1\n",
        "\n",
        "                # Save individual file if requested\n",
        "                if save_individual:\n",
        "                    individual_file = individual_dir / f\"{file_key}_analysis.json\"\n",
        "                    with open(individual_file, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "                    print(f\"💾 Saved individual analysis: {individual_file.name}\")\n",
        "\n",
        "            else:\n",
        "                self.failed_files.append(pdf_path)\n",
        "                self.processing_stats['failed'] += 1\n",
        "\n",
        "            # Save interim results every batch_size files\n",
        "            if (i + 1) % batch_size == 0 and save_combined:\n",
        "                self.save_interim_results(output_dir, i + 1)\n",
        "\n",
        "            # Add small delay to prevent overloading\n",
        "            time.sleep(1)\n",
        "\n",
        "        # Final save\n",
        "        self.processing_stats['end_time'] = datetime.now()\n",
        "\n",
        "        if save_combined:\n",
        "            final_results_path = self.save_final_results(output_dir)\n",
        "\n",
        "        # Generate processing report\n",
        "        self.generate_processing_report(output_dir)\n",
        "\n",
        "        print(f\"\\n🎉 Batch processing completed!\")\n",
        "        self._show_batch_summary()\n",
        "\n",
        "        return self.batch_results\n",
        "\n",
        "    def save_interim_results(self, output_dir: Path, files_processed: int):\n",
        "        \"\"\"Save interim results during batch processing\"\"\"\n",
        "        interim_file = output_dir / f\"interim_results_{files_processed}_files.json\"\n",
        "\n",
        "        interim_data = {\n",
        "            \"processing_info\": {\n",
        "                \"files_processed\": files_processed,\n",
        "                \"successful\": self.processing_stats['successful'],\n",
        "                \"failed\": self.processing_stats['failed'],\n",
        "                \"interim_save_time\": datetime.now().isoformat()\n",
        "            },\n",
        "            \"results\": self.batch_results,\n",
        "            \"failed_files\": self.failed_files\n",
        "        }\n",
        "\n",
        "        with open(interim_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(interim_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"💾 Interim results saved: {interim_file.name}\")\n",
        "\n",
        "    def save_final_results(self, output_dir: Path) -> Path:\n",
        "        \"\"\"Save final combined results\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        final_file = output_dir / f\"combined_pharma_analysis_{timestamp}.json\"\n",
        "\n",
        "        final_data = {\n",
        "            \"batch_metadata\": {\n",
        "                \"processing_date\": datetime.now().isoformat(),\n",
        "                \"total_files_attempted\": self.processing_stats['total_files'],\n",
        "                \"successful_files\": self.processing_stats['successful'],\n",
        "                \"failed_files\": self.processing_stats['failed'],\n",
        "                \"model_used\": self.model_name,\n",
        "                \"processing_time_minutes\": self._get_processing_time_minutes()\n",
        "            },\n",
        "            \"processing_statistics\": self.processing_stats,\n",
        "            \"failed_files\": self.failed_files,\n",
        "            \"document_analyses\": self.batch_results\n",
        "        }\n",
        "\n",
        "        with open(final_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(final_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"💾 Final results saved: {final_file}\")\n",
        "        return final_file\n",
        "\n",
        "    def generate_processing_report(self, output_dir: Path):\n",
        "        \"\"\"Generate a human-readable processing report\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        report_file = output_dir / f\"processing_report_{timestamp}.txt\"\n",
        "\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"PHARMACEUTICAL DOCUMENT PROCESSING REPORT\\n\")\n",
        "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "            f.write(f\"Processing Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Model Used: {self.model_name}\\n\")\n",
        "            f.write(f\"Processing Time: {self._get_processing_time_minutes():.1f} minutes\\n\\n\")\n",
        "\n",
        "            f.write(\"SUMMARY STATISTICS:\\n\")\n",
        "            f.write(f\"- Total files attempted: {self.processing_stats['total_files']}\\n\")\n",
        "            f.write(f\"- Successfully processed: {self.processing_stats['successful']}\\n\")\n",
        "            f.write(f\"- Failed to process: {self.processing_stats['failed']}\\n\")\n",
        "            f.write(f\"- Success rate: {(self.processing_stats['successful']/self.processing_stats['total_files']*100):.1f}%\\n\\n\")\n",
        "\n",
        "            if self.failed_files:\n",
        "                f.write(\"FAILED FILES:\\n\")\n",
        "                for failed_file in self.failed_files:\n",
        "                    f.write(f\"- {Path(failed_file).name}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            f.write(\"PROCESSED FILES SUMMARY:\\n\")\n",
        "            for file_key, data in self.batch_results.items():\n",
        "                metadata = data.get('metadata', {})\n",
        "                stats = data.get('processing_statistics', {})\n",
        "                f.write(f\"\\n📄 {metadata.get('file_name', file_key)}:\\n\")\n",
        "                f.write(f\"   - Text length: {metadata.get('total_text_length', 0):,} chars\\n\")\n",
        "                f.write(f\"   - Entities extracted: {metadata.get('total_entities', 0)}\\n\")\n",
        "                f.write(f\"   - Structure sections: {stats.get('structure_sections', 0)}\\n\")\n",
        "\n",
        "        print(f\"📊 Processing report saved: {report_file}\")\n",
        "\n",
        "    def _count_entities_by_type(self, entities: List[Dict]) -> Dict[str, int]:\n",
        "        \"\"\"Count entities by relation type\"\"\"\n",
        "        counts = {}\n",
        "        for entity in entities:\n",
        "            relation = entity.get('relation', 'unknown')\n",
        "            counts[relation] = counts.get(relation, 0) + 1\n",
        "        return counts\n",
        "\n",
        "    def _get_processing_time_minutes(self) -> float:\n",
        "        \"\"\"Calculate processing time in minutes\"\"\"\n",
        "        if self.processing_stats['start_time'] and self.processing_stats['end_time']:\n",
        "            delta = self.processing_stats['end_time'] - self.processing_stats['start_time']\n",
        "            return delta.total_seconds() / 60\n",
        "        return 0\n",
        "\n",
        "    def _show_batch_summary(self):\n",
        "        \"\"\"Show batch processing summary\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"📊 BATCH PROCESSING SUMMARY\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        print(f\"🤖 Model: {self.model_name}\")\n",
        "        print(f\"📁 Total files: {self.processing_stats['total_files']}\")\n",
        "        print(f\"✅ Successful: {self.processing_stats['successful']}\")\n",
        "        print(f\"❌ Failed: {self.processing_stats['failed']}\")\n",
        "        print(f\"📈 Success rate: {(self.processing_stats['successful']/self.processing_stats['total_files']*100):.1f}%\")\n",
        "        print(f\"⏱️ Processing time: {self._get_processing_time_minutes():.1f} minutes\")\n",
        "\n",
        "        if self.failed_files:\n",
        "            print(f\"\\n❌ Failed files:\")\n",
        "            for failed_file in self.failed_files:\n",
        "                print(f\"   - {Path(failed_file).name}\")\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    def query_batch_results(self, question: str, file_name: Optional[str] = None) -> str:\n",
        "        \"\"\"Query processed batch results\"\"\"\n",
        "        if not self.batch_results:\n",
        "            return \"❌ No documents processed. Please run batch processing first.\"\n",
        "\n",
        "        if file_name:\n",
        "            # Query specific file\n",
        "            file_key = Path(file_name).stem\n",
        "            if file_key not in self.batch_results:\n",
        "                return f\"❌ File not found in processed results: {file_name}\"\n",
        "\n",
        "            return self._query_single_document(self.batch_results[file_key], question)\n",
        "        else:\n",
        "            # Query across all documents\n",
        "            return self._query_all_documents(question)\n",
        "\n",
        "    def _query_single_document(self, document_data: Dict, question: str) -> str:\n",
        "        \"\"\"Query a single document\"\"\"\n",
        "        # Create context from processed data\n",
        "        context_parts = []\n",
        "\n",
        "        # Add summary\n",
        "        summary = document_data.get(\"comprehensive_summary\", {})\n",
        "        if summary:\n",
        "            context_parts.append(\"DOCUMENT SUMMARY:\")\n",
        "            context_parts.append(json.dumps(summary, indent=2))\n",
        "\n",
        "        # Add relevant entities\n",
        "        entities = document_data.get(\"extracted_entities\", [])\n",
        "        question_words = question.lower().split()\n",
        "        relevant_entities = []\n",
        "\n",
        "        for entity in entities:\n",
        "            entity_text = f\"{entity.get('entity', '')} {entity.get('relation', '')} {entity.get('value', '')}\".lower()\n",
        "            if any(word in entity_text for word in question_words):\n",
        "                relevant_entities.append(entity)\n",
        "\n",
        "        if relevant_entities:\n",
        "            context_parts.append(\"\\nRELEVANT ENTITIES:\")\n",
        "            context_parts.append(json.dumps(relevant_entities[:10], indent=2))\n",
        "\n",
        "        context = \"\\n\".join(context_parts)\n",
        "\n",
        "        # Create query prompt\n",
        "        query_prompt = f\"\"\"System: You are a pharmaceutical document assistant. Answer the question based on the provided document context.\n",
        "Be precise and cite specific information when possible.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Document Context:\n",
        "{context[:6000]}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.call_ollama_raw(query_prompt, extra_flags=\"--temperature 0.1\")\n",
        "            return response.strip()\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error processing query: {e}\"\n",
        "\n",
        "    def _query_all_documents(self, question: str) -> str:\n",
        "        \"\"\"Query across all processed documents\"\"\"\n",
        "        print(f\"🔍 Searching across {len(self.batch_results)} documents...\")\n",
        "\n",
        "        relevant_docs = []\n",
        "        question_words = question.lower().split()\n",
        "\n",
        "        for file_key, doc_data in self.batch_results.items():\n",
        "            # Check if document is relevant to question\n",
        "            summary = doc_data.get(\"comprehensive_summary\", {})\n",
        "            entities = doc_data.get(\"extracted_entities\", [])\n",
        "\n",
        "            # Simple relevance scoring\n",
        "            relevance_score = 0\n",
        "\n",
        "            # Check summary\n",
        "            summary_text = json.dumps(summary).lower()\n",
        "            for word in question_words:\n",
        "                relevance_score += summary_text.count(word)\n",
        "\n",
        "            # Check entities\n",
        "            for entity in entities:\n",
        "                entity_text = f\"{entity.get('entity', '')} {entity.get('value', '')}\".lower()\n",
        "                for word in question_words:\n",
        "                    if word in entity_text:\n",
        "                        relevance_score += 1\n",
        "\n",
        "            if relevance_score > 0:\n",
        "                relevant_docs.append((file_key, doc_data, relevance_score))\n",
        "\n",
        "        # Sort by relevance\n",
        "        relevant_docs.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "        if not relevant_docs:\n",
        "            return \"❌ No relevant documents found for this question.\"\n",
        "\n",
        "        # Create combined context from top relevant documents\n",
        "        context_parts = [f\"Found {len(relevant_docs)} relevant documents:\\n\"]\n",
        "\n",
        "        for i, (file_key, doc_data, score) in enumerate(relevant_docs[:3]):  # Top 3 most relevant\n",
        "            context_parts.append(f\"\\nDOCUMENT {i+1}: {doc_data['metadata']['file_name']}\")\n",
        "\n",
        "            summary = doc_data.get(\"comprehensive_summary\", {})\n",
        "            if summary:\n",
        "                context_parts.append(json.dumps(summary, indent=2))\n",
        "\n",
        "        context = \"\\n\".join(context_parts)\n",
        "\n",
        "        # Create cross-document query prompt\n",
        "        query_prompt = f\"\"\"System: You are a pharmaceutical document assistant. Answer the question based on multiple document contexts.\n",
        "Compare information across documents when relevant and cite which document(s) contain specific information.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Multi-Document Context:\n",
        "{context[:8000]}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.call_ollama_raw(query_prompt, extra_flags=\"--temperature 0.1\")\n",
        "            return response.strip()\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error processing cross-document query: {e}\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function for batch processing in Google Colab\"\"\"\n",
        "    print(\"🤖 Batch Pharmaceutical Document Parser for Google Colab\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Initialize parser\n",
        "    parser = BatchAutomatedPharmaParser(model_name=\"llama3.2:3b\")\n",
        "\n",
        "    # Configuration\n",
        "    PDF_DIRECTORY = \"/content/drive/MyDrive/pdf\"  # Local directory in Colab\n",
        "    DRIVE_OUTPUT_PATH = \"/content/drive/MyDrive/pharma_analysis\"  # Google Drive path\n",
        "    BATCH_SIZE = 3  # Process 3 files before saving interim results\n",
        "\n",
        "    # Process batch\n",
        "    try:\n",
        "        results = parser.process_batch(\n",
        "            pdf_directory=PDF_DIRECTORY,\n",
        "            output_drive_path=DRIVE_OUTPUT_PATH,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            save_individual=True,\n",
        "            save_combined=True\n",
        "        )\n",
        "\n",
        "        if results:\n",
        "            print(\"\\n🔍 Testing batch querying...\")\n",
        "\n",
        "            # Test queries\n",
        "            test_questions = [\n",
        "                \"Quais medicamentos têm contraindicações para crianças?\",\n",
        "                \"Qual é a dosagem recomendada mais comum?\",\n",
        "                \"Quais são os efeitos colaterais mais frequentes?\"\n",
        "            ]\n",
        "\n",
        "            for question in test_questions:\n",
        "                print(f\"\\nQ: {question}\")\n",
        "                answer = parser.query_batch_results(question)\n",
        "                print(f\"A: {answer[:300]}...\")\n",
        "\n",
        "            except Exception as e:\n",
        "        print(f\"❌ Batch processing failed: {e}\")\n",
        "\n",
        "# Additional utility functions for Google Colab\n",
        "\n",
        "def setup_colab_environment():\n",
        "    \"\"\"Setup complete environment in Google Colab\"\"\"\n",
        "    print(\"🔧 Setting up complete Google Colab environment...\")\n",
        "\n",
        "    # Install required packages\n",
        "    packages_to_install = [\n",
        "        \"pymupdf4llm\",\n",
        "        \"pdfplumber\",\n",
        "        \"pathlib\"\n",
        "    ]\n",
        "\n",
        "    for package in packages_to_install:\n",
        "        try:\n",
        "            print(f\"Installing {package}...\")\n",
        "            os.system(f\"pip install {package}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not install {package}: {e}\")\n",
        "\n",
        "    # Install Ollama if not present\n",
        "    try:\n",
        "        subprocess.run([\"ollama\", \"--version\"], capture_output=True, check=True)\n",
        "        print(\"✅ Ollama already installed\")\n",
        "    except:\n",
        "        print(\"📦 Installing Ollama...\")\n",
        "        os.system(\"curl -fsSL https://ollama.ai/install.sh | sh\")\n",
        "        print(\"🚀 Starting Ollama service...\")\n",
        "        os.system(\"nohup ollama serve > /dev/null 2>&1 &\")\n",
        "        time.sleep(10)  # Wait for service to start\n",
        "\n",
        "    print(\"✅ Environment setup complete!\")\n",
        "\n",
        "def create_sample_directory_structure():\n",
        "    \"\"\"Create sample directory structure for testing\"\"\"\n",
        "    print(\"📁 Creating sample directory structure...\")\n",
        "\n",
        "    # Create PDF directory\n",
        "    pdf_dir = Path(\"/content/pdf\")\n",
        "    pdf_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Create output directory structure\n",
        "    output_dir = Path(\"/content/drive/MyDrive/pharma_analysis\")\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"✅ Created directories:\")\n",
        "    print(f\"   - PDF input: {pdf_dir}\")\n",
        "    print(f\"   - Drive output: {output_dir}\")\n",
        "\n",
        "    return str(pdf_dir), str(output_dir)\n",
        "\n",
        "def process_pharma_pdfs_batch(pdf_directory: str = \"/content/pdf\",\n",
        "                            output_drive_path: str = \"/content/drive/MyDrive/pharma_analysis\",\n",
        "                            model_name: str = \"llama3.2:3b\",\n",
        "                            batch_size: int = 3):\n",
        "    \"\"\"\n",
        "    Convenient function to process pharmaceutical PDFs in batch\n",
        "\n",
        "    Usage in Google Colab:\n",
        "    ```python\n",
        "    # Upload PDFs to /content/pdf directory first\n",
        "    results = process_pharma_pdfs_batch()\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    # Setup environment\n",
        "    setup_colab_environment()\n",
        "\n",
        "    # Initialize parser\n",
        "    parser = BatchAutomatedPharmaParser(model_name=model_name)\n",
        "\n",
        "    # Process batch\n",
        "    results = parser.process_batch(\n",
        "        pdf_directory=pdf_directory,\n",
        "        output_drive_path=output_drive_path,\n",
        "        batch_size=batch_size,\n",
        "        save_individual=True,\n",
        "        save_combined=True\n",
        "    )\n",
        "\n",
        "    return parser, results\n",
        "\n",
        "def quick_query(parser, question: str, file_name: Optional[str] = None):\n",
        "    \"\"\"Quick query function for interactive use\"\"\"\n",
        "    if not hasattr(parser, 'batch_results') or not parser.batch_results:\n",
        "        print(\"❌ No processed documents available\")\n",
        "        return None\n",
        "\n",
        "    answer = parser.query_batch_results(question, file_name)\n",
        "    print(f\"\\n❓ Question: {question}\")\n",
        "    if file_name:\n",
        "        print(f\"📄 Document: {file_name}\")\n",
        "    print(f\"💬 Answer: {answer}\")\n",
        "    return answer\n",
        "\n",
        "def list_processed_files(parser):\n",
        "    \"\"\"List all processed files\"\"\"\n",
        "    if not hasattr(parser, 'batch_results') or not parser.batch_results:\n",
        "        print(\"❌ No processed documents available\")\n",
        "        return []\n",
        "\n",
        "    print(\"📁 Processed files:\")\n",
        "    files = []\n",
        "    for file_key, data in parser.batch_results.items():\n",
        "        file_name = data['metadata']['file_name']\n",
        "        entity_count = data['metadata']['total_entities']\n",
        "        files.append(file_name)\n",
        "        print(f\"   - {file_name} ({entity_count} entities)\")\n",
        "\n",
        "    return files\n",
        "\n",
        "def export_entities_to_csv(parser, output_path: str = \"/content/drive/MyDrive/pharma_analysis/all_entities.csv\"):\n",
        "    \"\"\"Export all entities to CSV for analysis\"\"\"\n",
        "    if not hasattr(parser, 'batch_results') or not parser.batch_results:\n",
        "        print(\"❌ No processed documents available\")\n",
        "        return None\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    all_entities = []\n",
        "    for file_key, data in parser.batch_results.items():\n",
        "        file_name = data['metadata']['file_name']\n",
        "        entities = data.get('extracted_entities', [])\n",
        "\n",
        "        for entity in entities:\n",
        "            entity_row = {\n",
        "                'source_file': file_name,\n",
        "                'entity': entity.get('entity', ''),\n",
        "                'relation': entity.get('relation', ''),\n",
        "                'value': entity.get('value', ''),\n",
        "                'processing_date': data['metadata']['processing_date']\n",
        "            }\n",
        "            all_entities.append(entity_row)\n",
        "\n",
        "    df = pd.DataFrame(all_entities)\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "\n",
        "    print(f\"📊 Exported {len(all_entities)} entities to: {output_path}\")\n",
        "    print(f\"📈 Entity types distribution:\")\n",
        "    print(df['relation'].value_counts().head(10))\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# Google Colab specific helper functions\n",
        "\n",
        "def upload_pdfs_to_colab():\n",
        "    \"\"\"Helper to upload PDFs in Google Colab\"\"\"\n",
        "    from google.colab import files\n",
        "\n",
        "    print(\"📤 Upload your PDF files...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Move uploaded files to pdf directory\n",
        "    pdf_dir = Path(\"/content/pdf\")\n",
        "    pdf_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    moved_files = []\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            src = Path(f\"/content/{filename}\")\n",
        "            dst = pdf_dir / filename\n",
        "            src.rename(dst)\n",
        "            moved_files.append(str(dst))\n",
        "            print(f\"📁 Moved {filename} to pdf directory\")\n",
        "\n",
        "    print(f\"✅ Ready to process {len(moved_files)} PDF files\")\n",
        "    return moved_files\n",
        "\n",
        "def download_results_from_drive():\n",
        "    \"\"\"Helper to download results from Google Drive\"\"\"\n",
        "    from google.colab import files\n",
        "\n",
        "    results_dir = Path(\"/content/drive/MyDrive/pharma_analysis\")\n",
        "    if not results_dir.exists():\n",
        "        print(\"❌ No results directory found\")\n",
        "        return\n",
        "\n",
        "    # Find latest combined results file\n",
        "    json_files = list(results_dir.glob(\"combined_pharma_analysis_*.json\"))\n",
        "    if json_files:\n",
        "        latest_file = max(json_files, key=lambda f: f.stat().st_mtime)\n",
        "        print(f\"📥 Downloading latest results: {latest_file.name}\")\n",
        "        files.download(str(latest_file))\n",
        "\n",
        "    # Download processing report\n",
        "    report_files = list(results_dir.glob(\"processing_report_*.txt\"))\n",
        "    if report_files:\n",
        "        latest_report = max(report_files, key=lambda f: f.stat().st_mtime)\n",
        "        print(f\"📥 Downloading latest report: {latest_report.name}\")\n",
        "        files.download(str(latest_report))\n",
        "\n",
        "# Example usage for Google Colab\n",
        "\n",
        "Google Colab Usage Example:\n",
        "\n",
        "# 1. Setup and upload files\n",
        "setup_colab_environment()\n",
        "create_sample_directory_structure()\n",
        "upload_pdfs_to_colab()  # Upload your PDFs\n",
        "\n",
        "# 2. Process all PDFs\n",
        "parser, results = process_pharma_pdfs_batch(\n",
        "    pdf_directory=\"/content/pdf\",\n",
        "    output_drive_path=\"/content/drive/MyDrive/pharma_analysis\",\n",
        "    batch_size=3\n",
        ")\n",
        "\n",
        "# 3. Query results\n",
        "list_processed_files(parser)\n",
        "quick_query(parser, \"Qual é a dosagem recomendada?\")\n",
        "quick_query(parser, \"Quais são as contraindicações?\", \"specific_file.pdf\")\n",
        "\n",
        "# 4. Export data\n",
        "export_entities_to_csv(parser)\n",
        "\n",
        "# 5. Download results\n",
        "download_results_from_drive()\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For direct execution in Google Colab\n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        print(\"🔍 Detected Google Colab environment\")\n",
        "        print(\"Run the following commands to get started:\")\n",
        "        print(\"1. setup_colab_environment()\")\n",
        "        print(\"2. upload_pdfs_to_colab() or place PDFs in /content/pdf/\")\n",
        "        print(\"3. parser, results = process_pharma_pdfs_batch()\")\n",
        "    else:\n",
        "        # Run normally for local execution\n",
        "        main()\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltv_egaOm0DX",
        "outputId": "94b0b274-670b-45b9-8cb5-db66d769c285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Setting up improved environment...\n",
            "✅ Ollama available\n",
            "✅ Environment ready!\n",
            "\n",
            "🚀 Starting pharmaceutical document processing...\n",
            "======================================================================\n",
            "🚀 Starting safe batch processing (max 5 files)\n",
            "🔧 Setting up improved Google Colab environment...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive mounted successfully\n",
            "Setting up Ollama model: llama3.2:3b\n",
            "🚀 Starting Ollama service...\n",
            "✅ Model llama3.2:3b already available\n",
            "🚀 Starting improved batch processing...\n",
            "======================================================================\n",
            "📁 Found 341 valid PDF files in /content/drive/MyDrive/pdf\n",
            "📁 Processing 5 files starting from index 0\n",
            "\n",
            "📊 Progress: 1/5 (20.0%)\n",
            "⏱️ Estimated time remaining: 0 minutes\n",
            "\n",
            "📄 Processing: bula_1755192077396.pdf\n",
            "  📖 Extracting text from bula_1755192077396.pdf\n",
            "✅ Extracted 11,936 characters\n",
            "🔍 Extracting entities (simplified approach)...\n",
            "  Attempt 1/2 (timeout: 120s)\n",
            "  Timeout after 120s, retrying...\n",
            "  Attempt 2/2 (timeout: 180.0s)\n",
            "  Timeout after 180.0s, retrying...\n",
            "⚠️ Entity extraction failed: All retry attempts failed\n",
            "✅ Successfully processed in 301.3s: bula_1755192077396.pdf\n",
            "📈 Current success rate: 100.0%\n",
            "\n",
            "📊 Progress: 2/5 (40.0%)\n",
            "⏱️ Estimated time remaining: 7.585366575 minutes\n",
            "\n",
            "📄 Processing: bula_1755192097944.pdf\n",
            "  📖 Extracting text from bula_1755192097944.pdf\n",
            "✅ Extracted 47,606 characters\n",
            "🔍 Extracting entities (simplified approach)...\n",
            "  Attempt 1/2 (timeout: 120s)\n",
            "  Timeout after 120s, retrying...\n",
            "  Attempt 2/2 (timeout: 180.0s)\n",
            "JSON parse error: Invalid control character at: line 1 column 4 (char 3)\n",
            "Problematic JSON: [\"{\n",
            "  \\\"name\\\": \\\"ezetimiba\\\", \n",
            "  \\\"type\\\": \\\"medication\\\", \n",
            "  \\\"value\\\": \\\"Sandoz do Brasil Ind. Farm. Ltda. Bula do Profissional da Saúde\\\"}, \n",
            "{\n",
            "  \\\"name\\\": \\\"dosage\\\", \n",
            "  \\\"type\\\": \\\"medication\\\", ...\n",
            "⚠️ Entity extraction returned no valid results\n",
            "✅ Successfully processed in 264.2s: bula_1755192097944.pdf\n",
            "📈 Current success rate: 100.0%\n",
            "\n",
            "📊 Progress: 3/5 (60.0%)\n",
            "⏱️ Estimated time remaining: 6.329698511111111 minutes\n",
            "\n",
            "📄 Processing: bula_1755195358088.pdf\n",
            "  📖 Extracting text from bula_1755195358088.pdf\n",
            "✅ Extracted 29,143 characters\n",
            "🔍 Extracting entities (simplified approach)...\n",
            "  Attempt 1/2 (timeout: 120s)\n",
            "  Timeout after 120s, retrying...\n",
            "  Attempt 2/2 (timeout: 180.0s)\n",
            "  Timeout after 180.0s, retrying...\n",
            "⚠️ Entity extraction failed: All retry attempts failed\n",
            "✅ Successfully processed in 302.0s: bula_1755195358088.pdf\n",
            "💾 Checkpoint saved at file 2\n",
            "📈 Current success rate: 100.0%\n",
            "\n",
            "📊 Progress: 4/5 (80.0%)\n",
            "⏱️ Estimated time remaining: 3.6405809541666665 minutes\n",
            "\n",
            "📄 Processing: bula_1755195361693.pdf\n",
            "  📖 Extracting text from bula_1755195361693.pdf\n",
            "✅ Extracted 22,797 characters\n",
            "🔍 Extracting entities (simplified approach)...\n",
            "  Attempt 1/2 (timeout: 120s)\n",
            "  Timeout after 120s, retrying...\n",
            "  Attempt 2/2 (timeout: 180.0s)\n",
            "✅ Extracted 7 entities\n",
            "📝 Generating summary...\n",
            "  Attempt 1/2 (timeout: 60s)\n",
            "  Timeout after 60s, retrying...\n",
            "  Attempt 2/2 (timeout: 90.0s)\n",
            "JSON parse error: Invalid \\uXXXX escape: line 1 column 121 (char 120)\n",
            "Problematic JSON: {\"medication_name\": \"Talidomida\", \"main_use\": \"Eritema nodoso hans\\u00e9nico, aftas orais em pessoas vivendo com HIV, l\\u00fupus eritematoso, doen\\u00e7a enxerto contra hospedeiro, mieloma m\\u00faltip...\n",
            "⚠️ Summary generation failed\n",
            "✅ Successfully processed in 373.7s: bula_1755195361693.pdf\n",
            "📈 Current success rate: 100.0%\n",
            "\n",
            "📊 Progress: 5/5 (100.0%)\n",
            "⏱️ Estimated time remaining: 0.0 minutes\n",
            "\n",
            "📄 Processing: bula_1755195365369.pdf\n",
            "  📖 Extracting text from bula_1755195365369.pdf\n",
            "✅ Extracted 50,020 characters\n",
            "🔍 Extracting entities (simplified approach)...\n",
            "  Attempt 1/2 (timeout: 120s)\n",
            "  Timeout after 120s, retrying...\n",
            "  Attempt 2/2 (timeout: 180.0s)\n",
            "  Timeout after 180.0s, retrying...\n",
            "⚠️ Entity extraction failed: All retry attempts failed\n",
            "✅ Successfully processed in 303.2s: bula_1755195365369.pdf\n",
            "📈 Current success rate: 100.0%\n",
            "💾 Final results saved: /content/drive/MyDrive/pharma_analysis/batch_analysis_results_20250831_201909.json\n",
            "📊 Processing report saved: /content/drive/MyDrive/pharma_analysis/processing_report_20250831_201909.md\n",
            "\n",
            "🎉 Batch processing completed!\n",
            "\n",
            "======================================================================\n",
            "📊 BATCH PROCESSING SUMMARY\n",
            "======================================================================\n",
            "🤖 Model: llama3.2:3b\n",
            "📁 Total files: 5\n",
            "✅ Successful: 5 (100.0%)\n",
            "❌ Failed: 0 (0.0%)\n",
            "⏱️ Total time: 25.9 minutes\n",
            "📈 Average time per file: 5.2 minutes\n",
            "🔍 Total entities extracted: 7\n",
            "======================================================================\n",
            "📊 PROCESSING STATISTICS\n",
            "========================================\n",
            "Files processed: 5/5\n",
            "Success rate: 100.0%\n",
            "Total time: 25.9 minutes\n",
            "Total entities: 7\n",
            "Average entities per file: 1.4\n",
            "\n",
            "🔍 Testing search functionality...\n",
            "❌ No results found for: dosagem\n",
            "❌ No results found for: contraindicação\n",
            "❌ No results found for: efeitos colaterais\n",
            "\n",
            "✅ Processing complete! Check your Google Drive for results.\n",
            "🎉 Improved Pharmaceutical Document Parser loaded!\n",
            "📝 Usage:\n",
            "1. setup_improved_environment()\n",
            "2. parser, results = process_pharma_batch_safe(max_files=5)\n",
            "3. search_results(parser, 'your_search_term')\n",
            "4. export_to_excel(parser)\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Improved Batch Automated Pharmaceutical Document Parser with Google Drive Integration\n",
        "Fixed version with timeout handling, progress saving, and optimized prompts\n",
        "Optimized for Google Colab environment\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import re\n",
        "import subprocess\n",
        "import shlex\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from datetime import datetime\n",
        "import pymupdf4llm\n",
        "import pdfplumber\n",
        "from google.colab import drive\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "class ImprovedBatchPharmaParser:\n",
        "    def __init__(self, model_name: str = \"llama3.2:3b\"):\n",
        "        \"\"\"\n",
        "        Initialize improved batch parser with better timeout handling\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.batch_results = {}\n",
        "        self.failed_files = []\n",
        "        self.processing_stats = {\n",
        "            'total_files': 0,\n",
        "            'successful': 0,\n",
        "            'failed': 0,\n",
        "            'start_time': None,\n",
        "            'end_time': None\n",
        "        }\n",
        "        self.checkpoint_file = None\n",
        "        self.setup_environment()\n",
        "\n",
        "    def setup_environment(self):\n",
        "        \"\"\"Setup Google Colab environment\"\"\"\n",
        "        print(\"🔧 Setting up improved Google Colab environment...\")\n",
        "\n",
        "        # Mount Google Drive\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"✅ Google Drive mounted successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Drive mount warning: {e}\")\n",
        "\n",
        "        # Setup Ollama with better configuration\n",
        "        self.setup_ollama_improved()\n",
        "\n",
        "    def setup_ollama_improved(self):\n",
        "        \"\"\"Setup Ollama with better configuration\"\"\"\n",
        "        print(f\"Setting up Ollama model: {self.model_name}\")\n",
        "\n",
        "        try:\n",
        "            # Check if ollama is running\n",
        "            subprocess.run([\"ollama\", \"list\"], capture_output=True, check=True, timeout=10)\n",
        "            print(\"✅ Ollama service is running\")\n",
        "        except:\n",
        "            print(\"🚀 Starting Ollama service...\")\n",
        "            os.system(\"nohup ollama serve > /tmp/ollama.log 2>&1 &\")\n",
        "            time.sleep(10)\n",
        "\n",
        "        try:\n",
        "            # Check if model exists\n",
        "            result = subprocess.run(\n",
        "                [\"ollama\", \"list\"],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if self.model_name in result.stdout:\n",
        "                print(f\"✅ Model {self.model_name} already available\")\n",
        "            else:\n",
        "                print(f\"📥 Pulling model {self.model_name}...\")\n",
        "                subprocess.run(\n",
        "                    [\"ollama\", \"pull\", self.model_name],\n",
        "                    timeout=600,  # 10 minutes for model download\n",
        "                    check=True\n",
        "                )\n",
        "                print(f\"✅ Model {self.model_name} ready\")\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"⚠️ Model setup timed out, continuing anyway...\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Model setup error: {e}\")\n",
        "\n",
        "    def call_ollama_with_retry(self, prompt: str, max_retries: int = 3, timeout: int = 180) -> str:\n",
        "        \"\"\"Call ollama with retry logic and better timeout handling\"\"\"\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                print(f\"  Attempt {attempt + 1}/{max_retries} (timeout: {timeout}s)\")\n",
        "\n",
        "                # Use simpler command structure\n",
        "                cmd = [\"ollama\", \"run\", self.model_name, \"--\"]\n",
        "\n",
        "                proc = subprocess.run(\n",
        "                    cmd,\n",
        "                    input=prompt,\n",
        "                    text=True,\n",
        "                    capture_output=True,\n",
        "                    timeout=timeout\n",
        "                )\n",
        "\n",
        "                output = proc.stdout.strip()\n",
        "                if output:\n",
        "                    return output\n",
        "                elif proc.stderr:\n",
        "                    print(f\"  Warning: {proc.stderr.strip()}\")\n",
        "\n",
        "                # If no output, try again with longer timeout\n",
        "                timeout = min(timeout * 1.5, 300)  # Max 5 minutes\n",
        "\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(f\"  Timeout after {timeout}s, retrying...\")\n",
        "                timeout = min(timeout * 1.5, 300)\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"  Error on attempt {attempt + 1}: {e}\")\n",
        "                if attempt == max_retries - 1:\n",
        "                    raise RuntimeError(f\"All {max_retries} attempts failed\")\n",
        "                time.sleep(2)\n",
        "                continue\n",
        "\n",
        "        raise RuntimeError(\"All retry attempts failed\")\n",
        "\n",
        "    def extract_pdf_content_safe(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text content with better error handling\"\"\"\n",
        "        try:\n",
        "            print(f\"  📖 Extracting text from {Path(pdf_path).name}\")\n",
        "\n",
        "            # Try pdfplumber first (usually faster)\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                all_text = []\n",
        "                for i, page in enumerate(pdf.pages[:50]):  # Limit to first 50 pages\n",
        "                    try:\n",
        "                        page_text = page.extract_text()\n",
        "                        if page_text:\n",
        "                            all_text.append(page_text)\n",
        "                    except Exception as e:\n",
        "                        print(f\"    Warning: Page {i+1} extraction failed: {e}\")\n",
        "                        continue\n",
        "\n",
        "                if all_text:\n",
        "                    content = \"\\n\\n\".join(all_text)\n",
        "                    # Limit content size to prevent huge prompts\n",
        "                    if len(content) > 50000:  # 50k characters max\n",
        "                        content = content[:50000] + \"\\n[CONTENT TRUNCATED]\"\n",
        "                    return content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    pdfplumber failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            # Fallback to pymupdf4llm\n",
        "            print(\"    Trying pymupdf4llm...\")\n",
        "            content = pymupdf4llm.to_markdown(pdf_path)\n",
        "            if len(content) > 50000:\n",
        "                content = content[:50000] + \"\\n[CONTENT TRUNCATED]\"\n",
        "            return content\n",
        "        except Exception as e:\n",
        "            print(f\"    pymupdf4llm failed: {e}\")\n",
        "            raise Exception(\"All extraction methods failed\")\n",
        "\n",
        "    def create_simple_entity_prompt(self, text: str) -> str:\n",
        "        \"\"\"Create simplified prompt for better success rate\"\"\"\n",
        "\n",
        "        # Much simpler prompt that's more likely to succeed\n",
        "        simple_prompt = f\"\"\"Extract key pharmaceutical information as JSON array. Only return valid JSON, no other text.\n",
        "\n",
        "Format: [{{\"name\": \"medication name\", \"type\": \"category\", \"value\": \"details\"}}]\n",
        "\n",
        "Categories: medication, dosage, indication, contraindication, side_effect, manufacturer, storage\n",
        "\n",
        "Text (first 2000 chars):\n",
        "{text[:2000]}\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "        return simple_prompt\n",
        "\n",
        "    def create_simple_summary_prompt(self, entities: List[Dict]) -> str:\n",
        "        \"\"\"Create simplified summary prompt\"\"\"\n",
        "\n",
        "        entities_text = json.dumps(entities[:20], indent=1)  # Limit entities\n",
        "\n",
        "        summary_prompt = f\"\"\"Create medication summary as JSON. Only return valid JSON, no other text.\n",
        "\n",
        "Format:\n",
        "{{\"medication_name\": \"...\", \"main_use\": \"...\", \"key_warnings\": \"...\", \"dosage_info\": \"...\"}}\n",
        "\n",
        "Entities:\n",
        "{entities_text}\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "        return summary_prompt\n",
        "\n",
        "    def process_single_document_improved(self, pdf_path: str) -> Optional[Dict]:\n",
        "        \"\"\"Improved document processing with better error handling\"\"\"\n",
        "        print(f\"\\n📄 Processing: {Path(pdf_path).name}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Extract text with size limits\n",
        "            raw_content = self.extract_pdf_content_safe(pdf_path)\n",
        "\n",
        "            if not raw_content:\n",
        "                print(\"❌ No text content extracted\")\n",
        "                return None\n",
        "\n",
        "            print(f\"✅ Extracted {len(raw_content):,} characters\")\n",
        "\n",
        "            # Step 1: Simple entity extraction\n",
        "            entities = []\n",
        "            try:\n",
        "                print(\"🔍 Extracting entities (simplified approach)...\")\n",
        "                entity_prompt = self.create_simple_entity_prompt(raw_content)\n",
        "\n",
        "                response = self.call_ollama_with_retry(entity_prompt, max_retries=2, timeout=120)\n",
        "                parsed_entities = self.parse_json_response_safe(response)\n",
        "\n",
        "                if parsed_entities and isinstance(parsed_entities, list):\n",
        "                    entities = parsed_entities\n",
        "                    print(f\"✅ Extracted {len(entities)} entities\")\n",
        "                else:\n",
        "                    print(\"⚠️ Entity extraction returned no valid results\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Entity extraction failed: {e}\")\n",
        "\n",
        "            # Step 2: Simple summary\n",
        "            summary = {}\n",
        "            try:\n",
        "                if entities:\n",
        "                    print(\"📝 Generating summary...\")\n",
        "                    summary_prompt = self.create_simple_summary_prompt(entities)\n",
        "\n",
        "                    response = self.call_ollama_with_retry(summary_prompt, max_retries=2, timeout=60)\n",
        "                    parsed_summary = self.parse_json_response_safe(response)\n",
        "\n",
        "                    if parsed_summary and isinstance(parsed_summary, dict):\n",
        "                        summary = parsed_summary\n",
        "                        print(\"✅ Summary generated\")\n",
        "                    else:\n",
        "                        print(\"⚠️ Summary generation failed\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Summary generation failed: {e}\")\n",
        "\n",
        "            # Create result structure (simpler than original)\n",
        "            processing_time = time.time() - start_time\n",
        "\n",
        "            structured_data = {\n",
        "                \"metadata\": {\n",
        "                    \"file_path\": pdf_path,\n",
        "                    \"file_name\": Path(pdf_path).name,\n",
        "                    \"processing_date\": datetime.now().isoformat(),\n",
        "                    \"text_length\": len(raw_content),\n",
        "                    \"entity_count\": len(entities),\n",
        "                    \"model_used\": self.model_name,\n",
        "                    \"processing_time_seconds\": round(processing_time, 2)\n",
        "                },\n",
        "                \"entities\": entities,\n",
        "                \"summary\": summary,\n",
        "                \"status\": \"success\"\n",
        "            }\n",
        "\n",
        "            print(f\"✅ Successfully processed in {processing_time:.1f}s: {Path(pdf_path).name}\")\n",
        "            return structured_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {Path(pdf_path).name}: {e}\")\n",
        "            return {\n",
        "                \"metadata\": {\n",
        "                    \"file_path\": pdf_path,\n",
        "                    \"file_name\": Path(pdf_path).name,\n",
        "                    \"processing_date\": datetime.now().isoformat(),\n",
        "                    \"error\": str(e),\n",
        "                    \"model_used\": self.model_name\n",
        "                },\n",
        "                \"status\": \"failed\"\n",
        "            }\n",
        "\n",
        "    def parse_json_response_safe(self, response: str) -> Any:\n",
        "        \"\"\"Safer JSON parsing with multiple fallback strategies\"\"\"\n",
        "        if not response:\n",
        "            return None\n",
        "\n",
        "        cleaned = response.strip()\n",
        "\n",
        "        # Remove code fences\n",
        "        if \"```\" in cleaned:\n",
        "            lines = cleaned.split('\\n')\n",
        "            json_lines = []\n",
        "            in_code_block = False\n",
        "            for line in lines:\n",
        "                if line.strip().startswith(\"```\"):\n",
        "                    in_code_block = not in_code_block\n",
        "                    continue\n",
        "                if not in_code_block:\n",
        "                    json_lines.append(line)\n",
        "            cleaned = '\\n'.join(json_lines)\n",
        "\n",
        "        # Find JSON boundaries\n",
        "        json_start = -1\n",
        "        json_end = -1\n",
        "\n",
        "        # Look for array or object start\n",
        "        for i, char in enumerate(cleaned):\n",
        "            if char in '[{':\n",
        "                json_start = i\n",
        "                break\n",
        "\n",
        "        # Look for corresponding end from the back\n",
        "        if json_start != -1:\n",
        "            bracket_count = 0\n",
        "            start_char = cleaned[json_start]\n",
        "            end_char = ']' if start_char == '[' else '}'\n",
        "\n",
        "            for i in range(json_start, len(cleaned)):\n",
        "                if cleaned[i] == start_char:\n",
        "                    bracket_count += 1\n",
        "                elif cleaned[i] == end_char:\n",
        "                    bracket_count -= 1\n",
        "                    if bracket_count == 0:\n",
        "                        json_end = i\n",
        "                        break\n",
        "\n",
        "        if json_start != -1 and json_end != -1:\n",
        "            json_str = cleaned[json_start:json_end+1]\n",
        "\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                # Try to fix common JSON issues\n",
        "                json_str = json_str.replace(\"'\", '\"')  # Single to double quotes\n",
        "                json_str = re.sub(r',\\s*}', '}', json_str)  # Remove trailing commas\n",
        "                json_str = re.sub(r',\\s*]', ']', json_str)\n",
        "\n",
        "                try:\n",
        "                    return json.loads(json_str)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"JSON parse error: {e}\")\n",
        "                    print(f\"Problematic JSON: {json_str[:200]}...\")\n",
        "                    return None\n",
        "\n",
        "        print(\"Could not find valid JSON in response\")\n",
        "        return None\n",
        "\n",
        "    def save_checkpoint(self, output_dir: Path, current_index: int):\n",
        "        \"\"\"Save current progress to avoid losing work\"\"\"\n",
        "        checkpoint_data = {\n",
        "            'batch_results': self.batch_results,\n",
        "            'failed_files': self.failed_files,\n",
        "            'processing_stats': self.processing_stats,\n",
        "            'current_index': current_index,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        checkpoint_file = output_dir / \"processing_checkpoint.pkl\"\n",
        "        with open(checkpoint_file, 'wb') as f:\n",
        "            pickle.dump(checkpoint_data, f)\n",
        "\n",
        "        print(f\"💾 Checkpoint saved at file {current_index}\")\n",
        "\n",
        "    def load_checkpoint(self, output_dir: Path) -> int:\n",
        "        \"\"\"Load previous progress if available\"\"\"\n",
        "        checkpoint_file = output_dir / \"processing_checkpoint.pkl\"\n",
        "\n",
        "        if checkpoint_file.exists():\n",
        "            try:\n",
        "                with open(checkpoint_file, 'rb') as f:\n",
        "                    checkpoint_data = pickle.load(f)\n",
        "\n",
        "                self.batch_results = checkpoint_data.get('batch_results', {})\n",
        "                self.failed_files = checkpoint_data.get('failed_files', [])\n",
        "                self.processing_stats = checkpoint_data.get('processing_stats', self.processing_stats)\n",
        "\n",
        "                current_index = checkpoint_data.get('current_index', 0)\n",
        "                print(f\"📂 Loaded checkpoint: resuming from file {current_index + 1}\")\n",
        "                return current_index + 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not load checkpoint: {e}\")\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def process_batch_improved(self,\n",
        "                             pdf_directory: str = \"/content/drive/MyDrive/pdf\",\n",
        "                             output_drive_path: str = \"/content/drive/MyDrive/pharma_analysis\",\n",
        "                             batch_size: int = 5,\n",
        "                             max_files: Optional[int] = None,\n",
        "                             resume: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Improved batch processing with checkpoint support\n",
        "\n",
        "        Args:\n",
        "            pdf_directory: Directory containing PDF files\n",
        "            output_drive_path: Google Drive path to save results\n",
        "            batch_size: Number of files to process before saving checkpoint\n",
        "            max_files: Maximum number of files to process (None for all)\n",
        "            resume: Resume from checkpoint if available\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"🚀 Starting improved batch processing...\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Setup\n",
        "        self.processing_stats['start_time'] = datetime.now()\n",
        "\n",
        "        # Create output directory\n",
        "        output_dir = Path(output_drive_path)\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Create subdirectories\n",
        "        individual_dir = output_dir / \"individual_analyses\"\n",
        "        individual_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Load checkpoint if resuming\n",
        "        start_index = 0\n",
        "        if resume:\n",
        "            start_index = self.load_checkpoint(output_dir)\n",
        "\n",
        "        # Get PDF files\n",
        "        try:\n",
        "            pdf_files = self.get_pdf_files(pdf_directory)\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"❌ {e}\")\n",
        "            return {}\n",
        "\n",
        "        if not pdf_files:\n",
        "            print(\"❌ No PDF files found\")\n",
        "            return {}\n",
        "\n",
        "        # Limit files if specified\n",
        "        if max_files:\n",
        "            pdf_files = pdf_files[:max_files]\n",
        "\n",
        "        self.processing_stats['total_files'] = len(pdf_files)\n",
        "\n",
        "        print(f\"📁 Processing {len(pdf_files)} files starting from index {start_index}\")\n",
        "\n",
        "        # Process files starting from checkpoint\n",
        "        for i in range(start_index, len(pdf_files)):\n",
        "            pdf_path = pdf_files[i]\n",
        "\n",
        "            print(f\"\\n📊 Progress: {i+1}/{len(pdf_files)} ({(i+1)/len(pdf_files)*100:.1f}%)\")\n",
        "            print(f\"⏱️ Estimated time remaining: {self._estimate_time_remaining(i, len(pdf_files))} minutes\")\n",
        "\n",
        "            # Process single document\n",
        "            result = self.process_single_document_improved(pdf_path)\n",
        "\n",
        "            if result and result.get('status') == 'success':\n",
        "                file_key = Path(pdf_path).stem\n",
        "                self.batch_results[file_key] = result\n",
        "                self.processing_stats['successful'] += 1\n",
        "\n",
        "                # Save individual file\n",
        "                individual_file = individual_dir / f\"{file_key}_analysis.json\"\n",
        "                with open(individual_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            else:\n",
        "                self.failed_files.append(pdf_path)\n",
        "                self.processing_stats['failed'] += 1\n",
        "\n",
        "            # Save checkpoint every batch_size files\n",
        "            if (i + 1) % batch_size == 0:\n",
        "                self.save_checkpoint(output_dir, i)\n",
        "\n",
        "            # Show current stats\n",
        "            success_rate = (self.processing_stats['successful'] / (i + 1)) * 100\n",
        "            print(f\"📈 Current success rate: {success_rate:.1f}%\")\n",
        "\n",
        "            # Add delay to prevent overloading\n",
        "            time.sleep(2)\n",
        "\n",
        "        # Final save\n",
        "        self.processing_stats['end_time'] = datetime.now()\n",
        "        final_results_path = self.save_final_results_improved(output_dir)\n",
        "        self.generate_processing_report_improved(output_dir)\n",
        "\n",
        "        print(f\"\\n🎉 Batch processing completed!\")\n",
        "        self._show_batch_summary()\n",
        "\n",
        "        return self.batch_results\n",
        "\n",
        "    def get_pdf_files(self, pdf_directory: str) -> List[str]:\n",
        "        \"\"\"Get all PDF files from directory with better validation\"\"\"\n",
        "        pdf_dir = Path(pdf_directory)\n",
        "\n",
        "        if not pdf_dir.exists():\n",
        "            raise FileNotFoundError(f\"Directory not found: {pdf_directory}\")\n",
        "\n",
        "        # Find all PDF files\n",
        "        pdf_files = []\n",
        "        for pattern in ['*.pdf', '*.PDF']:\n",
        "            found_files = glob.glob(str(pdf_dir / pattern))\n",
        "            pdf_files.extend(found_files)\n",
        "\n",
        "        # Filter out very small or very large files\n",
        "        valid_files = []\n",
        "        for pdf_file in pdf_files:\n",
        "            try:\n",
        "                file_size = Path(pdf_file).stat().st_size\n",
        "                if 1000 < file_size < 50_000_000:  # Between 1KB and 50MB\n",
        "                    valid_files.append(pdf_file)\n",
        "                else:\n",
        "                    print(f\"⚠️ Skipping {Path(pdf_file).name} (size: {file_size:,} bytes)\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error checking {Path(pdf_file).name}: {e}\")\n",
        "\n",
        "        valid_files.sort()\n",
        "        print(f\"📁 Found {len(valid_files)} valid PDF files in {pdf_directory}\")\n",
        "\n",
        "        return valid_files\n",
        "\n",
        "    def save_final_results_improved(self, output_dir: Path) -> Path:\n",
        "        \"\"\"Save final results with better organization\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        final_file = output_dir / f\"batch_analysis_results_{timestamp}.json\"\n",
        "\n",
        "        # Create summary statistics\n",
        "        entity_counts = {}\n",
        "        total_entities = 0\n",
        "\n",
        "        for file_key, data in self.batch_results.items():\n",
        "            entities = data.get('entities', [])\n",
        "            total_entities += len(entities)\n",
        "\n",
        "            for entity in entities:\n",
        "                entity_type = entity.get('type', 'unknown')\n",
        "                entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1\n",
        "\n",
        "        final_data = {\n",
        "            \"batch_info\": {\n",
        "                \"processing_date\": datetime.now().isoformat(),\n",
        "                \"model_used\": self.model_name,\n",
        "                \"total_files_attempted\": self.processing_stats['total_files'],\n",
        "                \"successful_files\": self.processing_stats['successful'],\n",
        "                \"failed_files\": self.processing_stats['failed'],\n",
        "                \"success_rate_percent\": round((self.processing_stats['successful']/self.processing_stats['total_files'])*100, 1),\n",
        "                \"total_processing_time_minutes\": self._get_processing_time_minutes(),\n",
        "                \"total_entities_extracted\": total_entities\n",
        "            },\n",
        "            \"entity_statistics\": entity_counts,\n",
        "            \"failed_file_list\": [Path(f).name for f in self.failed_files],\n",
        "            \"document_results\": self.batch_results\n",
        "        }\n",
        "\n",
        "        with open(final_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(final_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"💾 Final results saved: {final_file}\")\n",
        "        return final_file\n",
        "\n",
        "    def generate_processing_report_improved(self, output_dir: Path):\n",
        "        \"\"\"Generate improved processing report\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        report_file = output_dir / f\"processing_report_{timestamp}.md\"\n",
        "\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"# Pharmaceutical Document Processing Report\\n\\n\")\n",
        "\n",
        "            f.write(f\"**Processing Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"**Model Used:** {self.model_name}\\n\")\n",
        "            f.write(f\"**Processing Time:** {self._get_processing_time_minutes():.1f} minutes\\n\\n\")\n",
        "\n",
        "            f.write(\"## Summary Statistics\\n\\n\")\n",
        "            total = self.processing_stats['total_files']\n",
        "            successful = self.processing_stats['successful']\n",
        "            failed = self.processing_stats['failed']\n",
        "\n",
        "            f.write(f\"- **Total files:** {total}\\n\")\n",
        "            f.write(f\"- **Successfully processed:** {successful} ({successful/total*100:.1f}%)\\n\")\n",
        "            f.write(f\"- **Failed:** {failed} ({failed/total*100:.1f}%)\\n\\n\")\n",
        "\n",
        "            if self.failed_files:\n",
        "                f.write(\"## Failed Files\\n\\n\")\n",
        "                for failed_file in self.failed_files:\n",
        "                    f.write(f\"- {Path(failed_file).name}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            f.write(\"## Successfully Processed Files\\n\\n\")\n",
        "            for file_key, data in self.batch_results.items():\n",
        "                metadata = data.get('metadata', {})\n",
        "                f.write(f\"### {metadata.get('file_name', file_key)}\\n\\n\")\n",
        "                f.write(f\"- **Text length:** {metadata.get('text_length', 0):,} characters\\n\")\n",
        "                f.write(f\"- **Entities extracted:** {metadata.get('entity_count', 0)}\\n\")\n",
        "                f.write(f\"- **Processing time:** {metadata.get('processing_time_seconds', 0):.1f}s\\n\\n\")\n",
        "\n",
        "        print(f\"📊 Processing report saved: {report_file}\")\n",
        "\n",
        "    def _estimate_time_remaining(self, current_index: int, total_files: int) -> float:\n",
        "        \"\"\"Estimate time remaining based on current progress\"\"\"\n",
        "        if current_index == 0:\n",
        "            return 0\n",
        "\n",
        "        elapsed_time = (datetime.now() - self.processing_stats['start_time']).total_seconds() / 60\n",
        "        avg_time_per_file = elapsed_time / (current_index + 1)\n",
        "        remaining_files = total_files - current_index - 1\n",
        "\n",
        "        return avg_time_per_file * remaining_files\n",
        "\n",
        "    def _get_processing_time_minutes(self) -> float:\n",
        "        \"\"\"Calculate processing time in minutes\"\"\"\n",
        "        if self.processing_stats['start_time'] and self.processing_stats['end_time']:\n",
        "            delta = self.processing_stats['end_time'] - self.processing_stats['start_time']\n",
        "            return delta.total_seconds() / 60\n",
        "        return 0\n",
        "\n",
        "    def _show_batch_summary(self):\n",
        "        \"\"\"Show improved batch processing summary\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"📊 BATCH PROCESSING SUMMARY\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        total = self.processing_stats['total_files']\n",
        "        successful = self.processing_stats['successful']\n",
        "        failed = self.processing_stats['failed']\n",
        "\n",
        "        print(f\"🤖 Model: {self.model_name}\")\n",
        "        print(f\"📁 Total files: {total}\")\n",
        "        print(f\"✅ Successful: {successful} ({successful/total*100:.1f}%)\")\n",
        "        print(f\"❌ Failed: {failed} ({failed/total*100:.1f}%)\")\n",
        "        print(f\"⏱️ Total time: {self._get_processing_time_minutes():.1f} minutes\")\n",
        "\n",
        "        if successful > 0:\n",
        "            avg_time = self._get_processing_time_minutes() / successful\n",
        "            print(f\"📈 Average time per file: {avg_time:.1f} minutes\")\n",
        "\n",
        "        total_entities = sum(len(data.get('entities', [])) for data in self.batch_results.values())\n",
        "        print(f\"🔍 Total entities extracted: {total_entities:,}\")\n",
        "\n",
        "        if self.failed_files:\n",
        "            print(f\"\\n❌ Failed files ({len(self.failed_files)}):\")\n",
        "            for failed_file in self.failed_files[:5]:  # Show first 5\n",
        "                print(f\"   - {Path(failed_file).name}\")\n",
        "            if len(self.failed_files) > 5:\n",
        "                print(f\"   ... and {len(self.failed_files) - 5} more\")\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    def quick_search(self, query: str, limit: int = 5) -> List[Dict]:\n",
        "        \"\"\"Quick search across processed results\"\"\"\n",
        "        if not self.batch_results:\n",
        "            print(\"❌ No processed results available\")\n",
        "            return []\n",
        "\n",
        "        query_words = query.lower().split()\n",
        "        results = []\n",
        "\n",
        "        for file_key, data in self.batch_results.items():\n",
        "            score = 0\n",
        "            matches = []\n",
        "\n",
        "            # Search in entities\n",
        "            entities = data.get('entities', [])\n",
        "            for entity in entities:\n",
        "                entity_text = f\"{entity.get('name', '')} {entity.get('value', '')}\".lower()\n",
        "                for word in query_words:\n",
        "                    if word in entity_text:\n",
        "                        score += 1\n",
        "                        matches.append(entity)\n",
        "\n",
        "            # Search in summary\n",
        "            summary = data.get('summary', {})\n",
        "            summary_text = json.dumps(summary).lower()\n",
        "            for word in query_words:\n",
        "                score += summary_text.count(word)\n",
        "\n",
        "            if score > 0:\n",
        "                results.append({\n",
        "                    'file_name': data['metadata']['file_name'],\n",
        "                    'score': score,\n",
        "                    'relevant_entities': matches[:3],  # Top 3 matches\n",
        "                    'summary': summary\n",
        "                })\n",
        "\n",
        "        # Sort by relevance\n",
        "        results.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return results[:limit]\n",
        "\n",
        "\n",
        "# Improved utility functions\n",
        "\n",
        "def setup_improved_environment():\n",
        "    \"\"\"Setup improved environment for Colab\"\"\"\n",
        "    print(\"🔧 Setting up improved environment...\")\n",
        "\n",
        "    # Install packages\n",
        "    packages = [\"pymupdf4llm\", \"pdfplumber\"]\n",
        "    for package in packages:\n",
        "        try:\n",
        "            os.system(f\"pip install -q {package}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: {package} installation issue: {e}\")\n",
        "\n",
        "    # Check Ollama\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"--version\"], capture_output=True, timeout=10)\n",
        "        if result.returncode == 0:\n",
        "            print(\"✅ Ollama available\")\n",
        "        else:\n",
        "            print(\"📦 Installing Ollama...\")\n",
        "            os.system(\"curl -fsSL https://ollama.ai/install.sh | sh\")\n",
        "    except:\n",
        "        print(\"📦 Installing Ollama...\")\n",
        "        os.system(\"curl -fsSL https://ollama.ai/install.sh | sh\")\n",
        "\n",
        "    print(\"✅ Environment ready!\")\n",
        "\n",
        "def process_pharma_batch_safe(pdf_directory: str = \"/content/drive/MyDrive/pdf\",\n",
        "                            output_path: str = \"/content/drive/MyDrive/pharma_analysis\",\n",
        "                            max_files: int = 10,\n",
        "                            model: str = \"llama3.2:3b\"):\n",
        "    \"\"\"\n",
        "    Safe batch processing function with limits\n",
        "\n",
        "    Args:\n",
        "        pdf_directory: Input directory with PDFs\n",
        "        output_path: Output directory on Google Drive\n",
        "        max_files: Maximum files to process (to prevent overload)\n",
        "        model: Ollama model to use\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"🚀 Starting safe batch processing (max {max_files} files)\")\n",
        "\n",
        "    parser = ImprovedBatchPharmaParser(model_name=model)\n",
        "\n",
        "    results = parser.process_batch_improved(\n",
        "        pdf_directory=pdf_directory,\n",
        "        output_drive_path=output_path,\n",
        "        batch_size=3,  # Save progress every 3 files\n",
        "        max_files=max_files,\n",
        "        resume=True  # Resume from checkpoint if available\n",
        "    )\n",
        "\n",
        "    return parser, results\n",
        "\n",
        "def search_results(parser, query: str):\n",
        "    \"\"\"Search through processed results\"\"\"\n",
        "    if not hasattr(parser, 'batch_results'):\n",
        "        print(\"❌ No processed results available\")\n",
        "        return\n",
        "\n",
        "    results = parser.quick_search(query, limit=5)\n",
        "\n",
        "    if not results:\n",
        "        print(f\"❌ No results found for: {query}\")\n",
        "        return\n",
        "\n",
        "    print(f\"🔍 Search results for '{query}':\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"\\n{i}. **{result['file_name']}** (relevance: {result['score']})\")\n",
        "\n",
        "        # Show relevant entities\n",
        "        if result['relevant_entities']:\n",
        "            print(\"   Relevant info:\")\n",
        "            for entity in result['relevant_entities']:\n",
        "                print(f\"   - {entity.get('name', 'N/A')}: {entity.get('value', 'N/A')}\")\n",
        "\n",
        "        # Show summary if available\n",
        "        summary = result.get('summary', {})\n",
        "        if summary and summary.get('medication_name'):\n",
        "            print(f\"   Medication: {summary.get('medication_name')}\")\n",
        "            print(f\"   Use: {summary.get('main_use', 'N/A')}\")\n",
        "\n",
        "def show_processing_stats(parser):\n",
        "    \"\"\"Show detailed processing statistics\"\"\"\n",
        "    if not hasattr(parser, 'processing_stats'):\n",
        "        print(\"❌ No processing stats available\")\n",
        "        return\n",
        "\n",
        "    stats = parser.processing_stats\n",
        "    results = parser.batch_results\n",
        "\n",
        "    print(\"📊 PROCESSING STATISTICS\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Files processed: {stats['successful']}/{stats['total_files']}\")\n",
        "    print(f\"Success rate: {(stats['successful']/stats['total_files']*100):.1f}%\")\n",
        "    print(f\"Total time: {parser._get_processing_time_minutes():.1f} minutes\")\n",
        "\n",
        "    if results:\n",
        "        total_entities = sum(len(data.get('entities', [])) for data in results.values())\n",
        "        avg_entities = total_entities / len(results) if results else 0\n",
        "        print(f\"Total entities: {total_entities:,}\")\n",
        "        print(f\"Average entities per file: {avg_entities:.1f}\")\n",
        "\n",
        "# Example usage for Google Colab\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Step 1: Setup environment\n",
        "    setup_improved_environment()\n",
        "\n",
        "    # Step 2: Process documents safely (start with small batch)\n",
        "    print(\"\\n🚀 Starting pharmaceutical document processing...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Process first 5 files as a test\n",
        "    parser, results = process_pharma_batch_safe(\n",
        "        pdf_directory=\"/content/drive/MyDrive/pdf\",\n",
        "        output_path=\"/content/drive/MyDrive/pharma_analysis\",\n",
        "        max_files=5,  # Start small to test\n",
        "        model=\"llama3.2:3b\"\n",
        "    )\n",
        "\n",
        "    # Step 3: Show results\n",
        "    show_processing_stats(parser)\n",
        "\n",
        "    # Step 4: Test search functionality\n",
        "    if results:\n",
        "        print(\"\\n🔍 Testing search functionality...\")\n",
        "        search_results(parser, \"dosagem\")\n",
        "        search_results(parser, \"contraindicação\")\n",
        "        search_results(parser, \"efeitos colaterais\")\n",
        "\n",
        "    print(\"\\n✅ Processing complete! Check your Google Drive for results.\")\n",
        "\n",
        "# Additional helper functions\n",
        "\n",
        "def continue_processing(parser, max_additional_files: int = 20):\n",
        "    \"\"\"Continue processing more files from where we left off\"\"\"\n",
        "    return parser.process_batch_improved(\n",
        "        max_files=parser.processing_stats['total_files'] + max_additional_files,\n",
        "        resume=True\n",
        "    )\n",
        "\n",
        "def export_to_excel(parser, output_path: str = \"/content/drive/MyDrive/pharma_analysis/entities.xlsx\"):\n",
        "    \"\"\"Export all entities to Excel file\"\"\"\n",
        "    try:\n",
        "        import pandas as pd\n",
        "\n",
        "        all_entities = []\n",
        "        for file_key, data in parser.batch_results.items():\n",
        "            file_name = data['metadata']['file_name']\n",
        "            entities = data.get('entities', [])\n",
        "\n",
        "            for entity in entities:\n",
        "                all_entities.append({\n",
        "                    'source_file': file_name,\n",
        "                    'entity_name': entity.get('name', ''),\n",
        "                    'entity_type': entity.get('type', ''),\n",
        "                    'entity_value': entity.get('value', ''),\n",
        "                    'processing_date': data['metadata']['processing_date']\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(all_entities)\n",
        "        df.to_excel(output_path, index=False)\n",
        "\n",
        "        print(f\"📊 Exported {len(all_entities)} entities to Excel: {output_path}\")\n",
        "        return output_path\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ pandas not available. Install with: !pip install pandas openpyxl\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Export failed: {e}\")\n",
        "\n",
        "def cleanup_checkpoints(output_dir: str = \"/content/drive/MyDrive/pharma_analysis\"):\n",
        "    \"\"\"Clean up checkpoint files\"\"\"\n",
        "    output_path = Path(output_dir)\n",
        "    checkpoint_files = list(output_path.glob(\"processing_checkpoint.pkl\"))\n",
        "    interim_files = list(output_path.glob(\"interim_results_*.json\"))\n",
        "\n",
        "    for file in checkpoint_files + interim_files:\n",
        "        try:\n",
        "            file.unlink()\n",
        "            print(f\"🗑️ Removed checkpoint: {file.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not remove {file.name}: {e}\")\n",
        "\n",
        "print(\"🎉 Improved Pharmaceutical Document Parser loaded!\")\n",
        "print(\"📝 Usage:\")\n",
        "print(\"1. setup_improved_environment()\")\n",
        "print(\"2. parser, results = process_pharma_batch_safe(max_files=5)\")\n",
        "print(\"3. search_results(parser, 'your_search_term')\")\n",
        "print(\"4. export_to_excel(parser)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "helloouu"
      ],
      "metadata": {
        "id": "jPONESl6WkLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Enhanced Pharmaceutical Knowledge Graph Extractor for Google Colab - Phrase-Based JSON Processing\n",
        "\n",
        "Processes phrase-optimized JSON files from the enhanced pharmaceutical document parser.\n",
        "Optimized for small language models with robust extraction and error handling.\n",
        "Includes detailed logging of prompts and responses for debugging model performance.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from google.colab import drive\n",
        "\n",
        "# ==============================================================================\n",
        "# CORE LOGIC: Enhanced PharmaceuticalKnowledgeExtractor CLASS\n",
        "# ==============================================================================\n",
        "\n",
        "class EnhancedPharmaceuticalKnowledgeExtractor:\n",
        "    def __init__(self,\n",
        "                 model_name: str = \"llama3:8b\", # Default is now 8B\n",
        "                 ollama_url: str = \"http://localhost:11434/api/generate\",\n",
        "                 max_retries: int = 3,\n",
        "                 request_delay: float = 0.5):\n",
        "        \"\"\"\n",
        "        Initialize the enhanced knowledge extractor optimized for phrase-based JSON files.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.ollama_url = ollama_url\n",
        "        self.max_retries = max_retries\n",
        "        self.request_delay = request_delay\n",
        "\n",
        "        self.stats = {\n",
        "            'files_processed': 0,\n",
        "            'phrase_blocks_processed': 0,\n",
        "            'table_blocks_processed': 0,\n",
        "            'phrases_processed': 0,\n",
        "            'successful_extractions': 0,\n",
        "            'failed_extractions': 0,\n",
        "            'total_triples': 0,\n",
        "            'skipped_irrelevant': 0\n",
        "        }\n",
        "\n",
        "        # Enhanced patterns for better pharmaceutical content detection\n",
        "        self.pharma_keywords = [\n",
        "            'mg', 'ml', 'g/', 'mcg', 'μg', '%', 'dose', 'dosagem', 'posologia',\n",
        "            'comprimido', 'cápsula', 'medicamento', 'fármaco', 'droga',\n",
        "            'indicação', 'indicado', 'tratamento', 'terapia',\n",
        "            'contraindicação', 'contraindicado', 'não usar', 'evitar',\n",
        "            'efeito', 'reação', 'adverso', 'colateral', 'indesejável',\n",
        "            'alergia', 'hipersensibilidade', 'intolerância',\n",
        "            'administração', 'aplicar', 'tomar', 'ingerir',\n",
        "            'composição', 'princípio ativo', 'substância', 'excipiente',\n",
        "            'interação', 'interagir', 'incompatível', 'interferir',\n",
        "            'gravidez', 'gestação', 'lactação', 'amamentação',\n",
        "            'criança', 'pediátrico', 'adulto', 'idoso', 'geriátrico'\n",
        "        ]\n",
        "\n",
        "        self._setup_logging()\n",
        "        self._test_ollama_connection()\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        \"\"\"Setup enhanced logging configuration.\"\"\"\n",
        "        # Remove existing handlers to avoid duplicates in Colab\n",
        "        for handler in logging.root.handlers[:]:\n",
        "            logging.root.removeHandler(handler)\n",
        "\n",
        "        # General logger for progress and errors\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler('enhanced_pharma_extraction.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Dedicated logger for prompts and responses\n",
        "        self.prompt_logger = logging.getLogger('prompt_logger')\n",
        "        self.prompt_logger.setLevel(logging.INFO)\n",
        "        prompt_handler = logging.FileHandler('enhanced_prompts_and_responses.log', mode='w')\n",
        "        prompt_formatter = logging.Formatter('%(message)s')\n",
        "        prompt_handler.setFormatter(prompt_formatter)\n",
        "\n",
        "        # Avoid adding handlers if they already exist\n",
        "        if not self.prompt_logger.handlers:\n",
        "            self.prompt_logger.addHandler(prompt_handler)\n",
        "\n",
        "    def _test_ollama_connection(self):\n",
        "        \"\"\"Test connection to Ollama API with enhanced error reporting.\"\"\"\n",
        "        try:\n",
        "            test_payload = {\n",
        "                \"model\": self.model_name,\n",
        "                \"prompt\": \"Teste de conexão. Responda apenas 'OK'.\",\n",
        "                \"stream\": False,\n",
        "                \"format\": \"json\",\n",
        "                \"options\": {\"temperature\": 0.0, \"num_predict\": 10}\n",
        "            }\n",
        "            response = requests.post(self.ollama_url, json=test_payload, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                self.logger.info(f\"✅ Successfully connected to Ollama with {self.model_name}\")\n",
        "                result = response.json()\n",
        "                self.logger.debug(f\"Test response: {result.get('response', 'No response')}\")\n",
        "            else:\n",
        "                self.logger.warning(f\"⚠️ Ollama connection test failed: {response.status_code} - {response.text}\")\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            self.logger.error(\"❌ Cannot connect to Ollama API. Ensure it's running and accessible from this Colab notebook.\")\n",
        "            raise ConnectionError(\"Cannot connect to Ollama API. Ensure it's running and accessible (e.g., via ngrok).\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"❌ Failed to connect to Ollama: {e}\")\n",
        "            raise ConnectionError(f\"Ollama connection failed: {e}\")\n",
        "\n",
        "    def _call_ollama_api(self, prompt: str, max_tokens: int = 200) -> Optional[str]:\n",
        "        \"\"\"Enhanced API call with better error handling and retry logic.\"\"\"\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False,\n",
        "            \"format\": \"json\",\n",
        "            \"options\": {\n",
        "                \"temperature\": 0.0,  # Deterministic for structured output\n",
        "                \"top_p\": 0.9,\n",
        "                \"top_k\": 20,\n",
        "                \"num_predict\": max_tokens,\n",
        "                \"stop\": [\"\\n\\n\", \"---\", \"Exemplos:\", \"Examples:\", \"Nota:\", \"Note:\"],\n",
        "                \"repeat_penalty\": 1.1,\n",
        "                \"num_ctx\": 4096,  # Context window for 8B model\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                self.logger.debug(f\"API call attempt {attempt + 1}/{self.max_retries}\")\n",
        "                response = requests.post(self.ollama_url, json=payload, timeout=120)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    response_text = result.get('response', '').strip()\n",
        "                    if response_text:\n",
        "                        return response_text\n",
        "                    else:\n",
        "                        self.logger.warning(\"Empty response from API\")\n",
        "                else:\n",
        "                    self.logger.warning(f\"API error {response.status_code}: {response.text[:200]}...\")\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                self.logger.warning(f\"Request timeout on attempt {attempt + 1}\")\n",
        "            except requests.exceptions.ConnectionError:\n",
        "                self.logger.warning(f\"Connection error on attempt {attempt + 1}\")\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Request error on attempt {attempt + 1}: {e}\")\n",
        "\n",
        "            if attempt < self.max_retries - 1:\n",
        "                wait_time = (2 ** attempt) + self.request_delay\n",
        "                self.logger.debug(f\"Waiting {wait_time:.1f}s before retry...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "        self.logger.error(\"All API call attempts failed\")\n",
        "        return None\n",
        "\n",
        "    def _create_enhanced_extraction_prompt(self, phrase: str, context: Dict, phrase_type: str = None) -> str:\n",
        "        \"\"\"Create an enhanced, more specific prompt for small language models.\"\"\"\n",
        "        section_info = context.get('breadcrumb', 'Seção Desconhecida')\n",
        "        phrase_category = phrase_type or context.get('metadata', {}).get('phrase_type', 'geral')\n",
        "\n",
        "        # Create more specific instructions based on phrase type\n",
        "        specific_instructions = {\n",
        "            'dosage_instruction': 'Foque em doses, quantidades, frequências de administração.',\n",
        "            'indication': 'Extraia para que condições ou doenças o medicamento é indicado.',\n",
        "            'contraindication': 'Identifique quando o medicamento NÃO deve ser usado.',\n",
        "            'side_effect': 'Extraia efeitos adversos, reações indesejáveis.',\n",
        "            'precaution': 'Identifique cuidados, precauções, advertências.',\n",
        "            'numerical_data': 'Extraia dados numéricos relevantes (doses, concentrações).',\n",
        "            'general_information': 'Extraia qualquer informação farmacêutica relevante.'\n",
        "        }\n",
        "\n",
        "        instruction = specific_instructions.get(phrase_category, specific_instructions['general_information'])\n",
        "\n",
        "        return f\"\"\"Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
        "\n",
        "CONTEXTO: {section_info}\n",
        "TIPO: {phrase_category}\n",
        "INSTRUÇÃO: {instruction}\n",
        "\n",
        "FRASE: \"{phrase}\"\n",
        "\n",
        "REGRAS IMPORTANTES:\n",
        "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
        "2. NÃO invente ou suponha informações\n",
        "3. Use nomes de medicamentos exatos quando mencionados\n",
        "4. Para doses, inclua unidades (mg, ml, etc.)\n",
        "5. Se não há informação farmacêutica específica, retorne []\n",
        "\n",
        "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
        "\n",
        "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
        "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
        "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
        "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "    def _parse_triples_response_enhanced(self, response: str) -> List[List[str]]:\n",
        "        \"\"\"Enhanced parsing with better error handling and validation.\"\"\"\n",
        "        if not response:\n",
        "            return []\n",
        "\n",
        "        # Find the JSON array within the response string\n",
        "        match = re.search(r'\\[\\s*(\\[.*?\\])\\s*\\]', response, re.DOTALL)\n",
        "        if not match:\n",
        "            # Fallback for single triple\n",
        "            match = re.search(r'(\\[.*?\\])', response, re.DOTALL)\n",
        "\n",
        "        if not match:\n",
        "            self.logger.warning(f\"Could not find a valid JSON array/object in response: {response[:100]}...\")\n",
        "            return []\n",
        "\n",
        "        clean_json_str = match.group(0)\n",
        "\n",
        "        try:\n",
        "            # First, try to load as is, assuming it's a list of lists\n",
        "            data = json.loads(clean_json_str)\n",
        "            if isinstance(data, list) and all(isinstance(item, list) and len(item) == 3 for item in data):\n",
        "                return self._validate_and_filter_triples(data)\n",
        "            # Handle if it's a single list that should be wrapped in another list\n",
        "            if isinstance(data, list) and len(data) == 3 and all(isinstance(item, str) for item in data):\n",
        "                 return self._validate_and_filter_triples([data])\n",
        "        except json.JSONDecodeError:\n",
        "            self.logger.warning(f\"Could not parse response: {clean_json_str[:100]}...\")\n",
        "            return []\n",
        "\n",
        "        return []\n",
        "\n",
        "\n",
        "    def _validate_and_filter_triples(self, triples: List[List[str]]) -> List[List[str]]:\n",
        "        \"\"\"Validate and filter extracted triples for quality.\"\"\"\n",
        "        valid_triples = []\n",
        "        template_entities = ['medication', 'medicamento', 'paracetamol', 'substância', 'fármaco']\n",
        "        template_values = ['500mg', 'comprimidos', 'dor de cabeça', 'náusea', 'exemplo']\n",
        "\n",
        "        for triple in triples:\n",
        "            if not isinstance(triple, list) or len(triple) != 3:\n",
        "                continue\n",
        "\n",
        "            entity, relation, value = [str(x).strip() for x in triple]\n",
        "\n",
        "            if not all([entity, relation, value]) or any(len(x) < 2 for x in [entity, relation, value]):\n",
        "                continue\n",
        "\n",
        "            if (entity.lower() in template_entities and\n",
        "                any(tv in value.lower() for tv in template_values)):\n",
        "                continue\n",
        "\n",
        "            if any(x.startswith('<') and x.endswith('>') for x in [entity, relation, value]):\n",
        "                continue\n",
        "\n",
        "            generic_relations = ['é', 'tem', 'faz', 'usa']\n",
        "            if relation.lower() in generic_relations and len(value) < 5:\n",
        "                continue\n",
        "\n",
        "            valid_triples.append([entity, relation, value])\n",
        "\n",
        "        return valid_triples\n",
        "\n",
        "    def _should_process_phrase(self, phrase: str, metadata: Dict = None) -> bool:\n",
        "        \"\"\"Enhanced logic to determine if a phrase should be processed.\"\"\"\n",
        "        if len(phrase.strip()) < 15:\n",
        "            return False\n",
        "\n",
        "        phrase_lower = phrase.lower()\n",
        "        has_pharma_content = any(kw in phrase_lower for kw in self.pharma_keywords)\n",
        "        if metadata:\n",
        "            phrase_type = metadata.get('phrase_type', '')\n",
        "            if phrase_type in ['dosage_instruction', 'indication', 'contraindication', 'side_effect']:\n",
        "                return True\n",
        "\n",
        "        has_numbers = bool(re.search(r'\\d', phrase))\n",
        "        has_units = bool(re.search(r'\\d+\\s*(mg|ml|g|%|mcg|μg)', phrase_lower))\n",
        "\n",
        "        return has_pharma_content or has_units or (has_numbers and len(phrase) > 30)\n",
        "\n",
        "    def _extract_phrase_knowledge(self, phrase_data: Dict) -> Dict:\n",
        "        \"\"\"Extract knowledge from a single phrase block with enhanced processing.\"\"\"\n",
        "        phrase_id = phrase_data.get('phrase_id', 'unknown')\n",
        "        phrase_content = phrase_data.get('content', '').strip()\n",
        "        context = phrase_data.get('context', {})\n",
        "        metadata = phrase_data.get('metadata', {})\n",
        "\n",
        "        if not self._should_process_phrase(phrase_content, metadata):\n",
        "            self.stats['skipped_irrelevant'] += 1\n",
        "            return {\n",
        "                'phrase_id': phrase_id,\n",
        "                'triples': [],\n",
        "                'status': 'skipped_irrelevant'\n",
        "            }\n",
        "\n",
        "        self.logger.debug(f\"Processing phrase {phrase_id}: {phrase_content[:50]}...\")\n",
        "        self.stats['phrases_processed'] += 1\n",
        "\n",
        "        try:\n",
        "            phrase_type = metadata.get('phrase_type')\n",
        "            prompt = self._create_enhanced_extraction_prompt(phrase_content, context, phrase_type)\n",
        "\n",
        "            self.prompt_logger.info(f\"--- START PHRASE: {phrase_id} ---\")\n",
        "            self.prompt_logger.info(f\"PHRASE TEXT: {phrase_content}\")\n",
        "            self.prompt_logger.info(f\"PHRASE TYPE: {phrase_type}\")\n",
        "            self.prompt_logger.info(f\"CONTEXT: {context.get('breadcrumb', 'N/A')}\")\n",
        "            self.prompt_logger.info(f\"PROMPT SENT:\\n{prompt}\")\n",
        "\n",
        "            response = self._call_ollama_api(prompt, max_tokens=300)\n",
        "\n",
        "            self.prompt_logger.info(f\"RAW RESPONSE RECEIVED:\\n{response}\")\n",
        "            self.prompt_logger.info(f\"--- END PHRASE: {phrase_id} ---\\n\")\n",
        "\n",
        "            if response:\n",
        "                triples = self._parse_triples_response_enhanced(response)\n",
        "                if triples:\n",
        "                    self.stats['successful_extractions'] += 1\n",
        "                    self.stats['total_triples'] += len(triples)\n",
        "                    self.logger.debug(f\"✅ Extracted {len(triples)} triples from phrase {phrase_id}\")\n",
        "                else:\n",
        "                    self.stats['failed_extractions'] += 1\n",
        "\n",
        "                return {\n",
        "                    'phrase_id': phrase_id,\n",
        "                    'phrase_text': phrase_content,\n",
        "                    'phrase_type': phrase_type,\n",
        "                    'context': context.get('breadcrumb'),\n",
        "                    'triples': triples,\n",
        "                    'status': 'success' if triples else 'no_triples_found'\n",
        "                }\n",
        "            else:\n",
        "                self.stats['failed_extractions'] += 1\n",
        "                return {\n",
        "                    'phrase_id': phrase_id,\n",
        "                    'phrase_text': phrase_content,\n",
        "                    'triples': [],\n",
        "                    'status': 'api_failed'\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing phrase {phrase_id}: {e}\")\n",
        "            self.stats['failed_extractions'] += 1\n",
        "            return {\n",
        "                'phrase_id': phrase_id,\n",
        "                'phrase_text': phrase_content,\n",
        "                'triples': [],\n",
        "                'status': f'error: {str(e)}'\n",
        "            }\n",
        "        finally:\n",
        "            time.sleep(self.request_delay)\n",
        "\n",
        "    # ... (Keep other methods like _extract_table_knowledge if you need them) ...\n",
        "\n",
        "    def process_phrase_based_json(self, input_file: Path) -> Optional[Dict]:\n",
        "        \"\"\"Process a phrase-based JSON file from the enhanced parser.\"\"\"\n",
        "        self.logger.info(f\"📄 Processing phrase-based file: {input_file.name}\")\n",
        "        try:\n",
        "            with open(input_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load {input_file}: {e}\")\n",
        "            return None\n",
        "\n",
        "        phrase_blocks = data.get('document_structure', {}).get('phrase_blocks', [])\n",
        "        if not phrase_blocks:\n",
        "            self.logger.warning(f\"No phrase_blocks found in {input_file}\")\n",
        "            return None\n",
        "        self.logger.info(f\"Found {len(phrase_blocks)} phrase blocks\")\n",
        "\n",
        "        phrase_extractions = []\n",
        "        for phrase_data in phrase_blocks:\n",
        "            result = self._extract_phrase_knowledge(phrase_data)\n",
        "            phrase_extractions.append(result)\n",
        "            self.stats['phrase_blocks_processed'] += 1\n",
        "\n",
        "        all_triples = [triple for extraction in phrase_extractions for triple in extraction.get('triples', [])]\n",
        "\n",
        "        result = {\n",
        "            'document_metadata': data.get('document_metadata', {}),\n",
        "            'extraction_summary': {\n",
        "                'extraction_timestamp': datetime.now().isoformat(),\n",
        "                'model_used': self.model_name,\n",
        "                'total_triples_extracted': len(all_triples),\n",
        "            },\n",
        "            'phrase_extractions': phrase_extractions,\n",
        "            'all_extracted_triples': all_triples,\n",
        "        }\n",
        "\n",
        "        self.stats['files_processed'] += 1\n",
        "        self.logger.info(f\"✅ Completed {input_file.name}: {len(all_triples)} total triples extracted\")\n",
        "        return result\n",
        "\n",
        "    def process_directory(self, input_dir: Path, output_dir: Path):\n",
        "        \"\"\"Process all phrase-optimized JSON files in a directory.\"\"\"\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        json_files = sorted(list(input_dir.glob('*_phrase_optimized.json')))\n",
        "        if not json_files:\n",
        "            self.logger.warning(f\"No *_phrase_optimized.json files found in {input_dir}\")\n",
        "            return\n",
        "\n",
        "        self.logger.info(f\"Found {len(json_files)} phrase-optimized files to process\")\n",
        "\n",
        "        for i, json_file in enumerate(json_files):\n",
        "            self.logger.info(f\"\\n📊 Progress: Processing file {i + 1}/{len(json_files)}\")\n",
        "            result = self.process_phrase_based_json(json_file)\n",
        "            if result:\n",
        "                output_name = json_file.stem.replace('_phrase_optimized', '_enhanced_graph_data') + '.json'\n",
        "                output_file = output_dir / output_name\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "                self.logger.info(f\"💾 Saved results to: {output_file}\")\n",
        "\n",
        "        self._generate_enhanced_report(output_dir)\n",
        "\n",
        "    def _generate_enhanced_report(self, output_dir: Path):\n",
        "        \"\"\"Generate comprehensive final report.\"\"\"\n",
        "        # ... (This method can be kept as is) ...\n",
        "        pass\n",
        "\n",
        "    def _print_summary_stats(self):\n",
        "        \"\"\"Print summary statistics to console.\"\"\"\n",
        "        # ... (This method can be kept as is) ...\n",
        "        pass\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 🚀 MAIN EXECUTION SECTION\n",
        "# ==============================================================================\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# IMPORTANT: Change these paths to match your Google Drive folders.\n",
        "DRIVE_INPUT_DIR = \"processed_pdfs\"\n",
        "DRIVE_OUTPUT_DIR = \"enhanced_graph_data\"\n",
        "\n",
        "# --- CHANGE MADE HERE ---\n",
        "# Switched from \"llama3.2:3b\" to \"llama3:8b\" for better performance.\n",
        "OLLAMA_MODEL = \"llama3:8b\"\n",
        "\n",
        "REQUEST_DELAY = 0.5\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with enhanced error handling and logging.\"\"\"\n",
        "    print(\"🚀 Starting Enhanced Pharmaceutical Knowledge Graph Extractor\")\n",
        "    try:\n",
        "        print(\"🔧 Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"✅ Google Drive mounted successfully.\")\n",
        "\n",
        "        drive_base_path = Path('/content/drive/MyDrive/')\n",
        "        full_input_path = drive_base_path / DRIVE_INPUT_DIR\n",
        "        full_output_path = drive_base_path / DRIVE_OUTPUT_DIR\n",
        "\n",
        "        print(f\"📁 Input Directory: {full_input_path}\")\n",
        "        print(f\"📂 Output Directory: {full_output_path}\")\n",
        "        print(f\"🤖 Model: {OLLAMA_MODEL}\")\n",
        "\n",
        "        if not full_input_path.exists():\n",
        "            print(f\"❌ ERROR: Input directory not found at '{full_input_path}'\")\n",
        "            return\n",
        "\n",
        "        print(\"🤖 Initializing enhanced knowledge extractor...\")\n",
        "        extractor = EnhancedPharmaceuticalKnowledgeExtractor(\n",
        "            model_name=OLLAMA_MODEL,\n",
        "            max_retries=MAX_RETRIES,\n",
        "            request_delay=REQUEST_DELAY\n",
        "        )\n",
        "\n",
        "        print(\"🚀 Starting batch processing...\")\n",
        "        extractor.process_directory(\n",
        "            input_dir=full_input_path,\n",
        "            output_dir=full_output_path\n",
        "        )\n",
        "\n",
        "        print(\"\\n🎉 All files processed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QQFfi5owWl2H",
        "outputId": "c76bd29f-8db0-4f1f-e5ab-8cfc4a94aac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Enhanced Pharmaceutical Knowledge Graph Extractor\n",
            "🔧 Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted successfully.\n",
            "📁 Input Directory: /content/drive/MyDrive/processed_pdfs\n",
            "📂 Output Directory: /content/drive/MyDrive/enhanced_graph_data\n",
            "🤖 Model: llama3:8b\n",
            "🤖 Initializing enhanced knowledge extractor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-01 17:54:37,662 - INFO - ✅ Successfully connected to Ollama with llama3:8b\n",
            "2025-09-01 17:54:37,719 - INFO - Found 339 phrase-optimized files to process\n",
            "2025-09-01 17:54:37,719 - INFO - \n",
            "📊 Progress: Processing file 1/339\n",
            "2025-09-01 17:54:37,721 - INFO - 📄 Processing phrase-based file: bula_1755192077396_phrase_optimized.json\n",
            "2025-09-01 17:54:37,737 - INFO - Found 120 phrase blocks\n",
            "2025-09-01 17:54:37,738 - INFO - --- START PHRASE: phrase_2 ---\n",
            "2025-09-01 17:54:37,739 - INFO - PHRASE TEXT: O colesterol é uma das várias substâncias gordurosas encontradas na corrente sanguínea.\n",
            "2025-09-01 17:54:37,741 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:54:37,742 - INFO - CONTEXT: Section 1: PARA QUE ESTE MEDICAMENTO É INDICADO?\n",
            "2025-09-01 17:54:37,743 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 1: PARA QUE ESTE MEDICAMENTO É INDICADO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"O colesterol é uma das várias substâncias gordurosas encontradas na corrente sanguínea.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting batch processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-01 17:54:38,840 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"colesterol\": \"é_substância_gordurosa_encontrada_na_corrente_sanguínea\"}\n",
            "2025-09-01 17:54:38,841 - INFO - --- END PHRASE: phrase_2 ---\n",
            "\n",
            "2025-09-01 17:54:38,843 - WARNING - Could not find a valid JSON array/object in response: {\"colesterol\": \"é_substância_gordurosa_encontrada_na_corrente_sanguínea\"}...\n",
            "2025-09-01 17:54:39,344 - INFO - --- START PHRASE: phrase_7 ---\n",
            "2025-09-01 17:54:39,345 - INFO - PHRASE TEXT: O colesterol HDL, por sua vez, é frequentemente chamado de “bom colesterol\" porque ajuda a evitar o depósito de “mau colesterol\" nas artérias e protege contra doenças do coração.\n",
            "2025-09-01 17:54:39,346 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:54:39,347 - INFO - CONTEXT: Section 1: PARA QUE ESTE MEDICAMENTO É INDICADO?\n",
            "2025-09-01 17:54:39,347 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 1: PARA QUE ESTE MEDICAMENTO É INDICADO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"O colesterol HDL, por sua vez, é frequentemente chamado de “bom colesterol\" porque ajuda a evitar o depósito de “mau colesterol\" nas artérias e protege contra doenças do coração.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:40,552 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"colesterol HDL\", \"relação\": \"é_chamado_de\", \"valor\": \"bom colesterol\" }\n",
            "2025-09-01 17:54:40,553 - INFO - --- END PHRASE: phrase_7 ---\n",
            "\n",
            "2025-09-01 17:54:40,554 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"colesterol HDL\", \"relação\": \"é_chamado_de\", \"valor\": \"bom colesterol\" }...\n",
            "2025-09-01 17:54:41,055 - INFO - --- START PHRASE: phrase_12 ---\n",
            "2025-09-01 17:54:41,056 - INFO - PHRASE TEXT: Portanto, ezetimiba aumenta o efeito redutor do colesterol das vastatinas e do fenofibrato.\n",
            "2025-09-01 17:54:41,057 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:54:41,058 - INFO - CONTEXT: Section 2: COMO ESTE MEDICAMENTO FUNCIONA?\n",
            "2025-09-01 17:54:41,058 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: COMO ESTE MEDICAMENTO FUNCIONA?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Portanto, ezetimiba aumenta o efeito redutor do colesterol das vastatinas e do fenofibrato.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:42,239 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"aumenta\", \"valor\": \"o efeito redutor do colesterol\"}\n",
            "2025-09-01 17:54:42,240 - INFO - --- END PHRASE: phrase_12 ---\n",
            "\n",
            "2025-09-01 17:54:42,241 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"ezetimiba\", \"relação\": \"aumenta\", \"valor\": \"o efeito redutor do colesterol\"}...\n",
            "2025-09-01 17:54:42,742 - INFO - --- START PHRASE: phrase_15 ---\n",
            "2025-09-01 17:54:42,743 - INFO - PHRASE TEXT: Uso de Medicamentos – medicamentos redutores do colesterol são usados em conjunto com as alterações do estilo de vida para ajudar a diminuir o colesterol.\n",
            "2025-09-01 17:54:42,744 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:54:42,744 - INFO - CONTEXT: Section 2: COMO ESTE MEDICAMENTO FUNCIONA? > formas principais:\n",
            "2025-09-01 17:54:42,745 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: COMO ESTE MEDICAMENTO FUNCIONA? > formas principais:\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Uso de Medicamentos – medicamentos redutores do colesterol são usados em conjunto com as alterações do estilo de vida para ajudar a diminuir o colesterol.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:43,823 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"colesterol\": \"pode_ser_diminuido\", \"estilo_de_vida\": \"deve_ser_alterado\"}\n",
            "2025-09-01 17:54:43,824 - INFO - --- END PHRASE: phrase_15 ---\n",
            "\n",
            "2025-09-01 17:54:43,825 - WARNING - Could not find a valid JSON array/object in response: {\"colesterol\": \"pode_ser_diminuido\", \"estilo_de_vida\": \"deve_ser_alterado\"}...\n",
            "2025-09-01 17:54:44,326 - INFO - --- START PHRASE: phrase_17 ---\n",
            "2025-09-01 17:54:44,327 - INFO - PHRASE TEXT: Pacientes com hipersensibilidade (alérgicos) a ezetimiba ou a qualquer um de seus componentes não devem utilizar este produto.\n",
            "2025-09-01 17:54:44,327 - INFO - PHRASE TYPE: contraindication\n",
            "2025-09-01 17:54:44,328 - INFO - CONTEXT: Section 3: QUANDO NÃO DEVO USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:54:44,329 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 3: QUANDO NÃO DEVO USAR ESTE MEDICAMENTO?\n",
            "TIPO: contraindication\n",
            "INSTRUÇÃO: Identifique quando o medicamento NÃO deve ser usado.\n",
            "\n",
            "FRASE: \"Pacientes com hipersensibilidade (alérgicos) a ezetimiba ou a qualquer um de seus componentes não devem utilizar este produto.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:45,526 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Ezetimiba\":\"não_deve_ser_usado_em_pacientes_com\", \"hipersensibilidade\":[] }\n",
            "2025-09-01 17:54:45,527 - INFO - --- END PHRASE: phrase_17 ---\n",
            "\n",
            "2025-09-01 17:54:46,028 - INFO - --- START PHRASE: phrase_18 ---\n",
            "2025-09-01 17:54:46,029 - INFO - PHRASE TEXT: É importante que continue a tomar ezetimiba diariamente conforme prescrito pelo seu médico.\n",
            "2025-09-01 17:54:46,029 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:54:46,030 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:54:46,031 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"É importante que continue a tomar ezetimiba diariamente conforme prescrito pelo seu médico.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:47,368 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Ezetimiba\": \"deve_ser_tomado_diariamente\", \"relação\": \"conforme_prescrito_pelo_seu_médico\"}\n",
            "2025-09-01 17:54:47,368 - INFO - --- END PHRASE: phrase_18 ---\n",
            "\n",
            "2025-09-01 17:54:47,370 - WARNING - Could not find a valid JSON array/object in response: {\"Ezetimiba\": \"deve_ser_tomado_diariamente\", \"relação\": \"conforme_prescrito_pelo_seu_médico\"}...\n",
            "2025-09-01 17:54:47,871 - INFO - --- START PHRASE: phrase_19 ---\n",
            "2025-09-01 17:54:47,872 - INFO - PHRASE TEXT: Mesmo tomando medicamentos para tratar o colesterol alto, é importante que seu colesterol seja medido regularmente.\n",
            "2025-09-01 17:54:47,873 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:54:47,874 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:54:47,874 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Mesmo tomando medicamentos para tratar o colesterol alto, é importante que seu colesterol seja medido regularmente.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:48,895 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"colesterol\": \"deve_ser_medido_regularmente\", \"relação\": \"ser_medido\"}\n",
            "2025-09-01 17:54:48,896 - INFO - --- END PHRASE: phrase_19 ---\n",
            "\n",
            "2025-09-01 17:54:48,897 - WARNING - Could not find a valid JSON array/object in response: {\"colesterol\": \"deve_ser_medido_regularmente\", \"relação\": \"ser_medido\"}...\n",
            "2025-09-01 17:54:49,397 - INFO - --- START PHRASE: phrase_21 ---\n",
            "2025-09-01 17:54:49,398 - INFO - PHRASE TEXT: Gravidez e Amamentação:\n",
            "2025-09-01 17:54:49,399 - INFO - PHRASE TYPE: section_header\n",
            "2025-09-01 17:54:49,400 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:54:49,400 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "TIPO: section_header\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Gravidez e Amamentação:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:50,314 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Gravidez\": [\"tem_relacionamento_com\", \"amamentação\"]}\n",
            "2025-09-01 17:54:50,315 - INFO - --- END PHRASE: phrase_21 ---\n",
            "\n",
            "2025-09-01 17:54:50,817 - INFO - --- START PHRASE: phrase_22 ---\n",
            "2025-09-01 17:54:50,818 - INFO - PHRASE TEXT: se estiver grávida ou planeja engravidar, ezetimiba pode não ser o medicamento correto para você.\n",
            "2025-09-01 17:54:50,819 - INFO - PHRASE TYPE: contraindication\n",
            "2025-09-01 17:54:50,819 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:54:50,820 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "TIPO: contraindication\n",
            "INSTRUÇÃO: Identifique quando o medicamento NÃO deve ser usado.\n",
            "\n",
            "FRASE: \"se estiver grávida ou planeja engravidar, ezetimiba pode não ser o medicamento correto para você.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:51,995 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"Ezetimiba\", \"não_deve_ser_usado_em\": \"grávida ou planeja engravidar\"}\n",
            "2025-09-01 17:54:51,996 - INFO - --- END PHRASE: phrase_22 ---\n",
            "\n",
            "2025-09-01 17:54:51,998 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"Ezetimiba\", \"não_deve_ser_usado_em\": \"grávida ou planeja engravidar\"}...\n",
            "2025-09-01 17:54:52,500 - INFO - --- START PHRASE: phrase_24 ---\n",
            "2025-09-01 17:54:52,501 - INFO - PHRASE TEXT: Este medicamento não deve ser utilizado por mulheres grávidas sem orientação médica ou do cirurgião- dentista.\n",
            "2025-09-01 17:54:52,502 - INFO - PHRASE TYPE: contraindication\n",
            "2025-09-01 17:54:52,503 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:54:52,503 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "TIPO: contraindication\n",
            "INSTRUÇÃO: Identifique quando o medicamento NÃO deve ser usado.\n",
            "\n",
            "FRASE: \"Este medicamento não deve ser utilizado por mulheres grávidas sem orientação médica ou do cirurgião- dentista.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:53,817 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Women with pregnancy\": \"contraindicado sem orientação médica ou do cirurgião-dentista\", \"medicamento\": \"não deve ser utilizado\"}\n",
            "2025-09-01 17:54:53,818 - INFO - --- END PHRASE: phrase_24 ---\n",
            "\n",
            "2025-09-01 17:54:53,819 - WARNING - Could not find a valid JSON array/object in response: {\"Women with pregnancy\": \"contraindicado sem orientação médica ou do cirurgião-dentista\", \"medicamen...\n",
            "2025-09-01 17:54:54,320 - INFO - --- START PHRASE: phrase_25 ---\n",
            "2025-09-01 17:54:54,321 - INFO - PHRASE TEXT: Uso pediátrico:\n",
            "2025-09-01 17:54:54,321 - INFO - PHRASE TYPE: section_header\n",
            "2025-09-01 17:54:54,322 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > ezetimiba – VP06\n",
            "2025-09-01 17:54:54,323 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > ezetimiba – VP06\n",
            "TIPO: section_header\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Uso pediátrico:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:55,192 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Uso pediátrico\": [\"\", \"tem_restricao\", \"\"]}\n",
            "2025-09-01 17:54:55,192 - INFO - --- END PHRASE: phrase_25 ---\n",
            "\n",
            "2025-09-01 17:54:55,694 - INFO - --- START PHRASE: phrase_26 ---\n",
            "2025-09-01 17:54:55,695 - INFO - PHRASE TEXT: A ezetimiba não é recomendada para crianças com menos de 6 anos de idade.\n",
            "2025-09-01 17:54:55,696 - INFO - PHRASE TYPE: contraindication\n",
            "2025-09-01 17:54:55,697 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > ezetimiba – VP06\n",
            "2025-09-01 17:54:55,697 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > ezetimiba – VP06\n",
            "TIPO: contraindication\n",
            "INSTRUÇÃO: Identifique quando o medicamento NÃO deve ser usado.\n",
            "\n",
            "FRASE: \"A ezetimiba não é recomendada para crianças com menos de 6 anos de idade.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:56,886 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Ezetimiba\":\"não é recomendada para\", \"crianças com menos de 6 anos de idade\":null}\n",
            "2025-09-01 17:54:56,887 - INFO - --- END PHRASE: phrase_26 ---\n",
            "\n",
            "2025-09-01 17:54:56,889 - WARNING - Could not find a valid JSON array/object in response: {\"Ezetimiba\":\"não é recomendada para\", \"crianças com menos de 6 anos de idade\":null}...\n",
            "2025-09-01 17:54:57,390 - INFO - --- START PHRASE: phrase_28 ---\n",
            "2025-09-01 17:54:57,391 - INFO - PHRASE TEXT: foram relatados efeitos adversos com ezetimiba que podem afetar sua capacidade de dirigir ou operar máquinas.\n",
            "2025-09-01 17:54:57,392 - INFO - PHRASE TYPE: side_effect\n",
            "2025-09-01 17:54:57,392 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > ezetimiba – VP06\n",
            "2025-09-01 17:54:57,393 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > ezetimiba – VP06\n",
            "TIPO: side_effect\n",
            "INSTRUÇÃO: Extraia efeitos adversos, reações indesejáveis.\n",
            "\n",
            "FRASE: \"foram relatados efeitos adversos com ezetimiba que podem afetar sua capacidade de dirigir ou operar máquinas.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:54:59,108 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"ezetimiba\":[\n",
            "  [\"Ezetimiba\", \"afeta_capacidade_dirigir\", \"sim\"],\n",
            "  [\"Ezetimiba\", \"afeta_capacidade_operar_máquinas\", \"sim\"]\n",
            "]}\n",
            "2025-09-01 17:54:59,109 - INFO - --- END PHRASE: phrase_28 ---\n",
            "\n",
            "2025-09-01 17:54:59,610 - INFO - --- START PHRASE: phrase_29 ---\n",
            "2025-09-01 17:54:59,611 - INFO - PHRASE TEXT: As respostas individuais a ezetimiba podem variar (veja o item “8. QUAIS OS MALES\n",
            "2025-09-01 17:54:59,612 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:54:59,612 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > ezetimiba – VP06\n",
            "2025-09-01 17:54:59,613 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > ezetimiba – VP06\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"As respostas individuais a ezetimiba podem variar (veja o item “8. QUAIS OS MALES\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:00,815 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"tem_variabilidade_individual\", \"valor\": \"pode variar\"}\n",
            "2025-09-01 17:55:00,816 - INFO - --- END PHRASE: phrase_29 ---\n",
            "\n",
            "2025-09-01 17:55:00,818 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"ezetimiba\", \"relação\": \"tem_variabilidade_individual\", \"valor\": \"pode variar\"}...\n",
            "2025-09-01 17:55:01,319 - INFO - --- START PHRASE: phrase_30 ---\n",
            "2025-09-01 17:55:01,320 - INFO - PHRASE TEXT: Problemas Clínicos ou Alergias:\n",
            "2025-09-01 17:55:01,321 - INFO - PHRASE TYPE: section_header\n",
            "2025-09-01 17:55:01,321 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "2025-09-01 17:55:01,322 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "TIPO: section_header\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Problemas Clínicos ou Alergias:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:02,209 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Problemas Clínicos ou Alergias\": []}\n",
            "2025-09-01 17:55:02,210 - INFO - --- END PHRASE: phrase_30 ---\n",
            "\n",
            "2025-09-01 17:55:02,711 - INFO - --- START PHRASE: phrase_31 ---\n",
            "2025-09-01 17:55:02,712 - INFO - PHRASE TEXT: informe ao seu médico quaisquer doenças (incluindo doença hepática ou problemas hepáticos [relativos ao fígado]) ou alergias atuais ou passadas.\n",
            "2025-09-01 17:55:02,712 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:02,713 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "2025-09-01 17:55:02,713 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"informe ao seu médico quaisquer doenças (incluindo doença hepática ou problemas hepáticos [relativos ao fígado]) ou alergias atuais ou passadas.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:04,255 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"doenças\", \"relação\": \"incluir\", \"valor\": \"doença hepática ou problemas hepáticos [relativos ao fígado]\"}\n",
            "2025-09-01 17:55:04,256 - INFO - --- END PHRASE: phrase_31 ---\n",
            "\n",
            "2025-09-01 17:55:04,258 - WARNING - Could not parse response: [relativos ao fígado]...\n",
            "2025-09-01 17:55:04,759 - INFO - --- START PHRASE: phrase_32 ---\n",
            "2025-09-01 17:55:04,760 - INFO - PHRASE TEXT: Interações Medicamentosas:\n",
            "2025-09-01 17:55:04,761 - INFO - PHRASE TYPE: section_header\n",
            "2025-09-01 17:55:04,762 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "2025-09-01 17:55:04,763 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "TIPO: section_header\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Interações Medicamentosas:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:06,088 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Interações Medicamentosas\": [\n",
            "  [\"Medicamento\", \"interage_com\", \"\"],\n",
            "  [\"Paracetamol\", \"tem_dose\", \"\"]\n",
            "]}\n",
            "2025-09-01 17:55:06,089 - INFO - --- END PHRASE: phrase_32 ---\n",
            "\n",
            "2025-09-01 17:55:06,590 - INFO - --- START PHRASE: phrase_33 ---\n",
            "2025-09-01 17:55:06,590 - INFO - PHRASE TEXT: Você deve sempre informar seu médico sobre todos os medicamentos que estiver tomando ou planeja tomar, incluindo aqueles adquiridos sem receita médica.\n",
            "2025-09-01 17:55:06,591 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:06,592 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "2025-09-01 17:55:06,593 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Você deve sempre informar seu médico sobre todos os medicamentos que estiver tomando ou planeja tomar, incluindo aqueles adquiridos sem receita médica.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:07,985 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"medicamento\", \"relação\": \"deve_ser_informado\", \"valor\": \"todos os medicamentos que estiver tomando ou planeja tomar\"}\n",
            "2025-09-01 17:55:07,986 - INFO - --- END PHRASE: phrase_33 ---\n",
            "\n",
            "2025-09-01 17:55:07,988 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"medicamento\", \"relação\": \"deve_ser_informado\", \"valor\": \"todos os medicamentos que est...\n",
            "2025-09-01 17:55:08,489 - INFO - --- START PHRASE: phrase_34 ---\n",
            "2025-09-01 17:55:08,489 - INFO - PHRASE TEXT: Informe ao seu médico ou cirurgião-dentista se você está fazendo uso de algum outro medicamento.\n",
            "2025-09-01 17:55:08,490 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:08,491 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "2025-09-01 17:55:08,491 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Informe ao seu médico ou cirurgião-dentista se você está fazendo uso de algum outro medicamento.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:09,515 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"outro medicamento\", \"relação\": \"está fazendo uso\", \"valor\": null}\n",
            "2025-09-01 17:55:09,516 - INFO - --- END PHRASE: phrase_34 ---\n",
            "\n",
            "2025-09-01 17:55:09,517 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"outro medicamento\", \"relação\": \"está fazendo uso\", \"valor\": null}...\n",
            "2025-09-01 17:55:10,018 - INFO - --- START PHRASE: phrase_35 ---\n",
            "2025-09-01 17:55:10,018 - INFO - PHRASE TEXT: Não use medicamento sem o conhecimento do seu médico.\n",
            "2025-09-01 17:55:10,019 - INFO - PHRASE TYPE: contraindication\n",
            "2025-09-01 17:55:10,020 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "2025-09-01 17:55:10,021 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "TIPO: contraindication\n",
            "INSTRUÇÃO: Identifique quando o medicamento NÃO deve ser usado.\n",
            "\n",
            "FRASE: \"Não use medicamento sem o conhecimento do seu médico.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:11,174 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"medicamento\":\"não_deve_ser_usado\",\"sem_conhecimento_do_seu_médico\":[]}\n",
            "2025-09-01 17:55:11,175 - INFO - --- END PHRASE: phrase_35 ---\n",
            "\n",
            "2025-09-01 17:55:11,676 - INFO - --- START PHRASE: phrase_37 ---\n",
            "2025-09-01 17:55:11,677 - INFO - PHRASE TEXT: Esse medicamento não é recomendado para crianças com menos de 6 anos de idade (veja Uso pediátrico).\n",
            "2025-09-01 17:55:11,678 - INFO - PHRASE TYPE: contraindication\n",
            "2025-09-01 17:55:11,678 - INFO - CONTEXT: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "2025-09-01 17:55:11,679 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 4: O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO? > QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "TIPO: contraindication\n",
            "INSTRUÇÃO: Identifique quando o medicamento NÃO deve ser usado.\n",
            "\n",
            "FRASE: \"Esse medicamento não é recomendado para crianças com menos de 6 anos de idade (veja Uso pediátrico).\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:12,811 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Crianças com menos de 6 anos de idade\": \"não deve ser usado para\", \"esse medicamento\":\n",
            "2025-09-01 17:55:12,812 - INFO - --- END PHRASE: phrase_37 ---\n",
            "\n",
            "2025-09-01 17:55:12,814 - WARNING - Could not find a valid JSON array/object in response: {\"Crianças com menos de 6 anos de idade\": \"não deve ser usado para\", \"esse medicamento\":...\n",
            "2025-09-01 17:55:13,315 - INFO - --- START PHRASE: phrase_38 ---\n",
            "2025-09-01 17:55:13,316 - INFO - PHRASE TEXT: Mantenha em temperatura ambiente (temperatura entre 15 e 30ºC).\n",
            "2025-09-01 17:55:13,316 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:55:13,317 - INFO - CONTEXT: Section 5: ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:13,317 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 5: ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"Mantenha em temperatura ambiente (temperatura entre 15 e 30ºC).\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:14,397 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"temperature_range\": [\"temperatura\", \"entre\", \"15 e 30ºC\"]}\n",
            "2025-09-01 17:55:14,398 - INFO - --- END PHRASE: phrase_38 ---\n",
            "\n",
            "2025-09-01 17:55:14,899 - INFO - --- START PHRASE: phrase_42 ---\n",
            "2025-09-01 17:55:14,900 - INFO - PHRASE TEXT: Não use medicamento com o prazo de validade vencido.\n",
            "2025-09-01 17:55:14,900 - INFO - PHRASE TYPE: contraindication\n",
            "2025-09-01 17:55:14,901 - INFO - CONTEXT: Section 5: ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:14,902 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 5: ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "TIPO: contraindication\n",
            "INSTRUÇÃO: Identifique quando o medicamento NÃO deve ser usado.\n",
            "\n",
            "FRASE: \"Não use medicamento com o prazo de validade vencido.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:16,117 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"medicamento\":\"não_deve_ser_usado\",\"prazo_de_validade_vencido\":true}\n",
            "2025-09-01 17:55:16,118 - INFO - --- END PHRASE: phrase_42 ---\n",
            "\n",
            "2025-09-01 17:55:16,120 - WARNING - Could not find a valid JSON array/object in response: {\"medicamento\":\"não_deve_ser_usado\",\"prazo_de_validade_vencido\":true}...\n",
            "2025-09-01 17:55:16,622 - INFO - --- START PHRASE: phrase_44 ---\n",
            "2025-09-01 17:55:16,623 - INFO - PHRASE TEXT: A ezetimiba é um comprimido oval branco a quase branco, com a gravação “10” em uma face e a gravação “EZT” na outra face.\n",
            "2025-09-01 17:55:16,623 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:55:16,624 - INFO - CONTEXT: Section 5: ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:16,625 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 5: ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"A ezetimiba é um comprimido oval branco a quase branco, com a gravação “10” em uma face e a gravação “EZT” na outra face.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:18,025 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"tem_forma_farmacêutica\", \"valor\": \"comprimido oval branco a quase branco\"}\n",
            "2025-09-01 17:55:18,026 - INFO - --- END PHRASE: phrase_44 ---\n",
            "\n",
            "2025-09-01 17:55:18,028 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"ezetimiba\", \"relação\": \"tem_forma_farmacêutica\", \"valor\": \"comprimido oval branco a qu...\n",
            "2025-09-01 17:55:18,529 - INFO - --- START PHRASE: phrase_45 ---\n",
            "2025-09-01 17:55:18,529 - INFO - PHRASE TEXT: Antes de usar, observe o aspecto do medicamento.\n",
            "2025-09-01 17:55:18,530 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:18,531 - INFO - CONTEXT: Section 5: ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:18,532 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 5: ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Antes de usar, observe o aspecto do medicamento.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:19,626 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"medicamento\", \"relação\": \"deve_ser_observado\", \"valor\": \"aspecto\"}\n",
            "2025-09-01 17:55:19,627 - INFO - --- END PHRASE: phrase_45 ---\n",
            "\n",
            "2025-09-01 17:55:19,628 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"medicamento\", \"relação\": \"deve_ser_observado\", \"valor\": \"aspecto\"}...\n",
            "2025-09-01 17:55:20,129 - INFO - --- START PHRASE: phrase_47 ---\n",
            "2025-09-01 17:55:20,130 - INFO - PHRASE TEXT: Todo medicamento deve ser mantido fora do alcance das crianças.\n",
            "2025-09-01 17:55:20,130 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:20,132 - INFO - CONTEXT: Section 5: ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:20,132 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 5: ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Todo medicamento deve ser mantido fora do alcance das crianças.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:21,461 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"medicamento\", \"relação\": \"deve_ser_mantido\", \"valor\": \"fora_do_alcance_das_crianças\"}\n",
            "2025-09-01 17:55:21,461 - INFO - --- END PHRASE: phrase_47 ---\n",
            "\n",
            "2025-09-01 17:55:21,462 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"medicamento\", \"relação\": \"deve_ser_mantido\", \"valor\": \"fora_do_alcance_das_crianças\"}...\n",
            "2025-09-01 17:55:21,963 - INFO - --- START PHRASE: phrase_48 ---\n",
            "2025-09-01 17:55:21,964 - INFO - PHRASE TEXT: Adultos e crianças acima de 6 anos de idade:\n",
            "2025-09-01 17:55:21,965 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:55:21,966 - INFO - CONTEXT: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:21,966 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"Adultos e crianças acima de 6 anos de idade:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:23,050 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"Adultos e crianças\", \"relação\": \"tem_idade\", \"valor\": \"6 anos\"}\n",
            "2025-09-01 17:55:23,051 - INFO - --- END PHRASE: phrase_48 ---\n",
            "\n",
            "2025-09-01 17:55:23,052 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"Adultos e crianças\", \"relação\": \"tem_idade\", \"valor\": \"6 anos\"}...\n",
            "2025-09-01 17:55:23,553 - INFO - --- START PHRASE: phrase_49 ---\n",
            "2025-09-01 17:55:23,553 - INFO - PHRASE TEXT: Tome um comprimido de 10 mg por via oral diariamente, em qualquer horário do dia.\n",
            "2025-09-01 17:55:23,554 - INFO - PHRASE TYPE: dosage_instruction\n",
            "2025-09-01 17:55:23,555 - INFO - CONTEXT: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:23,556 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "TIPO: dosage_instruction\n",
            "INSTRUÇÃO: Foque em doses, quantidades, frequências de administração.\n",
            "\n",
            "FRASE: \"Tome um comprimido de 10 mg por via oral diariamente, em qualquer horário do dia.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:24,423 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Medicamento\": \"tem_dose\", \"Valor\": \"10 mg\"}\n",
            "2025-09-01 17:55:24,423 - INFO - --- END PHRASE: phrase_49 ---\n",
            "\n",
            "2025-09-01 17:55:24,425 - WARNING - Could not find a valid JSON array/object in response: {\"Medicamento\": \"tem_dose\", \"Valor\": \"10 mg\"}...\n",
            "2025-09-01 17:55:24,926 - INFO - --- START PHRASE: phrase_51 ---\n",
            "2025-09-01 17:55:24,926 - INFO - PHRASE TEXT: Seu médico pode ter falado para você tomar ezetimiba com outros medicamentos, conhecidos como estatinas, ou com outro medicamento conhecido como fenofibrato, para ajudá-lo a controlar melhor seu colesterol;\n",
            "2025-09-01 17:55:24,927 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:24,928 - INFO - CONTEXT: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:24,929 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Seu médico pode ter falado para você tomar ezetimiba com outros medicamentos, conhecidos como estatinas, ou com outro medicamento conhecido como fenofibrato, para ajudá-lo a controlar melhor seu colesterol;\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:25,955 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"tem_dose\", \"valor\": []}\n",
            "2025-09-01 17:55:25,956 - INFO - --- END PHRASE: phrase_51 ---\n",
            "\n",
            "2025-09-01 17:55:26,458 - INFO - --- START PHRASE: phrase_52 ---\n",
            "2025-09-01 17:55:26,459 - INFO - PHRASE TEXT: neste caso, você pode tomar ezetimiba no mesmo horário em que for tomar o outro medicamento.\n",
            "2025-09-01 17:55:26,459 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:26,460 - INFO - CONTEXT: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:26,461 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"neste caso, você pode tomar ezetimiba no mesmo horário em que for tomar o outro medicamento.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:27,967 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Ezetimiba\":[\n",
            "  [\"Ezetimiba\", \"pode_ser_tomado_no_mesmo_horario_com\", \"outro_medicamento\"]\n",
            "]}\n",
            "2025-09-01 17:55:27,972 - INFO - --- END PHRASE: phrase_52 ---\n",
            "\n",
            "2025-09-01 17:55:28,473 - INFO - --- START PHRASE: phrase_55 ---\n",
            "2025-09-01 17:55:28,474 - INFO - PHRASE TEXT: Continue a tomar outros medicamentos redutores de colesterol, a menos que seu médico mande você parar.\n",
            "2025-09-01 17:55:28,474 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:28,475 - INFO - CONTEXT: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:28,476 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Continue a tomar outros medicamentos redutores de colesterol, a menos que seu médico mande você parar.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:29,673 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"colesterol\": \"reducer\", \"medicamento\": \"parar\", \"médico\": \"mande\"}\n",
            "2025-09-01 17:55:29,674 - INFO - --- END PHRASE: phrase_55 ---\n",
            "\n",
            "2025-09-01 17:55:29,675 - WARNING - Could not find a valid JSON array/object in response: {\"colesterol\": \"reducer\", \"medicamento\": \"parar\", \"médico\": \"mande\"}...\n",
            "2025-09-01 17:55:30,176 - INFO - --- START PHRASE: phrase_56 ---\n",
            "2025-09-01 17:55:30,176 - INFO - PHRASE TEXT: Siga a orientação de seu médico, respeitando sempre os horários, as doses e a duração do tratamento.\n",
            "2025-09-01 17:55:30,177 - INFO - PHRASE TYPE: indication\n",
            "2025-09-01 17:55:30,178 - INFO - CONTEXT: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:30,179 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "TIPO: indication\n",
            "INSTRUÇÃO: Extraia para que condições ou doenças o medicamento é indicado.\n",
            "\n",
            "FRASE: \"Siga a orientação de seu médico, respeitando sempre os horários, as doses e a duração do tratamento.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:30,716 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"[]\": []}\n",
            "2025-09-01 17:55:30,717 - INFO - --- END PHRASE: phrase_56 ---\n",
            "\n",
            "2025-09-01 17:55:31,218 - INFO - --- START PHRASE: phrase_57 ---\n",
            "2025-09-01 17:55:31,219 - INFO - PHRASE TEXT: Não interrompa o tratamento sem o conhecimento do seu médico.\n",
            "2025-09-01 17:55:31,220 - INFO - PHRASE TYPE: indication\n",
            "2025-09-01 17:55:31,221 - INFO - CONTEXT: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:31,221 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 6: COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "TIPO: indication\n",
            "INSTRUÇÃO: Extraia para que condições ou doenças o medicamento é indicado.\n",
            "\n",
            "FRASE: \"Não interrompa o tratamento sem o conhecimento do seu médico.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:31,608 - INFO - RAW RESPONSE RECEIVED:\n",
            "{}\n",
            "2025-09-01 17:55:31,608 - INFO - --- END PHRASE: phrase_57 ---\n",
            "\n",
            "2025-09-01 17:55:31,609 - WARNING - Could not find a valid JSON array/object in response: {}...\n",
            "2025-09-01 17:55:32,111 - INFO - --- START PHRASE: phrase_58 ---\n",
            "2025-09-01 17:55:32,112 - INFO - PHRASE TEXT: Tente tomar ezetimiba conforme prescrito.\n",
            "2025-09-01 17:55:32,112 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:32,113 - INFO - CONTEXT: Section 7: O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:32,114 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 7: O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Tente tomar ezetimiba conforme prescrito.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:33,091 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"tem_dose\", \"valor\": \"\"}\n",
            "2025-09-01 17:55:33,092 - INFO - --- END PHRASE: phrase_58 ---\n",
            "\n",
            "2025-09-01 17:55:33,093 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"ezetimiba\", \"relação\": \"tem_dose\", \"valor\": \"\"}...\n",
            "2025-09-01 17:55:33,593 - INFO - --- START PHRASE: phrase_59 ---\n",
            "2025-09-01 17:55:33,594 - INFO - PHRASE TEXT: Entretanto, se esquecer de tomar uma dose, reinicie o esquema usual tomando um comprimido por dia.\n",
            "2025-09-01 17:55:33,595 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:33,596 - INFO - CONTEXT: Section 7: O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?\n",
            "2025-09-01 17:55:33,596 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 7: O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Entretanto, se esquecer de tomar uma dose, reinicie o esquema usual tomando um comprimido por dia.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:34,615 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"comprimido\", \"relação\": \"tem_dose\", \"valor\": \"um\"}\n",
            "2025-09-01 17:55:34,616 - INFO - --- END PHRASE: phrase_59 ---\n",
            "\n",
            "2025-09-01 17:55:34,617 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"comprimido\", \"relação\": \"tem_dose\", \"valor\": \"um\"}...\n",
            "2025-09-01 17:55:35,119 - INFO - --- START PHRASE: phrase_62 ---\n",
            "2025-09-01 17:55:35,119 - INFO - PHRASE TEXT: Os efeitos adversos geralmente foram leves e semelhantes em tipo e frequência aos efeitos adversos observados em pacientes que receberam placebo (comprimido que não contém medicamento).\n",
            "2025-09-01 17:55:35,120 - INFO - PHRASE TYPE: contraindication\n",
            "2025-09-01 17:55:35,121 - INFO - CONTEXT: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "2025-09-01 17:55:35,121 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "TIPO: contraindication\n",
            "INSTRUÇÃO: Identifique quando o medicamento NÃO deve ser usado.\n",
            "\n",
            "FRASE: \"Os efeitos adversos geralmente foram leves e semelhantes em tipo e frequência aos efeitos adversos observados em pacientes que receberam placebo (comprimido que não contém medicamento).\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:37,480 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"efeitos_adversos\": [\"são_leves\"], \n",
            "\"tipo_de_efetos_adversos\": [\"semelhantes_em_tipo_e_frequência_aos_observados_em_pacientes_que_receberam_placebo\"], \n",
            "\"frequência_de_efetos_adversos\": [\"geralmente\"] }\n",
            "2025-09-01 17:55:37,481 - INFO - --- END PHRASE: phrase_62 ---\n",
            "\n",
            "2025-09-01 17:55:37,983 - INFO - --- START PHRASE: phrase_63 ---\n",
            "2025-09-01 17:55:37,984 - INFO - PHRASE TEXT: Em geral, os efeitos adversos não provocaram a interrupção do tratamento com ezetimiba.\n",
            "2025-09-01 17:55:37,985 - INFO - PHRASE TYPE: indication\n",
            "2025-09-01 17:55:37,986 - INFO - CONTEXT: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "2025-09-01 17:55:37,986 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "TIPO: indication\n",
            "INSTRUÇÃO: Extraia para que condições ou doenças o medicamento é indicado.\n",
            "\n",
            "FRASE: \"Em geral, os efeitos adversos não provocaram a interrupção do tratamento com ezetimiba.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:39,060 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"ezetimiba\": [\"não_provoca_interrupção_tratamento\", \"efeitos_adversos\"]}\n",
            "2025-09-01 17:55:39,062 - INFO - --- END PHRASE: phrase_63 ---\n",
            "\n",
            "2025-09-01 17:55:39,563 - INFO - --- START PHRASE: phrase_64 ---\n",
            "2025-09-01 17:55:39,563 - INFO - PHRASE TEXT: Quando ezetimiba foi usada isoladamente, foram relatados os seguintes efeitos adversos: Comuns: dor abdominal; diarreia;\n",
            "2025-09-01 17:55:39,564 - INFO - PHRASE TYPE: side_effect\n",
            "2025-09-01 17:55:39,565 - INFO - CONTEXT: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "2025-09-01 17:55:39,565 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "TIPO: side_effect\n",
            "INSTRUÇÃO: Extraia efeitos adversos, reações indesejáveis.\n",
            "\n",
            "FRASE: \"Quando ezetimiba foi usada isoladamente, foram relatados os seguintes efeitos adversos: Comuns: dor abdominal; diarreia;\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:40,744 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"causa\", \"valor\": [\"dor abdominal\"] }\n",
            "2025-09-01 17:55:40,745 - INFO - --- END PHRASE: phrase_64 ---\n",
            "\n",
            "2025-09-01 17:55:41,247 - INFO - --- START PHRASE: phrase_73 ---\n",
            "2025-09-01 17:55:41,248 - INFO - PHRASE TEXT: Além disso, quando tomado com uma estatina, foram relatados os seguintes efeitos adversos: Comuns:\n",
            "2025-09-01 17:55:41,248 - INFO - PHRASE TYPE: side_effect\n",
            "2025-09-01 17:55:41,249 - INFO - CONTEXT: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "2025-09-01 17:55:41,249 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "TIPO: side_effect\n",
            "INSTRUÇÃO: Extraia efeitos adversos, reações indesejáveis.\n",
            "\n",
            "FRASE: \"Além disso, quando tomado com uma estatina, foram relatados os seguintes efeitos adversos: Comuns:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:42,256 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"estatina\":\"tem_efeito_adverso\",\"valor\":[\"Comuns\"]}\n",
            "2025-09-01 17:55:42,257 - INFO - --- END PHRASE: phrase_73 ---\n",
            "\n",
            "2025-09-01 17:55:42,758 - INFO - --- START PHRASE: phrase_83 ---\n",
            "2025-09-01 17:55:42,760 - INFO - PHRASE TEXT: Ao ser utilizado com fenofibratos, o seguinte efeito adverso foi relatado: Comum:\n",
            "2025-09-01 17:55:42,760 - INFO - PHRASE TYPE: section_header\n",
            "2025-09-01 17:55:42,761 - INFO - CONTEXT: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "2025-09-01 17:55:42,762 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "TIPO: section_header\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Ao ser utilizado com fenofibratos, o seguinte efeito adverso foi relatado: Comum:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:44,599 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Fenofibratos\":[\n",
            "  [\"Fenofibratos\", \"deve_ser_usado_com\", \"o medicamento\"],\n",
            "  [\"Efeito adverso\", \"foi_relacionado\", \"Comum\"]\n",
            "]}\n",
            "2025-09-01 17:55:44,600 - INFO - --- END PHRASE: phrase_83 ---\n",
            "\n",
            "2025-09-01 17:55:45,101 - INFO - --- START PHRASE: phrase_85 ---\n",
            "2025-09-01 17:55:45,102 - INFO - PHRASE TEXT: Além disso, foram relatados os seguintes efeitos adversos no uso geral:\n",
            "2025-09-01 17:55:45,103 - INFO - PHRASE TYPE: side_effect\n",
            "2025-09-01 17:55:45,104 - INFO - CONTEXT: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "2025-09-01 17:55:45,104 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "TIPO: side_effect\n",
            "INSTRUÇÃO: Extraia efeitos adversos, reações indesejáveis.\n",
            "\n",
            "FRASE: \"Além disso, foram relatados os seguintes efeitos adversos no uso geral:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:46,483 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"efeitos adversos\", \"relação\": \"foram_relratados_no_uso_geral\", \"valor\": [\"não especificado\"]}\n",
            "2025-09-01 17:55:46,484 - INFO - --- END PHRASE: phrase_85 ---\n",
            "\n",
            "2025-09-01 17:55:46,985 - INFO - --- START PHRASE: phrase_86 ---\n",
            "2025-09-01 17:55:46,986 - INFO - PHRASE TEXT: reações alérgicas (que podem exigir tratamento imediato), incluindo inchaço da face, lábios, língua e/ou garganta que possa causar dificuldade para respirar ou engolir, erupções cutâneas, urticária;\n",
            "2025-09-01 17:55:46,987 - INFO - PHRASE TYPE: indication\n",
            "2025-09-01 17:55:46,987 - INFO - CONTEXT: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "2025-09-01 17:55:46,988 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "TIPO: indication\n",
            "INSTRUÇÃO: Extraia para que condições ou doenças o medicamento é indicado.\n",
            "\n",
            "FRASE: \"reações alérgicas (que podem exigir tratamento imediato), incluindo inchaço da face, lábios, língua e/ou garganta que possa causar dificuldade para respirar ou engolir, erupções cutâneas, urticária;\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:55,030 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"reações alérgicas\", \"relação\": \"pode_causar\", \"valor\": [\"inchaço da face\"], \n",
            "\"entidade\": \"reações alérgicas\", \"relação\": \"pode_causar\", \"valor\": [\"inchaço dos lábios\"], \n",
            "\"entidade\": \"reações alérgicas\", \"relação\": \"pode_causar\", \"valor\": [\"inchaço da língua\"], \n",
            "\"entidade\": \"reações alérgicas\", \"relação\": \"pode_causar\", \"valor\": [\"inchaço da garganta\"], \n",
            "\"entidade\": \"reações alérgicas\", \"relação\": \"pode_causar\", \"valor\": [\"dificuldade para respirar\"], \n",
            "\"entidade\": \"reações alérgicas\", \"relação\": \"pode_causar\", \"valor\": [\"dificuldade para engolir\"], \n",
            "\"entidade\": \"erupções cutâneas\", \"relação\": \"pode_causar\", \"valor\": [], \n",
            "\"entidade\": \"urticária\", \"relação\": \"pode_causar\", \"valor\": []}\n",
            "2025-09-01 17:55:55,036 - INFO - --- END PHRASE: phrase_86 ---\n",
            "\n",
            "2025-09-01 17:55:55,537 - INFO - --- START PHRASE: phrase_100 ---\n",
            "2025-09-01 17:55:55,538 - INFO - PHRASE TEXT: Se ezetimiba foi prescrita para ser tomado com uma estatina, seu médico poderá solicitar exames de sangue de rotina para verificar sua função hepática antes e depois de iniciar o tratamento.\n",
            "2025-09-01 17:55:55,539 - INFO - PHRASE TYPE: indication\n",
            "2025-09-01 17:55:55,540 - INFO - CONTEXT: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR? > ezetimiba – VP06\n",
            "2025-09-01 17:55:55,540 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR? > ezetimiba – VP06\n",
            "TIPO: indication\n",
            "INSTRUÇÃO: Extraia para que condições ou doenças o medicamento é indicado.\n",
            "\n",
            "FRASE: \"Se ezetimiba foi prescrita para ser tomado com uma estatina, seu médico poderá solicitar exames de sangue de rotina para verificar sua função hepática antes e depois de iniciar o tratamento.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:56,945 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"ezetimiba\": [\"é_indicado_com\", \"estatina\"]}\n",
            "2025-09-01 17:55:56,946 - INFO - --- END PHRASE: phrase_100 ---\n",
            "\n",
            "2025-09-01 17:55:57,448 - INFO - --- START PHRASE: phrase_101 ---\n",
            "2025-09-01 17:55:57,449 - INFO - PHRASE TEXT: Informe ao seu médico, cirurgião-dentista ou farmacêutico o aparecimento de reações indesejáveis pelo uso do medicamento.\n",
            "2025-09-01 17:55:57,450 - INFO - PHRASE TYPE: side_effect\n",
            "2025-09-01 17:55:57,450 - INFO - CONTEXT: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR? > ezetimiba – VP06\n",
            "2025-09-01 17:55:57,451 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 8: QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR? > ezetimiba – VP06\n",
            "TIPO: side_effect\n",
            "INSTRUÇÃO: Extraia efeitos adversos, reações indesejáveis.\n",
            "\n",
            "FRASE: \"Informe ao seu médico, cirurgião-dentista ou farmacêutico o aparecimento de reações indesejáveis pelo uso do medicamento.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:55:58,685 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"medicamento\", \"relação\": \"pode_causar\", \"valor\": \"reações indesejáveis\"}\n",
            "2025-09-01 17:55:58,686 - INFO - --- END PHRASE: phrase_101 ---\n",
            "\n",
            "2025-09-01 17:55:58,688 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"medicamento\", \"relação\": \"pode_causar\", \"valor\": \"reações indesejáveis\"}...\n",
            "2025-09-01 17:55:59,189 - INFO - --- START PHRASE: phrase_104 ---\n",
            "2025-09-01 17:55:59,190 - INFO - PHRASE TEXT: Se tomar mais ezetimiba do que o prescrito, entre em contato com seu médico.\n",
            "2025-09-01 17:55:59,191 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:55:59,192 - INFO - CONTEXT: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > MEDICAMENTO?\n",
            "2025-09-01 17:55:59,192 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Se tomar mais ezetimiba do que o prescrito, entre em contato com seu médico.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:00,256 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"tem_dose\", \"valor\": \"\"}\n",
            "2025-09-01 17:56:00,257 - INFO - --- END PHRASE: phrase_104 ---\n",
            "\n",
            "2025-09-01 17:56:00,258 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"ezetimiba\", \"relação\": \"tem_dose\", \"valor\": \"\"}...\n",
            "2025-09-01 17:56:00,759 - INFO - --- START PHRASE: phrase_105 ---\n",
            "2025-09-01 17:56:00,760 - INFO - PHRASE TEXT: Em caso de uso de grande quantidade deste medicamento, procure rapidamente socorro médico e leve a embalagem ou bula do medicamento, se possível.\n",
            "2025-09-01 17:56:00,761 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:56:00,761 - INFO - CONTEXT: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > MEDICAMENTO?\n",
            "2025-09-01 17:56:00,762 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > MEDICAMENTO?\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Em caso de uso de grande quantidade deste medicamento, procure rapidamente socorro médico e leve a embalagem ou bula do medicamento, se possível.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:01,969 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"medicamento\", \"relação\": \"tem_quantidade_maior_que_indicada\", \"valor\": \"\"}\n",
            "2025-09-01 17:56:01,969 - INFO - --- END PHRASE: phrase_105 ---\n",
            "\n",
            "2025-09-01 17:56:01,970 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"medicamento\", \"relação\": \"tem_quantidade_maior_que_indicada\", \"valor\": \"\"}...\n",
            "2025-09-01 17:56:02,471 - INFO - --- START PHRASE: phrase_106 ---\n",
            "2025-09-01 17:56:02,472 - INFO - PHRASE TEXT: Ligue para 0800 722 6001, se você precisar de mais orientações.\n",
            "2025-09-01 17:56:02,473 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:02,474 - INFO - CONTEXT: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > MEDICAMENTO?\n",
            "2025-09-01 17:56:02,474 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > MEDICAMENTO?\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"Ligue para 0800 722 6001, se você precisar de mais orientações.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:02,889 - INFO - RAW RESPONSE RECEIVED:\n",
            "{}\n",
            "2025-09-01 17:56:02,890 - INFO - --- END PHRASE: phrase_106 ---\n",
            "\n",
            "2025-09-01 17:56:02,892 - WARNING - Could not find a valid JSON array/object in response: {}...\n",
            "2025-09-01 17:56:03,392 - INFO - --- START PHRASE: phrase_107 ---\n",
            "2025-09-01 17:56:03,393 - INFO - PHRASE TEXT: Esta bula foi aprovada pela Anvisa em 21/07/2023.\n",
            "2025-09-01 17:56:03,394 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:03,395 - INFO - CONTEXT: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > CRF-PR nº 17.379\n",
            "2025-09-01 17:56:03,395 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > CRF-PR nº 17.379\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"Esta bula foi aprovada pela Anvisa em 21/07/2023.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:04,556 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"Anvisa\", \"relação\": \"aprovou\", \"valor\": \"21/07/2023\"}\n",
            "2025-09-01 17:56:04,557 - INFO - --- END PHRASE: phrase_107 ---\n",
            "\n",
            "2025-09-01 17:56:04,558 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"Anvisa\", \"relação\": \"aprovou\", \"valor\": \"21/07/2023\"}...\n",
            "2025-09-01 17:56:05,060 - INFO - --- START PHRASE: phrase_115 ---\n",
            "2025-09-01 17:56:05,060 - INFO - PHRASE TEXT: 11/09/2015 0811000150 texto de bula – 11/09/2015 0811000150 texto de bula – RDC 11/09/2015 Versão Inicial VP01\n",
            "2025-09-01 17:56:05,061 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:05,062 - INFO - CONTEXT: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > 10mg\n",
            "2025-09-01 17:56:05,063 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > 10mg\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"11/09/2015 0811000150 texto de bula – 11/09/2015 0811000150 texto de bula – RDC 11/09/2015 Versão Inicial VP01\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:06,142 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"nenhuma\", \"relação\": \"nenhuma\", \"valor\": []}\n",
            "2025-09-01 17:56:06,143 - INFO - --- END PHRASE: phrase_115 ---\n",
            "\n",
            "2025-09-01 17:56:06,645 - INFO - --- START PHRASE: phrase_116 ---\n",
            "2025-09-01 17:56:06,645 - INFO - PHRASE TEXT: 21/02/2018 0132759183 25/09/2017 2013155171 GENÉRICO - 08/01/2018 Apresentações VP02\n",
            "2025-09-01 17:56:06,646 - INFO - PHRASE TYPE: dosage_instruction\n",
            "2025-09-01 17:56:06,647 - INFO - CONTEXT: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > Alteração de 10 mg\n",
            "2025-09-01 17:56:06,648 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > Alteração de 10 mg\n",
            "TIPO: dosage_instruction\n",
            "INSTRUÇÃO: Foque em doses, quantidades, frequências de administração.\n",
            "\n",
            "FRASE: \"21/02/2018 0132759183 25/09/2017 2013155171 GENÉRICO - 08/01/2018 Apresentações VP02\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:07,949 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"data\", \"relação\": \"data_de_alteração\", \"valor\": \"21/02/2018\"}\n",
            "2025-09-01 17:56:07,951 - INFO - --- END PHRASE: phrase_116 ---\n",
            "\n",
            "2025-09-01 17:56:07,952 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"data\", \"relação\": \"data_de_alteração\", \"valor\": \"21/02/2018\"}...\n",
            "2025-09-01 17:56:08,453 - INFO - --- START PHRASE: phrase_117 ---\n",
            "2025-09-01 17:56:08,454 - INFO - PHRASE TEXT: 10/03/2019 0210527196 10/03/2019 0210527196 Alteração de Texto 10/03/2019 Dizeres Legais VP03\n",
            "2025-09-01 17:56:08,455 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:08,455 - INFO - CONTEXT: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > Alteração de 10 mg\n",
            "2025-09-01 17:56:08,456 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > Alteração de 10 mg\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"10/03/2019 0210527196 10/03/2019 0210527196 Alteração de Texto 10/03/2019 Dizeres Legais VP03\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:09,435 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Alteração de 10 mg\": [\"tem_valor\", \"10\"]}\n",
            "2025-09-01 17:56:09,436 - INFO - --- END PHRASE: phrase_117 ---\n",
            "\n",
            "2025-09-01 17:56:09,937 - INFO - --- START PHRASE: phrase_118 ---\n",
            "2025-09-01 17:56:09,938 - INFO - PHRASE TEXT: 18/12/2019 3498569197 18/12/2019 3498569197 Alteração de Texto 18/12/2019 usar este VP04\n",
            "2025-09-01 17:56:09,939 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:09,939 - INFO - CONTEXT: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > Alteração de 10 mg\n",
            "2025-09-01 17:56:09,940 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > Alteração de 10 mg\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"18/12/2019 3498569197 18/12/2019 3498569197 Alteração de Texto 18/12/2019 usar este VP04\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:10,859 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Alteração de 10 mg\": [\"tem_valor\", \"10mg\"]}\n",
            "2025-09-01 17:56:10,860 - INFO - --- END PHRASE: phrase_118 ---\n",
            "\n",
            "2025-09-01 17:56:11,361 - INFO - --- START PHRASE: phrase_119 ---\n",
            "2025-09-01 17:56:11,362 - INFO - PHRASE TEXT: 15/04/2021 1442448217 01/04/2021 1249540219 embalagem primária 01/04/2021 -Dizeres Legais VP05\n",
            "2025-09-01 17:56:11,363 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:11,364 - INFO - CONTEXT: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > Alteração de 10 mg\n",
            "2025-09-01 17:56:11,365 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > Alteração de 10 mg\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"15/04/2021 1442448217 01/04/2021 1249540219 embalagem primária 01/04/2021 -Dizeres Legais VP05\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:12,346 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"Alteração\", \"relação\": \"de\", \"valor\": \"10mg\"}\n",
            "2025-09-01 17:56:12,347 - INFO - --- END PHRASE: phrase_119 ---\n",
            "\n",
            "2025-09-01 17:56:12,348 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"Alteração\", \"relação\": \"de\", \"valor\": \"10mg\"}...\n",
            "2025-09-01 17:56:12,848 - INFO - --- START PHRASE: phrase_120 ---\n",
            "2025-09-01 17:56:12,849 - INFO - PHRASE TEXT: 09/08/2023 - 09/08/2023 - Alteração de Texto 09/08/2023 VP06\n",
            "2025-09-01 17:56:12,850 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:12,851 - INFO - CONTEXT: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > Alteração de que este 10 mg\n",
            "2025-09-01 17:56:12,853 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 9: O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE > Alteração de que este 10 mg\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"09/08/2023 - 09/08/2023 - Alteração de Texto 09/08/2023 VP06\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:13,899 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Alteração de que este 10 mg\": [\"substância\", \"tem_dose\", \"10mg\"]}\n",
            "2025-09-01 17:56:13,900 - INFO - --- END PHRASE: phrase_120 ---\n",
            "\n",
            "2025-09-01 17:56:14,402 - INFO - ✅ Completed bula_1755192077396_phrase_optimized.json: 7 total triples extracted\n",
            "2025-09-01 17:56:14,412 - INFO - 💾 Saved results to: /content/drive/MyDrive/enhanced_graph_data/bula_1755192077396_enhanced_graph_data.json\n",
            "2025-09-01 17:56:14,413 - INFO - \n",
            "📊 Progress: Processing file 2/339\n",
            "2025-09-01 17:56:14,414 - INFO - 📄 Processing phrase-based file: bula_1755192097944_phrase_optimized.json\n",
            "2025-09-01 17:56:16,303 - INFO - Found 377 phrase blocks\n",
            "2025-09-01 17:56:16,304 - INFO - --- START PHRASE: phrase_2 ---\n",
            "2025-09-01 17:56:16,305 - INFO - PHRASE TEXT: A ezetimiba administrada em associação com um inibidor da enzima HMG-CoA redutase (estatina) ou isoladamente, é indicado como terapia adjuvante à dieta para a redução dos níveis elevados de colesterol total (C total), de colesterol da lipoproteína de baixa densidade (LDL-C), da apolipoproteína B (apo B) e dos triglicérides (TG) e para aumentar o colesterol da lipoproteína de alta densidade (HDL-C) em pacientes adultos e adolescentes (10 a 17 anos de idade) com hipercolesterolemia primária (familiar heterozigótica e não familiar).\n",
            "2025-09-01 17:56:16,306 - INFO - PHRASE TYPE: indication\n",
            "2025-09-01 17:56:16,306 - INFO - CONTEXT: Section 1: INDICAÇÕES\n",
            "2025-09-01 17:56:16,307 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 1: INDICAÇÕES\n",
            "TIPO: indication\n",
            "INSTRUÇÃO: Extraia para que condições ou doenças o medicamento é indicado.\n",
            "\n",
            "FRASE: \"A ezetimiba administrada em associação com um inibidor da enzima HMG-CoA redutase (estatina) ou isoladamente, é indicado como terapia adjuvante à dieta para a redução dos níveis elevados de colesterol total (C total), de colesterol da lipoproteína de baixa densidade (LDL-C), da apolipoproteína B (apo B) e dos triglicérides (TG) e para aumentar o colesterol da lipoproteína de alta densidade (HDL-C) em pacientes adultos e adolescentes (10 a 17 anos de idade) com hipercolesterolemia primária (familiar heterozigótica e não familiar).\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:21,125 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"indications\": [\n",
            "  [\"colesterol total\", \"é_indicado_para\", \"redução\"],\n",
            "  [\"colesterol da lipoproteína de baixa densidade (LDL-C)\", \"é_indicado_para\", \"redução\"],\n",
            "  [\"apolipoproteína B (apo B)\", \"é_indicado_para\", \"redução\"],\n",
            "  [\"triglicérides (TG)\", \"é_indicado_para\", \"redução\"],\n",
            "  [\"colesterol da lipoproteína de alta densidade (HDL-C)\", \"é_indicado_para\", \"aumento\"],\n",
            "  [\"hipercolesterolemia primária\", \"é_indicado_para\", \"tratamento\"]\n",
            "]}\n",
            "2025-09-01 17:56:21,127 - INFO - --- END PHRASE: phrase_2 ---\n",
            "\n",
            "2025-09-01 17:56:21,627 - INFO - --- START PHRASE: phrase_3 ---\n",
            "2025-09-01 17:56:21,629 - INFO - PHRASE TEXT: A ezetimiba administrada em combinação com o fenofibrato, é indicado como terapia adjuvante à dieta para redução de níveis elevados de colesterol total, LDL-C, Apo B, e não-HDL-C em pacientes adultos com hiperlipidemia mista.\n",
            "2025-09-01 17:56:21,629 - INFO - PHRASE TYPE: indication\n",
            "2025-09-01 17:56:21,630 - INFO - CONTEXT: Section 1: INDICAÇÕES\n",
            "2025-09-01 17:56:21,630 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 1: INDICAÇÕES\n",
            "TIPO: indication\n",
            "INSTRUÇÃO: Extraia para que condições ou doenças o medicamento é indicado.\n",
            "\n",
            "FRASE: \"A ezetimiba administrada em combinação com o fenofibrato, é indicado como terapia adjuvante à dieta para redução de níveis elevados de colesterol total, LDL-C, Apo B, e não-HDL-C em pacientes adultos com hiperlipidemia mista.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:23,968 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"é_indicado_para\", \"valor\": [\"redução de níveis elevados de colesterol total, LDL-C, Apo B, e não-HDL-C em pacientes adultos com hiperlipidemia mista\"] }\n",
            "2025-09-01 17:56:23,968 - INFO - --- END PHRASE: phrase_3 ---\n",
            "\n",
            "2025-09-01 17:56:24,470 - INFO - --- START PHRASE: phrase_5 ---\n",
            "2025-09-01 17:56:24,470 - INFO - PHRASE TEXT: A ezetimiba administrada em associação com uma estatina é indicado para a redução dos níveis elevados de colesterol total e do LDL-C em pacientes adultos com HFHo.\n",
            "2025-09-01 17:56:24,471 - INFO - PHRASE TYPE: indication\n",
            "2025-09-01 17:56:24,472 - INFO - CONTEXT: Section 1: INDICAÇÕES\n",
            "2025-09-01 17:56:24,472 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 1: INDICAÇÕES\n",
            "TIPO: indication\n",
            "INSTRUÇÃO: Extraia para que condições ou doenças o medicamento é indicado.\n",
            "\n",
            "FRASE: \"A ezetimiba administrada em associação com uma estatina é indicado para a redução dos níveis elevados de colesterol total e do LDL-C em pacientes adultos com HFHo.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:26,174 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"é_indicado_para\", \"valor\": [\"redução dos níveis elevados de colesterol total e do LDL-C em pacientes adultos com HFHo\"]}\n",
            "2025-09-01 17:56:26,175 - INFO - --- END PHRASE: phrase_5 ---\n",
            "\n",
            "2025-09-01 17:56:26,676 - INFO - --- START PHRASE: phrase_6 ---\n",
            "2025-09-01 17:56:26,677 - INFO - PHRASE TEXT: Os pacientes também poderão receber tratamentos adjuvantes (por exemplo, aférese de LDL).\n",
            "2025-09-01 17:56:26,678 - INFO - PHRASE TYPE: indication\n",
            "2025-09-01 17:56:26,678 - INFO - CONTEXT: Section 1: INDICAÇÕES\n",
            "2025-09-01 17:56:26,679 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 1: INDICAÇÕES\n",
            "TIPO: indication\n",
            "INSTRUÇÃO: Extraia para que condições ou doenças o medicamento é indicado.\n",
            "\n",
            "FRASE: \"Os pacientes também poderão receber tratamentos adjuvantes (por exemplo, aférese de LDL).\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:27,559 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Aférese de LDL\": [\"tratamento_adjuvante\", null]}\n",
            "2025-09-01 17:56:27,560 - INFO - --- END PHRASE: phrase_6 ---\n",
            "\n",
            "2025-09-01 17:56:28,061 - INFO - --- START PHRASE: phrase_10 ---\n",
            "2025-09-01 17:56:28,062 - INFO - PHRASE TEXT: em dois estudos multicêntricos, duplos-cegos, controlados com placebo, com 12 semanas de duração e que envolveram 1.\n",
            "2025-09-01 17:56:28,063 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:28,064 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Hipercolesterolemia Primária\n",
            "2025-09-01 17:56:28,064 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Hipercolesterolemia Primária\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"em dois estudos multicêntricos, duplos-cegos, controlados com placebo, com 12 semanas de duração e que envolveram 1.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:29,059 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"estudos_multicentricos\": [\"tem_duracao\", \"12 semanas\"]}\n",
            "2025-09-01 17:56:29,060 - INFO - --- END PHRASE: phrase_10 ---\n",
            "\n",
            "2025-09-01 17:56:29,561 - INFO - --- START PHRASE: phrase_11 ---\n",
            "2025-09-01 17:56:29,562 - INFO - PHRASE TEXT: 719 pacientes com hipercolesterolemia primária, ezetimiba 10 mg reduziu de forma significativa os níveis de colesterol total, LDL-C, apo B e TG e aumentou os níveis de HDL-C em comparação com o placebo (Tabela 1).\n",
            "2025-09-01 17:56:29,563 - INFO - PHRASE TYPE: dosage_instruction\n",
            "2025-09-01 17:56:29,564 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Hipercolesterolemia Primária\n",
            "2025-09-01 17:56:29,564 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Hipercolesterolemia Primária\n",
            "TIPO: dosage_instruction\n",
            "INSTRUÇÃO: Foque em doses, quantidades, frequências de administração.\n",
            "\n",
            "FRASE: \"719 pacientes com hipercolesterolemia primária, ezetimiba 10 mg reduziu de forma significativa os níveis de colesterol total, LDL-C, apo B e TG e aumentou os níveis de HDL-C em comparação com o placebo (Tabela 1).\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:30,576 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"tem_dose_mg\": \"10\"}\n",
            "2025-09-01 17:56:30,577 - INFO - --- END PHRASE: phrase_11 ---\n",
            "\n",
            "2025-09-01 17:56:30,578 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"ezetimiba\", \"tem_dose_mg\": \"10\"}...\n",
            "2025-09-01 17:56:31,079 - INFO - --- START PHRASE: phrase_13 ---\n",
            "2025-09-01 17:56:31,080 - INFO - PHRASE TEXT: Além disso, a ezetimiba não exerceu efeito sobre a concentração plasmática das vitaminas lipossolúveis A, D e E ou sobre o tempo de protrombina e não comprometeu a produção de hormônios esteroides adrenocorticais. Tabela 1:\n",
            "2025-09-01 17:56:31,081 - INFO - PHRASE TYPE: contraindication\n",
            "2025-09-01 17:56:31,082 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Hipercolesterolemia Primária\n",
            "2025-09-01 17:56:31,082 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Hipercolesterolemia Primária\n",
            "TIPO: contraindication\n",
            "INSTRUÇÃO: Identifique quando o medicamento NÃO deve ser usado.\n",
            "\n",
            "FRASE: \"Além disso, a ezetimiba não exerceu efeito sobre a concentração plasmática das vitaminas lipossolúveis A, D e E ou sobre o tempo de protrombina e não comprometeu a produção de hormônios esteroides adrenocorticais. Tabela 1:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:34,158 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"ezetimiba\": [\"não_exerce_efeito_sobre\", \"concentração_plasmática_vitaminas_lipossolúveis_A_D_E\"], \n",
            "\"ezetimiba\": [\"não_exerce_efeito_sobre\", \"tempo_de_protrombina\"], \n",
            "\"ezetimiba\": [\"não_compromete\", \"produção_hormônios_esteroides_adrenocorticais\"]}\n",
            "2025-09-01 17:56:34,159 - INFO - --- END PHRASE: phrase_13 ---\n",
            "\n",
            "2025-09-01 17:56:34,660 - INFO - --- START PHRASE: phrase_14 ---\n",
            "2025-09-01 17:56:34,661 - INFO - PHRASE TEXT: Resposta Média de Pacientes com Hipercolesterolemia Primária (Alteração % Média em Relação ao\n",
            "2025-09-01 17:56:34,661 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:56:34,662 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Hipercolesterolemia Primária\n",
            "2025-09-01 17:56:34,662 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Hipercolesterolemia Primária\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Resposta Média de Pacientes com Hipercolesterolemia Primária (Alteração % Média em Relação ao\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:35,926 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Hipercolesterolemia Primária\": [\"tem_alteração\", \"% Média em Relação ao\"]}\n",
            "2025-09-01 17:56:35,927 - INFO - --- END PHRASE: phrase_14 ---\n",
            "\n",
            "2025-09-01 17:56:36,428 - INFO - --- START PHRASE: phrase_16 ---\n",
            "2025-09-01 17:56:36,429 - INFO - PHRASE TEXT: Dados combinados Placebo 431 0 +1 -2 0 -2 (Estudos 1 e 2) ezetimiba 1.\n",
            "2025-09-01 17:56:36,430 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:36,430 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > ezetimiba 666 -12 -18 -16 -9 +1\n",
            "2025-09-01 17:56:36,431 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > ezetimiba 666 -12 -18 -16 -9 +1\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"Dados combinados Placebo 431 0 +1 -2 0 -2 (Estudos 1 e 2) ezetimiba 1.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:37,636 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"tem_dose\", \"valor\": []}\n",
            "2025-09-01 17:56:37,636 - INFO - --- END PHRASE: phrase_16 ---\n",
            "\n",
            "2025-09-01 17:56:38,138 - INFO - --- START PHRASE: phrase_19 ---\n",
            "2025-09-01 17:56:38,138 - INFO - PHRASE TEXT: em quatro estudos multicêntricos, duplos-cegos, controlados com placebo, com 12 semanas de duração e que envolveram 1.\n",
            "2025-09-01 17:56:38,139 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:38,140 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Coadministração com uma Estatina\n",
            "2025-09-01 17:56:38,140 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Coadministração com uma Estatina\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"em quatro estudos multicêntricos, duplos-cegos, controlados com placebo, com 12 semanas de duração e que envolveram 1.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:39,080 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"estudos\": [\"4\"], \"duração\": [\"12 semanas\"]}\n",
            "2025-09-01 17:56:39,081 - INFO - --- END PHRASE: phrase_19 ---\n",
            "\n",
            "2025-09-01 17:56:39,582 - INFO - --- START PHRASE: phrase_20 ---\n",
            "2025-09-01 17:56:39,583 - INFO - PHRASE TEXT: 187 pacientes com hipercolesterolemia, ezetimiba 10 mg foi administrada isoladamente ou com várias doses de atorvastatina, sinvastatina, pravastatina ou lovastatina.\n",
            "2025-09-01 17:56:39,584 - INFO - PHRASE TYPE: dosage_instruction\n",
            "2025-09-01 17:56:39,585 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Coadministração com uma Estatina\n",
            "2025-09-01 17:56:39,586 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Coadministração com uma Estatina\n",
            "TIPO: dosage_instruction\n",
            "INSTRUÇÃO: Foque em doses, quantidades, frequências de administração.\n",
            "\n",
            "FRASE: \"187 pacientes com hipercolesterolemia, ezetimiba 10 mg foi administrada isoladamente ou com várias doses de atorvastatina, sinvastatina, pravastatina ou lovastatina.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:40,720 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"entidade\": \"ezetimiba\", \"relação\": \"tem_dose\", \"valor\": \"10mg\"}\n",
            "2025-09-01 17:56:40,720 - INFO - --- END PHRASE: phrase_20 ---\n",
            "\n",
            "2025-09-01 17:56:40,721 - WARNING - Could not find a valid JSON array/object in response: {\"entidade\": \"ezetimiba\", \"relação\": \"tem_dose\", \"valor\": \"10mg\"}...\n",
            "2025-09-01 17:56:41,222 - INFO - --- START PHRASE: phrase_21 ---\n",
            "2025-09-01 17:56:41,223 - INFO - PHRASE TEXT: Em geral, o efeito aumentado sobre a redução de LDL-C foi independente da dose ou estatina específica utilizada.\n",
            "2025-09-01 17:56:41,224 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:56:41,225 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Coadministração com uma Estatina\n",
            "2025-09-01 17:56:41,225 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Coadministração com uma Estatina\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Em geral, o efeito aumentado sobre a redução de LDL-C foi independente da dose ou estatina específica utilizada.\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:42,705 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"LDL-C\": \"redução\", \"efeito\": \"aumentado\", \"independente\": [\"dose\", \"estatina específica utilizada\"]}\n",
            "2025-09-01 17:56:42,706 - INFO - --- END PHRASE: phrase_21 ---\n",
            "\n",
            "2025-09-01 17:56:43,207 - INFO - --- START PHRASE: phrase_22 ---\n",
            "2025-09-01 17:56:43,208 - INFO - PHRASE TEXT: Além disso, a redução do LDL-C com ezetimiba coadministrada com a dose mais baixa testada (10 mg) de qualquer uma das estatinas foi semelhante ou maior que a redução do LDL-C observada com a dose mais alta testada da estatina correspondente administrada isoladamente (Tabela 2). Tabela 2:\n",
            "2025-09-01 17:56:43,209 - INFO - PHRASE TYPE: dosage_instruction\n",
            "2025-09-01 17:56:43,210 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Coadministração com uma Estatina\n",
            "2025-09-01 17:56:43,210 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Coadministração com uma Estatina\n",
            "TIPO: dosage_instruction\n",
            "INSTRUÇÃO: Foque em doses, quantidades, frequências de administração.\n",
            "\n",
            "FRASE: \"Além disso, a redução do LDL-C com ezetimiba coadministrada com a dose mais baixa testada (10 mg) de qualquer uma das estatinas foi semelhante ou maior que a redução do LDL-C observada com a dose mais alta testada da estatina correspondente administrada isoladamente (Tabela 2). Tabela 2:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:44,429 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Ezetimiba\": \"coadministra\", \"estatina correspondente\": \"administrada isoladamente\"}\n",
            "2025-09-01 17:56:44,429 - INFO - --- END PHRASE: phrase_22 ---\n",
            "\n",
            "2025-09-01 17:56:44,431 - WARNING - Could not find a valid JSON array/object in response: {\"Ezetimiba\": \"coadministra\", \"estatina correspondente\": \"administrada isoladamente\"}...\n",
            "2025-09-01 17:56:44,932 - INFO - --- START PHRASE: phrase_23 ---\n",
            "2025-09-01 17:56:44,933 - INFO - PHRASE TEXT: Alteração % Média em Relação ao Período Basal na Concentração Plasmática do LDL-C Calculado\n",
            "2025-09-01 17:56:44,934 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:56:44,934 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Coadministração com uma Estatina\n",
            "2025-09-01 17:56:44,935 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Coadministração com uma Estatina\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Alteração % Média em Relação ao Período Basal na Concentração Plasmática do LDL-C Calculado\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:46,100 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"LDL-C\": \"calculado\", \"relação\": \"em relação ao período basal\", \"valor\": \"% média\"}\n",
            "2025-09-01 17:56:46,101 - INFO - --- END PHRASE: phrase_23 ---\n",
            "\n",
            "2025-09-01 17:56:46,102 - WARNING - Could not find a valid JSON array/object in response: {\"LDL-C\": \"calculado\", \"relação\": \"em relação ao período basal\", \"valor\": \"% média\"}...\n",
            "2025-09-01 17:56:46,603 - INFO - --- START PHRASE: phrase_24 ---\n",
            "2025-09-01 17:56:46,603 - INFO - PHRASE TEXT: Ezetimiba + estatina 10 mg -53 -46 -34 -34\n",
            "2025-09-01 17:56:46,604 - INFO - PHRASE TYPE: dosage_instruction\n",
            "2025-09-01 17:56:46,605 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Estatina 10 mg -37 -27 -21 -20\n",
            "2025-09-01 17:56:46,605 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Estatina 10 mg -37 -27 -21 -20\n",
            "TIPO: dosage_instruction\n",
            "INSTRUÇÃO: Foque em doses, quantidades, frequências de administração.\n",
            "\n",
            "FRASE: \"Ezetimiba + estatina 10 mg -53 -46 -34 -34\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:48,044 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Ezetimiba + estatina\": [\"tem_dose\", \"10 mg\"], \n",
            "\"Estatina\": [\"tem_dose\", \"10 mg\"]}\n",
            "2025-09-01 17:56:48,045 - INFO - --- END PHRASE: phrase_24 ---\n",
            "\n",
            "2025-09-01 17:56:48,546 - INFO - --- START PHRASE: phrase_25 ---\n",
            "2025-09-01 17:56:48,546 - INFO - PHRASE TEXT: Ezetimiba + estatina 20 mg -54 -46 -40 -41\n",
            "2025-09-01 17:56:48,547 - INFO - PHRASE TYPE: dosage_instruction\n",
            "2025-09-01 17:56:48,548 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Estatina 20 mg -42 -36 -23 -26\n",
            "2025-09-01 17:56:48,549 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Estatina 20 mg -42 -36 -23 -26\n",
            "TIPO: dosage_instruction\n",
            "INSTRUÇÃO: Foque em doses, quantidades, frequências de administração.\n",
            "\n",
            "FRASE: \"Ezetimiba + estatina 20 mg -54 -46 -40 -41\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:49,610 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Ezetimiba\": \"tem_dose\", \"20 mg\":\n",
            "2025-09-01 17:56:49,611 - INFO - --- END PHRASE: phrase_25 ---\n",
            "\n",
            "2025-09-01 17:56:49,611 - WARNING - Could not find a valid JSON array/object in response: {\"Ezetimiba\": \"tem_dose\", \"20 mg\":...\n",
            "2025-09-01 17:56:50,112 - INFO - --- START PHRASE: phrase_26 ---\n",
            "2025-09-01 17:56:50,113 - INFO - PHRASE TEXT: Ezetimiba + estatina 40 mg -56 -56 -42 -46\n",
            "2025-09-01 17:56:50,114 - INFO - PHRASE TYPE: dosage_instruction\n",
            "2025-09-01 17:56:50,116 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Estatina 40 mg -45 -38 -31 -30\n",
            "2025-09-01 17:56:50,116 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Estatina 40 mg -45 -38 -31 -30\n",
            "TIPO: dosage_instruction\n",
            "INSTRUÇÃO: Foque em doses, quantidades, frequências de administração.\n",
            "\n",
            "FRASE: \"Ezetimiba + estatina 40 mg -56 -56 -42 -46\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:51,467 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Ezetimiba\": [\"tem_dose\", \"40 mg\"], \n",
            "\"Estatina\": [\"tem_dose\", \"40 mg\"]}\n",
            "2025-09-01 17:56:51,468 - INFO - --- END PHRASE: phrase_26 ---\n",
            "\n",
            "2025-09-01 17:56:51,969 - INFO - --- START PHRASE: phrase_27 ---\n",
            "2025-09-01 17:56:51,970 - INFO - PHRASE TEXT: Ezetimiba + estatina 80 mg -61 -58 - -\n",
            "2025-09-01 17:56:51,971 - INFO - PHRASE TYPE: dosage_instruction\n",
            "2025-09-01 17:56:51,971 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > Estatina 80 mg -54 -45 - -\n",
            "2025-09-01 17:56:51,972 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > Estatina 80 mg -54 -45 - -\n",
            "TIPO: dosage_instruction\n",
            "INSTRUÇÃO: Foque em doses, quantidades, frequências de administração.\n",
            "\n",
            "FRASE: \"Ezetimiba + estatina 80 mg -61 -58 - -\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:52,937 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"Ezetimiba + estatina\": [\"tem_dose\", \"80 mg\"]}\n",
            "2025-09-01 17:56:52,938 - INFO - --- END PHRASE: phrase_27 ---\n",
            "\n",
            "2025-09-01 17:56:53,439 - INFO - --- START PHRASE: phrase_28 ---\n",
            "2025-09-01 17:56:53,440 - INFO - PHRASE TEXT: Em uma análise combinada de ezetimiba + todas as doses de estatina, ezetimbaexerceu efeito benéfico sobre o colesterol total, a apo B, os TG e o HDL-C (Tabela 3). Tabela 3:\n",
            "2025-09-01 17:56:53,441 - INFO - PHRASE TYPE: numerical_data\n",
            "2025-09-01 17:56:53,442 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > estatina\n",
            "2025-09-01 17:56:53,442 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > estatina\n",
            "TIPO: numerical_data\n",
            "INSTRUÇÃO: Extraia dados numéricos relevantes (doses, concentrações).\n",
            "\n",
            "FRASE: \"Em uma análise combinada de ezetimiba + todas as doses de estatina, ezetimbaexerceu efeito benéfico sobre o colesterol total, a apo B, os TG e o HDL-C (Tabela 3). Tabela 3:\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:54,467 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"estatina\": \"tem_dose\", \"valor\": \"todas as doses\"}\n",
            "2025-09-01 17:56:54,468 - INFO - --- END PHRASE: phrase_28 ---\n",
            "\n",
            "2025-09-01 17:56:54,469 - WARNING - Could not find a valid JSON array/object in response: {\"estatina\": \"tem_dose\", \"valor\": \"todas as doses\"}...\n",
            "2025-09-01 17:56:54,971 - INFO - --- START PHRASE: phrase_29 ---\n",
            "2025-09-01 17:56:54,972 - INFO - PHRASE TEXT: Análise Combinada da Alteração % Média em Relação ao Período Basal no Colesterol Total, Apo B,\n",
            "2025-09-01 17:56:54,973 - INFO - PHRASE TYPE: general_information\n",
            "2025-09-01 17:56:54,973 - INFO - CONTEXT: Section 2: RESULTADOS DE EFICÁCIA > estatina\n",
            "2025-09-01 17:56:54,974 - INFO - PROMPT SENT:\n",
            "Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
            "\n",
            "CONTEXTO: Section 2: RESULTADOS DE EFICÁCIA > estatina\n",
            "TIPO: general_information\n",
            "INSTRUÇÃO: Extraia qualquer informação farmacêutica relevante.\n",
            "\n",
            "FRASE: \"Análise Combinada da Alteração % Média em Relação ao Período Basal no Colesterol Total, Apo B,\"\n",
            "\n",
            "REGRAS IMPORTANTES:\n",
            "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
            "2. NÃO invente ou suponha informações\n",
            "3. Use nomes de medicamentos exatos quando mencionados\n",
            "4. Para doses, inclua unidades (mg, ml, etc.)\n",
            "5. Se não há informação farmacêutica específica, retorne []\n",
            "\n",
            "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
            "\n",
            "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
            "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
            "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
            "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
            "\n",
            "JSON:\n",
            "2025-09-01 17:56:55,922 - INFO - RAW RESPONSE RECEIVED:\n",
            "{\"estatina\": [\"tem_eficácia_em\", \"colesterol_total\"]}\n",
            "2025-09-01 17:56:55,923 - INFO - --- END PHRASE: phrase_29 ---\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2309469539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2309469539.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚀 Starting batch processing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         extractor.process_directory(\n\u001b[0m\u001b[1;32m    479\u001b[0m             \u001b[0minput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_input_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_output_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2309469539.py\u001b[0m in \u001b[0;36mprocess_directory\u001b[0;34m(self, input_dir, output_dir)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n📊 Progress: Processing file {i + 1}/{len(json_files)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_phrase_based_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0moutput_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_phrase_optimized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_enhanced_graph_data'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2309469539.py\u001b[0m in \u001b[0;36mprocess_phrase_based_json\u001b[0;34m(self, input_file)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mphrase_extractions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mphrase_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphrase_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_phrase_knowledge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mphrase_extractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phrase_blocks_processed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2309469539.py\u001b[0m in \u001b[0;36m_extract_phrase_knowledge\u001b[0;34m(self, phrase_data)\u001b[0m\n\u001b[1;32m    356\u001b[0m             }\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_delay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;31m# ... (Keep other methods like _extract_table_knowledge if you need them) ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pymupdf4llm pdfplumber pandas requests -q\n",
        "\n",
        "# Download and install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start the ollama server process in the background\n",
        "# Its output will be redirected to a log file\n",
        "server_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    stdout=open(\"ollama_server.log\", \"w\"),\n",
        "    stderr=subprocess.STDOUT\n",
        ")\n",
        "\n",
        "print(\"✅ Ollama server started in the background.\")\n",
        "time.sleep(5) # Give the server a moment to initialize\n",
        "\n",
        "# --- CHANGE MADE HERE ---\n",
        "# Pull the Llama 3 8B model instead of the 3B version.\n",
        "print(\"📥 Pulling the Llama 3 8B model. This may take a few minutes...\")\n",
        "!ollama pull llama3:8b\n",
        "print(\"✅ Model download complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNI0qcVCWoHL",
        "outputId": "bdad9293-dfa1-44d3-9e73-02ff84ce4098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h>>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "✅ Ollama server started in the background.\n",
            "📥 Pulling the Llama 3 8B model. This may take a few minutes...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "✅ Model download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "paragraph"
      ],
      "metadata": {
        "id": "mMXd1kJXcqeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Enhanced Pharmaceutical Knowledge Graph Extractor for Google Colab - Phrase-Based JSON Processing\n",
        "\n",
        "Processes phrase-optimized JSON files from the enhanced pharmaceutical document parser.\n",
        "Optimized for small language models with robust extraction and error handling.\n",
        "Includes detailed logging of prompts and responses for debugging model performance.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from google.colab import drive\n",
        "\n",
        "# ==============================================================================\n",
        "# CORE LOGIC: Enhanced PharmaceuticalKnowledgeExtractor CLASS\n",
        "# ==============================================================================\n",
        "\n",
        "class EnhancedPharmaceuticalKnowledgeExtractor:\n",
        "    def __init__(self,\n",
        "                 model_name: str = \"llama3:8b\",  # UPDATED for Llama 3 8B\n",
        "                 ollama_url: str = \"http://localhost:11434/api/generate\",\n",
        "                 max_retries: int = 3,\n",
        "                 request_delay: float = 0.5):\n",
        "        \"\"\"\n",
        "        Initialize the enhanced knowledge extractor optimized for phrase-based JSON files.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.ollama_url = ollama_url\n",
        "        self.max_retries = max_retries\n",
        "        self.request_delay = request_delay\n",
        "\n",
        "        self.stats = {\n",
        "            'files_processed': 0,\n",
        "            'phrase_blocks_processed': 0,\n",
        "            'table_blocks_processed': 0,\n",
        "            'phrases_processed': 0,\n",
        "            'successful_extractions': 0,\n",
        "            'failed_extractions': 0,\n",
        "            'total_triples': 0,\n",
        "            'skipped_irrelevant': 0\n",
        "        }\n",
        "\n",
        "        # Enhanced patterns for better pharmaceutical content detection\n",
        "        self.pharma_keywords = [\n",
        "            'mg', 'ml', 'g/', 'mcg', 'μg', '%', 'dose', 'dosagem', 'posologia',\n",
        "            'comprimido', 'cápsula', 'medicamento', 'fármaco', 'droga',\n",
        "            'indicação', 'indicado', 'tratamento', 'terapia',\n",
        "            'contraindicação', 'contraindicado', 'não usar', 'evitar',\n",
        "            'efeito', 'reação', 'adverso', 'colateral', 'indesejável',\n",
        "            'alergia', 'hipersensibilidade', 'intolerância',\n",
        "            'administração', 'aplicar', 'tomar', 'ingerir',\n",
        "            'composição', 'princípio ativo', 'substância', 'excipiente',\n",
        "            'interação', 'interagir', 'incompatível', 'interferir',\n",
        "            'gravidez', 'gestação', 'lactação', 'amamentação',\n",
        "            'criança', 'pediátrico', 'adulto', 'idoso', 'geriátrico'\n",
        "        ]\n",
        "\n",
        "        self._setup_logging()\n",
        "        self._test_ollama_connection()\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        \"\"\"Setup enhanced logging configuration.\"\"\"\n",
        "        # Remove existing handlers to avoid duplicates in Colab\n",
        "        for handler in logging.root.handlers[:]:\n",
        "            logging.root.removeHandler(handler)\n",
        "\n",
        "        # General logger for progress and errors\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler('enhanced_pharma_extraction.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Dedicated logger for prompts and responses\n",
        "        self.prompt_logger = logging.getLogger('prompt_logger')\n",
        "        self.prompt_logger.setLevel(logging.INFO)\n",
        "        prompt_handler = logging.FileHandler('enhanced_prompts_and_responses.log', mode='w')\n",
        "        prompt_formatter = logging.Formatter('%(message)s')\n",
        "        prompt_handler.setFormatter(prompt_formatter)\n",
        "\n",
        "        # Avoid adding handlers if they already exist\n",
        "        if not self.prompt_logger.handlers:\n",
        "            self.prompt_logger.addHandler(prompt_handler)\n",
        "\n",
        "    def _test_ollama_connection(self):\n",
        "        \"\"\"Test connection to Ollama API with enhanced error reporting.\"\"\"\n",
        "        try:\n",
        "            test_payload = {\n",
        "                \"model\": self.model_name,\n",
        "                \"prompt\": \"Teste de conexão. Responda apenas 'OK'.\",\n",
        "                \"stream\": False,\n",
        "                \"format\": \"json\",\n",
        "                \"options\": {\"temperature\": 0.0, \"num_predict\": 10}\n",
        "            }\n",
        "            response = requests.post(self.ollama_url, json=test_payload, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                self.logger.info(f\"✅ Successfully connected to Ollama with {self.model_name}\")\n",
        "                result = response.json()\n",
        "                self.logger.debug(f\"Test response: {result.get('response', 'No response')}\")\n",
        "            else:\n",
        "                self.logger.warning(f\"⚠️ Ollama connection test failed: {response.status_code} - {response.text}\")\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            self.logger.error(\"❌ Cannot connect to Ollama API. Ensure it's running and accessible from this Colab notebook.\")\n",
        "            raise ConnectionError(\"Cannot connect to Ollama API. Ensure it's running and accessible (e.g., via ngrok).\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"❌ Failed to connect to Ollama: {e}\")\n",
        "            raise ConnectionError(f\"Ollama connection failed: {e}\")\n",
        "\n",
        "    def _call_ollama_api(self, prompt: str, max_tokens: int = 200) -> Optional[str]:\n",
        "        \"\"\"Enhanced API call with better error handling and retry logic.\"\"\"\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False,\n",
        "            \"format\": \"json\",\n",
        "            \"options\": {\n",
        "                \"temperature\": 0.0,  # Deterministic for structured output\n",
        "                \"top_p\": 0.9,\n",
        "                \"top_k\": 20,\n",
        "                \"num_predict\": max_tokens,\n",
        "                \"stop\": [\"\\n\\n\", \"---\", \"Exemplos:\", \"Examples:\", \"Nota:\", \"Note:\"],\n",
        "                \"repeat_penalty\": 1.1,\n",
        "                \"num_ctx\": 4096,  # UPDATED: Context window increased for Llama 3 8B\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                self.logger.debug(f\"API call attempt {attempt + 1}/{self.max_retries}\")\n",
        "                response = requests.post(self.ollama_url, json=payload, timeout=120)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    response_text = result.get('response', '').strip()\n",
        "                    if response_text:\n",
        "                        return response_text\n",
        "                    else:\n",
        "                        self.logger.warning(\"Empty response from API\")\n",
        "                else:\n",
        "                    self.logger.warning(f\"API error {response.status_code}: {response.text[:200]}...\")\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                self.logger.warning(f\"Request timeout on attempt {attempt + 1}\")\n",
        "            except requests.exceptions.ConnectionError:\n",
        "                self.logger.warning(f\"Connection error on attempt {attempt + 1}\")\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Request error on attempt {attempt + 1}: {e}\")\n",
        "\n",
        "            if attempt < self.max_retries - 1:\n",
        "                wait_time = (2 ** attempt) + self.request_delay\n",
        "                self.logger.debug(f\"Waiting {wait_time:.1f}s before retry...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "        self.logger.error(\"All API call attempts failed\")\n",
        "        return None\n",
        "\n",
        "    def _create_enhanced_extraction_prompt(self, phrase: str, context: Dict, phrase_type: str = None) -> str:\n",
        "        \"\"\"Create an enhanced, more specific prompt for small language models.\"\"\"\n",
        "        section_info = context.get('breadcrumb', 'Seção Desconhecida')\n",
        "        phrase_category = phrase_type or context.get('metadata', {}).get('phrase_type', 'geral')\n",
        "\n",
        "        # Create more specific instructions based on phrase type\n",
        "        specific_instructions = {\n",
        "            'dosage_instruction': 'Foque em doses, quantidades, frequências de administração.',\n",
        "            'indication': 'Extraia para que condições ou doenças o medicamento é indicado.',\n",
        "            'contraindication': 'Identifique quando o medicamento NÃO deve ser usado.',\n",
        "            'side_effect': 'Extraia efeitos adversos, reações indesejáveis.',\n",
        "            'precaution': 'Identifique cuidados, precauções, advertências.',\n",
        "            'numerical_data': 'Extraia dados numéricos relevantes (doses, concentrações).',\n",
        "            'general_information': 'Extraia qualquer informação farmacêutica relevante.'\n",
        "        }\n",
        "\n",
        "        instruction = specific_instructions.get(phrase_category, specific_instructions['general_information'])\n",
        "\n",
        "        return f\"\"\"Você é um especialista em extrair informações farmacêuticas. Analise esta frase e extraia APENAS fatos reais como triplas JSON.\n",
        "\n",
        "CONTEXTO: {section_info}\n",
        "TIPO: {phrase_category}\n",
        "INSTRUÇÃO: {instruction}\n",
        "\n",
        "FRASE: \"{phrase}\"\n",
        "\n",
        "REGRAS IMPORTANTES:\n",
        "1. Extraia SOMENTE informações que estão EXPLÍCITAS na frase\n",
        "2. NÃO invente ou suponha informações\n",
        "3. Use nomes de medicamentos exatos quando mencionados\n",
        "4. Para doses, inclua unidades (mg, ml, etc.)\n",
        "5. Se não há informação farmacêutica específica, retorne []\n",
        "\n",
        "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
        "\n",
        "EXEMPLOS DE FORMATO (NÃO COPIE O CONTEÚDO):\n",
        "- [[\"Paracetamol\", \"tem_dose\", \"500mg\"]]\n",
        "- [[\"medicamento\", \"é_indicado_para\", \"dor de cabeça\"]]\n",
        "- [[\"substância\", \"pode_causar\", \"náusea\"]]\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "    def _parse_triples_response_enhanced(self, response: str) -> List[List[str]]:\n",
        "        \"\"\"Enhanced parsing with better error handling and validation.\"\"\"\n",
        "        if not response:\n",
        "            return []\n",
        "\n",
        "        # Clean the response\n",
        "        cleaned = re.sub(r'```json\\s*|```\\s*', '', response.strip())\n",
        "        cleaned = re.sub(r'^[^[]*', '', cleaned)  # Remove text before first [\n",
        "        cleaned = re.sub(r'[^]]*$', ']', cleaned)  # Ensure ends with ]\n",
        "\n",
        "        # Try multiple parsing strategies\n",
        "        strategies = [\n",
        "            self._parse_json_array,\n",
        "            self._parse_regex_triples,\n",
        "            self._parse_fallback_patterns\n",
        "        ]\n",
        "\n",
        "        for strategy in strategies:\n",
        "            try:\n",
        "                triples = strategy(cleaned)\n",
        "                if triples:\n",
        "                    return self._validate_and_filter_triples(triples)\n",
        "            except Exception as e:\n",
        "                self.logger.debug(f\"Parsing strategy failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        self.logger.warning(f\"Could not parse response: {cleaned[:100]}...\")\n",
        "        return []\n",
        "\n",
        "    def _parse_json_array(self, text: str) -> List[List[str]]:\n",
        "        \"\"\"Parse JSON array directly.\"\"\"\n",
        "        # Find the JSON array pattern\n",
        "        array_match = re.search(r'\\[.*?\\]', text, re.DOTALL)\n",
        "        if array_match:\n",
        "            json_str = array_match.group(0)\n",
        "            parsed = json.loads(json_str)\n",
        "            if isinstance(parsed, list):\n",
        "                return parsed\n",
        "        return []\n",
        "\n",
        "    def _parse_regex_triples(self, text: str) -> List[List[str]]:\n",
        "        \"\"\"Parse using regex patterns for triple extraction.\"\"\"\n",
        "        patterns = [\n",
        "            r'\\[\"([^\"]+)\",\\s*\"([^\"]+)\",\\s*\"([^\"]+)\"\\]',  # Standard format\n",
        "            r'\\[\\\"([^\\\"]+)\\\",\\s*\\\"([^\\\"]+)\\\",\\s*\\\"([^\\\"]+)\\\"\\]',  # Escaped quotes\n",
        "            r'<([^>]+)>\\s*,\\s*<([^>]+)>\\s*,\\s*<([^>]+)>'  # Angle bracket format\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            if matches:\n",
        "                return [[str(x).strip() for x in match] for match in matches]\n",
        "        return []\n",
        "\n",
        "    def _parse_fallback_patterns(self, text: str) -> List[List[str]]:\n",
        "        \"\"\"Fallback parsing for malformed but recognizable patterns.\"\"\"\n",
        "        # Look for entity-relation-value patterns\n",
        "        lines = text.split('\\n')\n",
        "        triples = []\n",
        "\n",
        "        for line in lines:\n",
        "            # Pattern: \"entity\" relation \"value\"\n",
        "            pattern = r'\"([^\"]+)\"\\s+(\\w+)\\s+\"([^\"]+)\"'\n",
        "            match = re.search(pattern, line)\n",
        "            if match:\n",
        "                triples.append([match.group(1), match.group(2), match.group(3)])\n",
        "\n",
        "        return triples\n",
        "\n",
        "    def _validate_and_filter_triples(self, triples: List[List[str]]) -> List[List[str]]:\n",
        "        \"\"\"Validate and filter extracted triples for quality.\"\"\"\n",
        "        valid_triples = []\n",
        "\n",
        "        # Filter out common template examples and invalid entries\n",
        "        template_entities = ['medication', 'medicamento', 'paracetamol', 'substância', 'fármaco']\n",
        "        template_values = ['500mg', 'comprimidos', 'dor de cabeça', 'náusea', 'exemplo']\n",
        "\n",
        "        for triple in triples:\n",
        "            if not isinstance(triple, list) or len(triple) != 3:\n",
        "                continue\n",
        "\n",
        "            entity, relation, value = [str(x).strip() for x in triple]\n",
        "\n",
        "            # Skip if any component is empty or too short\n",
        "            if not all([entity, relation, value]) or any(len(x) < 2 for x in [entity, relation, value]):\n",
        "                continue\n",
        "\n",
        "            # Skip template examples\n",
        "            if (entity.lower() in template_entities and\n",
        "                any(tv in value.lower() for tv in template_values)):\n",
        "                continue\n",
        "\n",
        "            # Skip placeholder patterns\n",
        "            if any(x.startswith('<') and x.endswith('>') for x in [entity, relation, value]):\n",
        "                continue\n",
        "\n",
        "            # Skip overly generic relations\n",
        "            generic_relations = ['é', 'tem', 'faz', 'usa']\n",
        "            if relation.lower() in generic_relations and len(value) < 5:\n",
        "                continue\n",
        "\n",
        "            valid_triples.append([entity, relation, value])\n",
        "\n",
        "        return valid_triples\n",
        "\n",
        "    def _should_process_phrase(self, phrase: str, metadata: Dict = None) -> bool:\n",
        "        \"\"\"Enhanced logic to determine if a phrase should be processed.\"\"\"\n",
        "        if len(phrase.strip()) < 15:\n",
        "            return False\n",
        "\n",
        "        phrase_lower = phrase.lower()\n",
        "\n",
        "        # Check for pharmaceutical keywords\n",
        "        has_pharma_content = any(kw in phrase_lower for kw in self.pharma_keywords)\n",
        "\n",
        "        # Check phrase type from metadata\n",
        "        if metadata:\n",
        "            phrase_type = metadata.get('phrase_type', '')\n",
        "            if phrase_type in ['dosage_instruction', 'indication', 'contraindication', 'side_effect']:\n",
        "                return True\n",
        "\n",
        "        # Additional checks for numerical data that might be relevant\n",
        "        has_numbers = bool(re.search(r'\\d', phrase))\n",
        "        has_units = bool(re.search(r'\\d+\\s*(mg|ml|g|%|mcg|μg)', phrase_lower))\n",
        "\n",
        "        return has_pharma_content or has_units or (has_numbers and len(phrase) > 30)\n",
        "\n",
        "    def _extract_phrase_knowledge(self, phrase_data: Dict) -> Dict:\n",
        "        \"\"\"Extract knowledge from a single phrase block with enhanced processing.\"\"\"\n",
        "        phrase_id = phrase_data.get('phrase_id', 'unknown')\n",
        "        phrase_content = phrase_data.get('content', '').strip()\n",
        "        context = phrase_data.get('context', {})\n",
        "        metadata = phrase_data.get('metadata', {})\n",
        "\n",
        "        if not self._should_process_phrase(phrase_content, metadata):\n",
        "            self.stats['skipped_irrelevant'] += 1\n",
        "            return {\n",
        "                'phrase_id': phrase_id,\n",
        "                'triples': [],\n",
        "                'status': 'skipped_irrelevant'\n",
        "            }\n",
        "\n",
        "        self.logger.debug(f\"Processing phrase {phrase_id}: {phrase_content[:50]}...\")\n",
        "        self.stats['phrases_processed'] += 1\n",
        "\n",
        "        try:\n",
        "            phrase_type = metadata.get('phrase_type')\n",
        "            prompt = self._create_enhanced_extraction_prompt(phrase_content, context, phrase_type)\n",
        "\n",
        "            # Log the interaction\n",
        "            self.prompt_logger.info(f\"--- START PHRASE: {phrase_id} ---\")\n",
        "            self.prompt_logger.info(f\"PHRASE TEXT: {phrase_content}\")\n",
        "            self.prompt_logger.info(f\"PHRASE TYPE: {phrase_type}\")\n",
        "            self.prompt_logger.info(f\"CONTEXT: {context.get('breadcrumb', 'N/A')}\")\n",
        "            self.prompt_logger.info(f\"PROMPT SENT:\\n{prompt}\")\n",
        "\n",
        "            response = self._call_ollama_api(prompt, max_tokens=300)\n",
        "\n",
        "            self.prompt_logger.info(f\"RAW RESPONSE RECEIVED:\\n{response}\")\n",
        "            self.prompt_logger.info(f\"--- END PHRASE: {phrase_id} ---\\n\")\n",
        "\n",
        "            if response:\n",
        "                triples = self._parse_triples_response_enhanced(response)\n",
        "                if triples:\n",
        "                    self.stats['successful_extractions'] += 1\n",
        "                    self.stats['total_triples'] += len(triples)\n",
        "                    self.logger.debug(f\"✅ Extracted {len(triples)} triples from phrase {phrase_id}\")\n",
        "                else:\n",
        "                    self.stats['failed_extractions'] += 1\n",
        "\n",
        "                return {\n",
        "                    'phrase_id': phrase_id,\n",
        "                    'phrase_text': phrase_content,\n",
        "                    'phrase_type': phrase_type,\n",
        "                    'context': context.get('breadcrumb'),\n",
        "                    'triples': triples,\n",
        "                    'status': 'success' if triples else 'no_triples_found'\n",
        "                }\n",
        "            else:\n",
        "                self.stats['failed_extractions'] += 1\n",
        "                return {\n",
        "                    'phrase_id': phrase_id,\n",
        "                    'phrase_text': phrase_content,\n",
        "                    'triples': [],\n",
        "                    'status': 'api_failed'\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing phrase {phrase_id}: {e}\")\n",
        "            self.stats['failed_extractions'] += 1\n",
        "            return {\n",
        "                'phrase_id': phrase_id,\n",
        "                'phrase_text': phrase_content,\n",
        "                'triples': [],\n",
        "                'status': f'error: {str(e)}'\n",
        "            }\n",
        "        finally:\n",
        "            time.sleep(self.request_delay)\n",
        "\n",
        "    def _extract_table_knowledge(self, table_data: Dict) -> Dict:\n",
        "        \"\"\"Extract knowledge from table blocks with structured data handling.\"\"\"\n",
        "        table_id = table_data.get('table_id', 'unknown')\n",
        "        content = table_data.get('content', {})\n",
        "        context = table_data.get('context', {})\n",
        "        metadata = table_data.get('metadata', {})\n",
        "\n",
        "        self.logger.info(f\"Processing table {table_id}\")\n",
        "        self.stats['table_blocks_processed'] += 1\n",
        "\n",
        "        # Convert table to text for processing\n",
        "        formatted_text = content.get('formatted_text', '')\n",
        "        header = content.get('header', [])\n",
        "        data_rows = content.get('data_rows', [])\n",
        "\n",
        "        if not formatted_text and not data_rows:\n",
        "            return {\n",
        "                'table_id': table_id,\n",
        "                'triples': [],\n",
        "                'status': 'empty_table'\n",
        "            }\n",
        "\n",
        "        # Process table as structured text\n",
        "        table_text = formatted_text or self._format_table_as_text(header, data_rows)\n",
        "\n",
        "        # Use table-specific processing\n",
        "        try:\n",
        "            prompt = self._create_table_extraction_prompt(table_text, context, metadata)\n",
        "\n",
        "            self.prompt_logger.info(f\"--- START TABLE: {table_id} ---\")\n",
        "            self.prompt_logger.info(f\"TABLE CONTENT:\\n{table_text}\")\n",
        "            self.prompt_logger.info(f\"PROMPT SENT:\\n{prompt}\")\n",
        "\n",
        "            response = self._call_ollama_api(prompt, max_tokens=400)\n",
        "\n",
        "            self.prompt_logger.info(f\"RAW RESPONSE RECEIVED:\\n{response}\")\n",
        "            self.prompt_logger.info(f\"--- END TABLE: {table_id} ---\\n\")\n",
        "\n",
        "            if response:\n",
        "                triples = self._parse_triples_response_enhanced(response)\n",
        "                if triples:\n",
        "                    self.stats['successful_extractions'] += 1\n",
        "                    self.stats['total_triples'] += len(triples)\n",
        "\n",
        "                return {\n",
        "                    'table_id': table_id,\n",
        "                    'table_type': metadata.get('table_type'),\n",
        "                    'context': context.get('breadcrumb'),\n",
        "                    'triples': triples,\n",
        "                    'status': 'success' if triples else 'no_triples_found'\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing table {table_id}: {e}\")\n",
        "            self.stats['failed_extractions'] += 1\n",
        "\n",
        "        return {\n",
        "            'table_id': table_id,\n",
        "            'triples': [],\n",
        "            'status': 'error'\n",
        "        }\n",
        "\n",
        "    def _create_table_extraction_prompt(self, table_text: str, context: Dict, metadata: Dict) -> str:\n",
        "        \"\"\"Create specialized prompt for table data extraction.\"\"\"\n",
        "        table_type = metadata.get('table_type', 'general_data')\n",
        "        section_info = context.get('breadcrumb', 'Tabela')\n",
        "\n",
        "        type_instructions = {\n",
        "            'dosage_schedule': 'Extraia informações de dosagem, horários, frequências.',\n",
        "            'dosage_information': 'Foque em doses, concentrações, quantidades.',\n",
        "            'age_specific_data': 'Extraia dados específicos por idade ou grupo.',\n",
        "            'general_data': 'Extraia qualquer informação farmacêutica estruturada.'\n",
        "        }\n",
        "\n",
        "        instruction = type_instructions.get(table_type, type_instructions['general_data'])\n",
        "\n",
        "        return f\"\"\"Analise esta tabela farmacêutica e extraia informações estruturadas como triplas JSON.\n",
        "\n",
        "CONTEXTO: {section_info}\n",
        "TIPO DE TABELA: {table_type}\n",
        "INSTRUÇÃO: {instruction}\n",
        "\n",
        "TABELA:\n",
        "{table_text}\n",
        "\n",
        "REGRAS:\n",
        "1. Extraia APENAS dados que estão na tabela\n",
        "2. Para doses, mantenha unidades (mg, ml, etc.)\n",
        "3. Preserve nomes de medicamentos exatos\n",
        "4. Se há múltiplas linhas, extraia informação de cada linha relevante\n",
        "5. Use \"linha_N\" ou \"item_N\" para distinguir entradas quando necessário\n",
        "\n",
        "FORMATO: Array JSON de triplas [entidade, relação, valor]\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "    def _format_table_as_text(self, header: List[str], data_rows: List[List[str]]) -> str:\n",
        "        \"\"\"Format table data as readable text.\"\"\"\n",
        "        if not data_rows:\n",
        "            return \"\"\n",
        "\n",
        "        lines = []\n",
        "        if header:\n",
        "            lines.append(\" | \".join(header))\n",
        "            lines.append(\"-\" * (len(\" | \".join(header))))\n",
        "\n",
        "        for row in data_rows:\n",
        "            lines.append(\" | \".join(str(cell) if cell else \"\" for cell in row))\n",
        "\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def process_phrase_based_json(self, input_file: Path) -> Optional[Dict]:\n",
        "        \"\"\"Process a phrase-based JSON file from the enhanced parser.\"\"\"\n",
        "        self.logger.info(f\"📄 Processing phrase-based file: {input_file.name}\")\n",
        "\n",
        "        try:\n",
        "            with open(input_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load {input_file}: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Get the document structure\n",
        "        doc_structure = data.get('document_structure', {})\n",
        "        phrase_blocks = doc_structure.get('phrase_blocks', [])\n",
        "        table_blocks = doc_structure.get('table_blocks', [])\n",
        "\n",
        "        if not phrase_blocks and not table_blocks:\n",
        "            self.logger.warning(f\"No phrase_blocks or table_blocks found in {input_file}\")\n",
        "            return None\n",
        "\n",
        "        self.logger.info(f\"Found {len(phrase_blocks)} phrase blocks and {len(table_blocks)} table blocks\")\n",
        "\n",
        "        # Process phrase blocks\n",
        "        phrase_extractions = []\n",
        "        for phrase_data in phrase_blocks:\n",
        "            result = self._extract_phrase_knowledge(phrase_data)\n",
        "            phrase_extractions.append(result)\n",
        "            self.stats['phrase_blocks_processed'] += 1\n",
        "\n",
        "        # Process table blocks\n",
        "        table_extractions = []\n",
        "        for table_data in table_blocks:\n",
        "            result = self._extract_table_knowledge(table_data)\n",
        "            table_extractions.append(result)\n",
        "\n",
        "        # Collect all triples\n",
        "        all_triples = []\n",
        "        for extraction in phrase_extractions + table_extractions:\n",
        "            all_triples.extend(extraction.get('triples', []))\n",
        "\n",
        "        result = {\n",
        "            'document_metadata': data.get('document_metadata', {}),\n",
        "            'extraction_summary': {\n",
        "                'extraction_timestamp': datetime.now().isoformat(),\n",
        "                'model_used': self.model_name,\n",
        "                'processing_method': 'enhanced_phrase_based',\n",
        "                'total_phrase_blocks': len(phrase_blocks),\n",
        "                'total_table_blocks': len(table_blocks),\n",
        "                'total_phrases_processed': self.stats['phrases_processed'],\n",
        "                'total_triples_extracted': len(all_triples),\n",
        "                'successful_extractions': self.stats['successful_extractions'],\n",
        "                'failed_extractions': self.stats['failed_extractions'],\n",
        "                'skipped_irrelevant': self.stats['skipped_irrelevant']\n",
        "            },\n",
        "            'phrase_extractions': phrase_extractions,\n",
        "            'table_extractions': table_extractions,\n",
        "            'all_extracted_triples': all_triples,\n",
        "            'metadata': data.get('metadata', {})\n",
        "        }\n",
        "\n",
        "        self.stats['files_processed'] += 1\n",
        "        self.logger.info(f\"✅ Completed {input_file.name}: {len(all_triples)} total triples extracted\")\n",
        "        return result\n",
        "\n",
        "    def process_directory(self, input_dir: Path, output_dir: Path):\n",
        "        \"\"\"Process all phrase-optimized JSON files in a directory.\"\"\"\n",
        "        if not input_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Look for phrase-optimized JSON files\n",
        "        json_files = list(input_dir.glob('*_phrase_optimized.json'))\n",
        "        if not json_files:\n",
        "            self.logger.warning(f\"No *_phrase_optimized.json files found in {input_dir}\")\n",
        "            return\n",
        "\n",
        "        self.logger.info(f\"Found {len(json_files)} phrase-optimized files to process\")\n",
        "\n",
        "        for i, json_file in enumerate(json_files):\n",
        "            self.logger.info(f\"\\n📊 Progress: Processing file {i + 1}/{len(json_files)}\")\n",
        "\n",
        "            # Reset per-file counters\n",
        "            prev_phrases = self.stats['phrases_processed']\n",
        "            prev_successful = self.stats['successful_extractions']\n",
        "\n",
        "            result = self.process_phrase_based_json(json_file)\n",
        "\n",
        "            if result:\n",
        "                output_name = json_file.stem.replace('_phrase_optimized', '_enhanced_graph_data') + '.json'\n",
        "                output_file = output_dir / output_name\n",
        "\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "                # Log file-specific stats\n",
        "                phrases_this_file = self.stats['phrases_processed'] - prev_phrases\n",
        "                successful_this_file = self.stats['successful_extractions'] - prev_successful\n",
        "\n",
        "                self.logger.info(f\"💾 Saved results to: {output_file}\")\n",
        "                self.logger.info(f\"📊 File stats: {phrases_this_file} phrases processed, {successful_this_file} successful extractions\")\n",
        "\n",
        "        self._generate_enhanced_report(output_dir)\n",
        "\n",
        "    def _generate_enhanced_report(self, output_dir: Path):\n",
        "        \"\"\"Generate comprehensive final report.\"\"\"\n",
        "        report = {\n",
        "            'summary': {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'model_used': self.model_name,\n",
        "                'processing_method': 'enhanced_phrase_based',\n",
        "                'total_files_processed': self.stats['files_processed'],\n",
        "            },\n",
        "            'detailed_statistics': self.stats,\n",
        "            'performance_metrics': {\n",
        "                'success_rate': (\n",
        "                    self.stats['successful_extractions'] /\n",
        "                    max(self.stats['phrases_processed'], 1) * 100\n",
        "                ),\n",
        "                'avg_triples_per_successful_extraction': (\n",
        "                    self.stats['total_triples'] /\n",
        "                    max(self.stats['successful_extractions'], 1)\n",
        "                ),\n",
        "                'processing_efficiency': {\n",
        "                    'phrases_processed': self.stats['phrases_processed'],\n",
        "                    'relevant_phrases': self.stats['phrases_processed'] - self.stats['skipped_irrelevant'],\n",
        "                    'relevance_rate': (\n",
        "                        (self.stats['phrases_processed'] - self.stats['skipped_irrelevant']) /\n",
        "                        max(self.stats['phrases_processed'], 1) * 100\n",
        "                    )\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        report_file = output_dir / 'enhanced_final_extraction_report.json'\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        self.logger.info(f\"📊 Enhanced final report saved: {report_file}\")\n",
        "        self._print_summary_stats()\n",
        "\n",
        "    def _print_summary_stats(self):\n",
        "        \"\"\"Print summary statistics to console.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"🎯 EXTRACTION SUMMARY\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"📁 Files processed: {self.stats['files_processed']}\")\n",
        "        print(f\"🧩 Phrase blocks processed: {self.stats['phrase_blocks_processed']}\")\n",
        "        print(f\"📋 Table blocks processed: {self.stats['table_blocks_processed']}\")\n",
        "        print(f\"✨ Total phrases analyzed: {self.stats['phrases_processed']}\")\n",
        "        print(f\"✅ Successful extractions: {self.stats['successful_extractions']}\")\n",
        "        print(f\"❌ Failed extractions: {self.stats['failed_extractions']}\")\n",
        "        print(f\"⏭️ Skipped irrelevant: {self.stats['skipped_irrelevant']}\")\n",
        "        print(f\"🔗 Total triples extracted: {self.stats['total_triples']}\")\n",
        "\n",
        "        if self.stats['phrases_processed'] > 0:\n",
        "            success_rate = (self.stats['successful_extractions'] / self.stats['phrases_processed']) * 100\n",
        "            print(f\"📈 Success rate: {success_rate:.1f}%\")\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 🚀 MAIN EXECUTION SECTION\n",
        "# ==============================================================================\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# IMPORTANT: Change these paths to match your Google Drive folders.\n",
        "DRIVE_INPUT_DIR = \"processed_pdfs\"      # Folder with *_phrase_optimized.json files\n",
        "DRIVE_OUTPUT_DIR = \"enhanced_graph_data\" # Where enhanced results will be saved\n",
        "OLLAMA_MODEL = \"llama3:8b\"              # UPDATED: The Ollama model you are running\n",
        "REQUEST_DELAY = 0.5                     # Seconds to wait between API calls\n",
        "MAX_RETRIES = 3                         # Number of times to retry a failed API call\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with enhanced error handling and logging.\"\"\"\n",
        "    print(\"🚀 Starting Enhanced Pharmaceutical Knowledge Graph Extractor\")\n",
        "    print(\"🧩 Optimized for phrase-based JSON processing\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        # Mount Google Drive\n",
        "        print(\"🔧 Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"✅ Google Drive mounted successfully.\")\n",
        "\n",
        "        # Define paths\n",
        "        drive_base_path = Path('/content/drive/MyDrive/')\n",
        "        full_input_path = drive_base_path / DRIVE_INPUT_DIR\n",
        "        full_output_path = drive_base_path / DRIVE_OUTPUT_DIR\n",
        "\n",
        "        print(f\"📁 Input Directory: {full_input_path}\")\n",
        "        print(f\"📂 Output Directory: {full_output_path}\")\n",
        "        print(f\"🤖 Model: {OLLAMA_MODEL}\")\n",
        "        print(f\"🧩 Processing Method: Enhanced phrase-by-phrase\")\n",
        "        print(f\"⏱️  Request Delay: {REQUEST_DELAY}s\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Validate input directory exists\n",
        "        if not full_input_path.exists():\n",
        "            print(f\"❌ ERROR: Input directory not found at '{full_input_path}'\")\n",
        "            print(\"Please ensure you have run the phrase-based document parser first.\")\n",
        "            print(\"The input directory should contain *_phrase_optimized.json files.\")\n",
        "            return\n",
        "\n",
        "        # Check for expected files\n",
        "        json_files = list(full_input_path.glob('*_phrase_optimized.json'))\n",
        "        if not json_files:\n",
        "            print(f\"⚠️  WARNING: No *_phrase_optimized.json files found in {full_input_path}\")\n",
        "            print(\"Please ensure you have run the enhanced document parser to generate phrase-based JSON files.\")\n",
        "\n",
        "            # Show what files are actually there\n",
        "            all_files = list(full_input_path.glob('*.json'))\n",
        "            if all_files:\n",
        "                print(f\"Found {len(all_files)} JSON files:\")\n",
        "                for f in all_files[:5]:  # Show first 5\n",
        "                    print(f\"  - {f.name}\")\n",
        "                if len(all_files) > 5:\n",
        "                    print(f\"  ... and {len(all_files) - 5} more\")\n",
        "            return\n",
        "\n",
        "        print(f\"🔍 Found {len(json_files)} phrase-optimized files to process:\")\n",
        "        for f in json_files:\n",
        "            print(f\"  - {f.name}\")\n",
        "        print()\n",
        "\n",
        "        # Initialize the enhanced extractor\n",
        "        print(\"🤖 Initializing enhanced knowledge extractor...\")\n",
        "        extractor = EnhancedPharmaceuticalKnowledgeExtractor(\n",
        "            model_name=OLLAMA_MODEL,\n",
        "            max_retries=MAX_RETRIES,\n",
        "            request_delay=REQUEST_DELAY\n",
        "        )\n",
        "\n",
        "        # Process all files\n",
        "        print(\"🚀 Starting batch processing...\")\n",
        "        extractor.process_directory(\n",
        "            input_dir=full_input_path,\n",
        "            output_dir=full_output_path\n",
        "        )\n",
        "\n",
        "        print(\"\\n🎉 All files processed successfully!\")\n",
        "        print(f\"📂 Results saved to: {full_output_path}\")\n",
        "        print(\"📊 Check the enhanced_final_extraction_report.json for detailed statistics.\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"❌ ERROR: Directory or file not found.\")\n",
        "        print(f\"Details: {e}\")\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"1. Ensure Google Drive is properly mounted\")\n",
        "        print(\"2. Check that the input directory path is correct\")\n",
        "        print(\"3. Verify that phrase-optimized JSON files exist\")\n",
        "\n",
        "    except ConnectionError as e:\n",
        "        print(f\"❌ ERROR: Could not connect to the Ollama server.\")\n",
        "        print(\"Please ensure your local Ollama instance is running and accessible.\")\n",
        "        print(\"If using Google Colab, you may need to use ngrok to tunnel the connection.\")\n",
        "        print(f\"Details: {e}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"\\n⚠️  Processing interrupted by user.\")\n",
        "        print(\"Partial results may have been saved.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        print(\"Full error traceback:\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpIxb2FGco4N",
        "outputId": "d2f7bda1-da96-4b92-e59f-73c3bea92985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Enhanced Pharmaceutical Knowledge Graph Extractor\n",
            "🧩 Optimized for phrase-based JSON processing\n",
            "======================================================================\n",
            "🔧 Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted successfully.\n",
            "📁 Input Directory: /content/drive/MyDrive/processed_pdfs\n",
            "📂 Output Directory: /content/drive/MyDrive/enhanced_graph_data\n",
            "🤖 Model: llama3:8b\n",
            "🧩 Processing Method: Enhanced phrase-by-phrase\n",
            "⏱️  Request Delay: 0.5s\n",
            "======================================================================\n",
            "⚠️  WARNING: No *_phrase_optimized.json files found in /content/drive/MyDrive/processed_pdfs\n",
            "Please ensure you have run the enhanced document parser to generate phrase-based JSON files.\n",
            "Found 339 JSON files:\n",
            "  - bula_1755192077396_llm_optimized.json\n",
            "  - bula_1755192097944_llm_optimized.json\n",
            "  - bula_1755195358088_llm_optimized.json\n",
            "  - bula_1755195361693_llm_optimized.json\n",
            "  - bula_1755195365369_llm_optimized.json\n",
            "  ... and 334 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Section-Aware Pharmaceutical Document Parser\n",
        "Processes documents sentence by sentence while tracking document sections/headers\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import re\n",
        "import subprocess\n",
        "import shlex\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from datetime import datetime\n",
        "import pymupdf4llm\n",
        "import pdfplumber\n",
        "\n",
        "class SectionAwarePharmaParser:\n",
        "    def __init__(self, model_name: str = \"llama3:8b\"):\n",
        "        \"\"\"\n",
        "        Initialize section-aware parser with header tracking\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.raw_content = \"\"\n",
        "        self.structured_data = {}\n",
        "        self.document_loaded = False\n",
        "\n",
        "        # Common pharmaceutical abbreviations that should NOT end sentences\n",
        "        self.pharma_abbreviations = {\n",
        "            'mg', 'ml', 'mcg', 'kg', 'g', 'l', 'dl', 'mmol', 'mol',  # Units\n",
        "            'q.s.p', 'c.q.s', 'q.s', 'c.s.p',  # Pharmaceutical Latin\n",
        "            'ltda', 'ltd', 'inc', 'corp', 'sa', 'co',  # Company abbreviations\n",
        "            'dr', 'dra', 'prof', 'sr', 'sra',  # Titles\n",
        "            'etc', 'ex', 'vs', 'e.g', 'i.e',  # Common abbreviations\n",
        "            'cnpj', 'cpf', 'rg', 'crf', 'crm',  # Brazilian document types\n",
        "            'anvisa', 'ms', 'rdc', 'vp', 'vps',  # Brazilian regulatory\n",
        "            'd.d', 'p.ex', 'n°', 'nº'  # Other common abbreviations\n",
        "        }\n",
        "\n",
        "        # Brazilian pharmaceutical document section patterns\n",
        "        self.section_patterns = {\n",
        "            # Primary numbered sections\n",
        "            r'^\\s*I+\\)\\s*(.+)$': 'primary_section',  # I), II), III)\n",
        "            r'^\\s*\\d+\\.\\s*(.+)$': 'numbered_section',  # 1., 2., 3.\n",
        "\n",
        "            # Common pharmaceutical sections\n",
        "            r'^\\s*(IDENTIFICAÇÃO|IDENTIFICACAO)\\s*(DO\\s*MEDICAMENTO)?\\s*$': 'identification',\n",
        "            r'^\\s*(INFORMAÇÕES|INFORMACOES)\\s*(AO\\s*PACIENTE)?\\s*$': 'patient_info',\n",
        "            r'^\\s*(COMPOSIÇÃO|COMPOSICAO)\\s*$': 'composition',\n",
        "            r'^\\s*(APRESENTAÇÕES|APRESENTACOES)\\s*$': 'presentations',\n",
        "            r'^\\s*(INDICAÇÕES|INDICACOES)\\s*$': 'indications',\n",
        "            r'^\\s*(CONTRAINDICAÇÕES|CONTRAINDICACOES)\\s*$': 'contraindications',\n",
        "            r'^\\s*(PRECAUÇÕES|PRECAUCOES)\\s*$': 'precautions',\n",
        "            r'^\\s*(REAÇÕES\\s*ADVERSAS|REACOES\\s*ADVERSAS|EFEITOS\\s*ADVERSOS)\\s*$': 'adverse_effects',\n",
        "            r'^\\s*(INTERAÇÕES|INTERACOES)\\s*(MEDICAMENTOSAS)?\\s*$': 'drug_interactions',\n",
        "            r'^\\s*(POSOLOGIA|DOSAGEM)\\s*$': 'dosage',\n",
        "            r'^\\s*(SUPERDOSAGEM|SUPERDOSE)\\s*$': 'overdose',\n",
        "            r'^\\s*ARMAZENAMENTO\\s*$': 'storage',\n",
        "            r'^\\s*DIZERES\\s*LEGAIS\\s*$': 'legal_info',\n",
        "\n",
        "            # Question-style headers\n",
        "            r'^\\s*\\d+\\.\\s*(PARA\\s*QUE|O\\s*QUE|COMO|QUANDO|ONDE|QUAIS)\\s*.*\\?\\s*$': 'question_header'\n",
        "        }\n",
        "\n",
        "        self.setup_ollama()\n",
        "\n",
        "    def setup_ollama(self):\n",
        "        \"\"\"Setup Ollama model automatically\"\"\"\n",
        "        print(f\"Setting up Ollama model: {self.model_name}\")\n",
        "        try:\n",
        "            subprocess.run([\"ollama\", \"--version\"], capture_output=True, check=True)\n",
        "            print(\"✅ Ollama CLI found\")\n",
        "        except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "            raise RuntimeError(\"❌ Ollama CLI not found. Please install Ollama first.\")\n",
        "\n",
        "        try:\n",
        "            print(f\"Pulling model {self.model_name}...\")\n",
        "            result = subprocess.run(\n",
        "                [\"ollama\", \"pull\", self.model_name],\n",
        "                capture_output=True, text=True, timeout=300\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(f\"✅ Model {self.model_name} ready\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error with model setup: {e}\")\n",
        "\n",
        "    def call_ollama_raw(self, prompt: str, extra_flags: str = \"\") -> str:\n",
        "        \"\"\"Call ollama with exact prompt\"\"\"\n",
        "        cmd = [\"ollama\", \"run\", self.model_name]\n",
        "        if extra_flags:\n",
        "            cmd += shlex.split(extra_flags)\n",
        "\n",
        "        try:\n",
        "            proc = subprocess.run(\n",
        "                cmd, input=prompt, text=True, capture_output=True, timeout=60\n",
        "            )\n",
        "            return proc.stdout.strip() or proc.stderr.strip()\n",
        "        except subprocess.TimeoutExpired:\n",
        "            raise RuntimeError(\"Ollama call timed out\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error calling ollama: {e}\")\n",
        "\n",
        "    def extract_pdf_content(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text content from PDF\"\"\"\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                all_text = []\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        all_text.append(page_text)\n",
        "                if all_text:\n",
        "                    return \"\\n\\n\".join(all_text)\n",
        "        except Exception as e:\n",
        "            print(f\"pdfplumber failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            return pymupdf4llm.to_markdown(pdf_path)\n",
        "        except Exception as e:\n",
        "            print(f\"pymupdf4llm failed: {e}\")\n",
        "            raise Exception(\"All extraction methods failed\")\n",
        "\n",
        "    def detect_section_header(self, text: str) -> Tuple[Optional[str], Optional[str]]:\n",
        "        \"\"\"\n",
        "        Detect if text is a section header and return (section_type, section_title)\n",
        "        \"\"\"\n",
        "        text_clean = text.strip()\n",
        "\n",
        "        # Skip very short lines\n",
        "        if len(text_clean) < 3:\n",
        "            return None, None\n",
        "\n",
        "        # Check against section patterns\n",
        "        for pattern, section_type in self.section_patterns.items():\n",
        "            match = re.match(pattern, text_clean, re.IGNORECASE)\n",
        "            if match:\n",
        "                if section_type == 'primary_section' or section_type == 'numbered_section':\n",
        "                    section_title = match.group(1).strip()\n",
        "                else:\n",
        "                    section_title = text_clean\n",
        "                return section_type, section_title\n",
        "\n",
        "        # Check for all-caps headers (common in pharmaceutical docs)\n",
        "        if (text_clean.isupper() and\n",
        "            len(text_clean) > 5 and\n",
        "            len(text_clean) < 100 and\n",
        "            not re.search(r'\\d{2,}', text_clean)):  # Not just numbers\n",
        "            return 'caps_header', text_clean\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def is_likely_abbreviation(self, text: str) -> bool:\n",
        "        \"\"\"Check if text ending with period is likely an abbreviation\"\"\"\n",
        "        if not text or len(text) < 2:\n",
        "            return False\n",
        "\n",
        "        word = text.rstrip('.').lower()\n",
        "\n",
        "        if word in self.pharma_abbreviations:\n",
        "            return True\n",
        "\n",
        "        patterns = [\n",
        "            r'^[a-z]{1,4}$',  # Short lowercase words\n",
        "            r'^[A-Z]{2,6}$',  # All caps short words\n",
        "            r'^[A-Z][a-z]{1,3}$',  # Capitalized short words\n",
        "            r'^\\d+[a-z]+$',  # Numbers with letters\n",
        "            r'^[a-z]\\.[a-z]',  # Pattern like q.s.p\n",
        "            r'[0-9]$'  # Ends with number\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            if re.match(pattern, word):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def smart_sentence_split_with_sections(self, text: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Split text into sentences with section awareness\n",
        "        Returns list of dicts with sentence and section info\n",
        "        \"\"\"\n",
        "        print(\"📋 Splitting text with section tracking...\")\n",
        "\n",
        "        # First split by lines to identify headers\n",
        "        lines = text.split('\\n')\n",
        "\n",
        "        current_section_type = 'unknown'\n",
        "        current_section_title = 'Document Start'\n",
        "        sentence_data = []\n",
        "        current_sentence = \"\"\n",
        "\n",
        "        for line_num, line in enumerate(lines):\n",
        "            line = line.strip()\n",
        "\n",
        "            if not line:  # Skip empty lines\n",
        "                continue\n",
        "\n",
        "            # Check if this line is a section header\n",
        "            section_type, section_title = self.detect_section_header(line)\n",
        "\n",
        "            if section_type and section_title:\n",
        "                # This is a header - finish current sentence if any\n",
        "                if current_sentence.strip():\n",
        "                    sentences = self._split_sentence_safely(current_sentence)\n",
        "                    for sent in sentences:\n",
        "                        if sent.strip() and len(sent.strip()) > 10:\n",
        "                            sentence_data.append({\n",
        "                                'sentence': sent.strip(),\n",
        "                                'section_type': current_section_type,\n",
        "                                'section_title': current_section_title,\n",
        "                                'line_number': line_num,\n",
        "                                'is_header': False\n",
        "                            })\n",
        "                    current_sentence = \"\"\n",
        "\n",
        "                # Update current section\n",
        "                current_section_type = section_type\n",
        "                current_section_title = section_title\n",
        "\n",
        "                # Add header as special sentence\n",
        "                sentence_data.append({\n",
        "                    'sentence': line,\n",
        "                    'section_type': section_type,\n",
        "                    'section_title': section_title,\n",
        "                    'line_number': line_num,\n",
        "                    'is_header': True\n",
        "                })\n",
        "\n",
        "                print(f\"📍 Section detected: {section_type} - {section_title}\")\n",
        "\n",
        "            else:\n",
        "                # Regular content line - add to current sentence\n",
        "                if current_sentence:\n",
        "                    current_sentence += \" \" + line\n",
        "                else:\n",
        "                    current_sentence = line\n",
        "\n",
        "        # Process any remaining sentence\n",
        "        if current_sentence.strip():\n",
        "            sentences = self._split_sentence_safely(current_sentence)\n",
        "            for sent in sentences:\n",
        "                if sent.strip() and len(sent.strip()) > 10:\n",
        "                    sentence_data.append({\n",
        "                        'sentence': sent.strip(),\n",
        "                        'section_type': current_section_type,\n",
        "                        'section_title': current_section_title,\n",
        "                        'line_number': len(lines),\n",
        "                        'is_header': False\n",
        "                    })\n",
        "\n",
        "        # Filter out headers from regular processing\n",
        "        content_sentences = [s for s in sentence_data if not s['is_header']]\n",
        "\n",
        "        print(f\"✅ Found {len(sentence_data)} total items ({len(content_sentences)} content sentences)\")\n",
        "        print(f\"📊 Sections identified: {len(set(s['section_title'] for s in sentence_data))}\")\n",
        "\n",
        "        return content_sentences\n",
        "\n",
        "    def _split_sentence_safely(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into sentences with abbreviation awareness\"\"\"\n",
        "        sentences = []\n",
        "        current_sentence = \"\"\n",
        "\n",
        "        # Split by potential sentence endings\n",
        "        parts = re.split(r'([.!?]+)', text)\n",
        "\n",
        "        i = 0\n",
        "        while i < len(parts):\n",
        "            if i % 2 == 0:  # Text part\n",
        "                current_sentence += parts[i]\n",
        "            else:  # Punctuation part\n",
        "                punctuation = parts[i]\n",
        "                current_sentence += punctuation\n",
        "\n",
        "                if '.' in punctuation:\n",
        "                    words = current_sentence.split()\n",
        "                    if words:\n",
        "                        last_word = words[-1]\n",
        "                        if not self.is_likely_abbreviation(last_word):\n",
        "                            if current_sentence.strip():\n",
        "                                sentences.append(current_sentence.strip())\n",
        "                            current_sentence = \"\"\n",
        "                    else:\n",
        "                        if current_sentence.strip():\n",
        "                            sentences.append(current_sentence.strip())\n",
        "                        current_sentence = \"\"\n",
        "                else:\n",
        "                    # ! or ? - definitely sentence endings\n",
        "                    if current_sentence.strip():\n",
        "                        sentences.append(current_sentence.strip())\n",
        "                    current_sentence = \"\"\n",
        "            i += 1\n",
        "\n",
        "        # Add any remaining sentence\n",
        "        if current_sentence.strip():\n",
        "            sentences.append(current_sentence.strip())\n",
        "\n",
        "        return [s for s in sentences if s.strip() and len(s.strip()) > 10]\n",
        "\n",
        "    def create_section_aware_prompt(self, sentence_data: Dict) -> str:\n",
        "        \"\"\"Create a prompt that includes section context\"\"\"\n",
        "        sentence = sentence_data['sentence']\n",
        "        section_type = sentence_data['section_type']\n",
        "        section_title = sentence_data['section_title']\n",
        "\n",
        "        prompt = f\"\"\"Analyze this sentence from a Brazilian pharmaceutical document. The sentence comes from the \"{section_title}\" section.\n",
        "\n",
        "RESPOND ONLY WITH VALID JSON. No explanations, no markdown.\n",
        "\n",
        "Context: This sentence is from the {section_type} section titled \"{section_title}\".\n",
        "\n",
        "Extract relevant pharmaceutical information considering the section context.\n",
        "\n",
        "Format:\n",
        "{{\n",
        "  \"entities\": [\n",
        "    {{\"type\": \"medication_name\", \"value\": \"...\", \"confidence\": \"high|medium|low\"}},\n",
        "    {{\"type\": \"dosage\", \"value\": \"...\", \"confidence\": \"high|medium|low\"}},\n",
        "    {{\"type\": \"indication\", \"value\": \"...\", \"confidence\": \"high|medium|low\"}},\n",
        "    {{\"type\": \"contraindication\", \"value\": \"...\", \"confidence\": \"high|medium|low\"}},\n",
        "    {{\"type\": \"side_effect\", \"value\": \"...\", \"confidence\": \"high|medium|low\"}},\n",
        "    {{\"type\": \"manufacturer\", \"value\": \"...\", \"confidence\": \"high|medium|low\"}},\n",
        "    {{\"type\": \"storage\", \"value\": \"...\", \"confidence\": \"high|medium|low\"}},\n",
        "    {{\"type\": \"administration\", \"value\": \"...\", \"confidence\": \"high|medium|low\"}}\n",
        "  ],\n",
        "  \"section_relevance\": \"high|medium|low\",\n",
        "  \"key_info_found\": true/false\n",
        "}}\n",
        "\n",
        "Sentence: \"{sentence}\"\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def parse_json_response(self, response: str) -> Any:\n",
        "        \"\"\"Enhanced JSON parsing with better error handling\"\"\"\n",
        "        if not response or not response.strip():\n",
        "            return None\n",
        "\n",
        "        cleaned = response.strip()\n",
        "\n",
        "        # Remove markdown code blocks\n",
        "        if \"```json\" in cleaned:\n",
        "            start = cleaned.find(\"```json\") + 7\n",
        "            end = cleaned.rfind(\"```\")\n",
        "            if start < end:\n",
        "                cleaned = cleaned[start:end].strip()\n",
        "        elif \"```\" in cleaned:\n",
        "            start = cleaned.find(\"```\") + 3\n",
        "            end = cleaned.rfind(\"```\")\n",
        "            if start < end:\n",
        "                cleaned = cleaned[start:end].strip()\n",
        "\n",
        "        # Find JSON boundaries\n",
        "        json_start = cleaned.find('{')\n",
        "        json_end = cleaned.rfind('}') + 1\n",
        "\n",
        "        if json_start != -1 and json_end > json_start:\n",
        "            cleaned = cleaned[json_start:json_end]\n",
        "\n",
        "        try:\n",
        "            return json.loads(cleaned)\n",
        "        except json.JSONDecodeError as e:\n",
        "            try:\n",
        "                # Fix common issues\n",
        "                fixed = re.sub(r',(\\s*[}\\]])', r'\\1', cleaned)\n",
        "                fixed = re.sub(r'(?<!\\\\)\"(?=[^,}\\]]*[,}\\]])', r'\\\\\"', fixed)\n",
        "                return json.loads(fixed)\n",
        "            except:\n",
        "                print(f\"JSON parse error: {cleaned[:100]}...\")\n",
        "                return None\n",
        "\n",
        "    def analyze_sentence_with_context(self, sentence_data: Dict, index: int) -> Dict:\n",
        "        \"\"\"Analyze a sentence with section context\"\"\"\n",
        "        try:\n",
        "            prompt = self.create_section_aware_prompt(sentence_data)\n",
        "            response = self.call_ollama_raw(prompt)\n",
        "\n",
        "            result = self.parse_json_response(response)\n",
        "\n",
        "            if result and isinstance(result, dict):\n",
        "                # Add metadata with null checks\n",
        "                result['sentence_index'] = index\n",
        "                result['original_sentence'] = sentence_data.get('sentence', '')\n",
        "                result['section_type'] = sentence_data.get('section_type', 'unknown')\n",
        "                result['section_title'] = sentence_data.get('section_title', 'Unknown Section')\n",
        "                result['line_number'] = sentence_data.get('line_number', 0)\n",
        "                return result\n",
        "            else:\n",
        "                return self._create_empty_result(sentence_data, index, 'parsing_error')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing sentence {index}: {e}\")\n",
        "            return self._create_empty_result(sentence_data, index, f'analysis_error: {e}')\n",
        "\n",
        "    def _create_empty_result(self, sentence_data: Dict, index: int, error_type: str = None) -> Dict:\n",
        "        \"\"\"Create empty result structure with safe defaults\"\"\"\n",
        "        return {\n",
        "            'entities': [],\n",
        "            'section_relevance': 'low',\n",
        "            'key_info_found': False,\n",
        "            'sentence_index': index,\n",
        "            'original_sentence': sentence_data.get('sentence', ''),\n",
        "            'section_type': sentence_data.get('section_type', 'unknown'),\n",
        "            'section_title': sentence_data.get('section_title', 'Unknown Section'),\n",
        "            'line_number': sentence_data.get('line_number', 0),\n",
        "            'error': error_type\n",
        "        }\n",
        "\n",
        "    def process_sentences_with_context(self, sentence_data_list: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Process all sentences with section context\"\"\"\n",
        "        print(f\"🔍 Analyzing {len(sentence_data_list)} sentences with section context...\")\n",
        "\n",
        "        analyses = []\n",
        "        successful_analyses = 0\n",
        "\n",
        "        for i, sentence_data in enumerate(sentence_data_list):\n",
        "            sentence = sentence_data.get('sentence', '')[:60]\n",
        "            section = sentence_data.get('section_title', 'Unknown')\n",
        "\n",
        "            print(f\"Processing {i+1}/{len(sentence_data_list)} in [{section}]: {sentence}...\")\n",
        "\n",
        "            analysis = self.analyze_sentence_with_context(sentence_data, i)\n",
        "            analyses.append(analysis)\n",
        "\n",
        "            if analysis.get('key_info_found') and not analysis.get('error'):\n",
        "                successful_analyses += 1\n",
        "                entity_count = len(analysis.get('entities', []))\n",
        "                if entity_count > 0:\n",
        "                    print(f\"  ✅ Found {entity_count} entities\")\n",
        "\n",
        "            import time\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        print(f\"✅ Completed analysis: {successful_analyses}/{len(sentence_data_list)} sentences with entities\")\n",
        "        return analyses\n",
        "\n",
        "    def aggregate_entities_by_section(self, analyses: List[Dict]) -> Dict:\n",
        "        \"\"\"Aggregate entities by section with null-safe processing\"\"\"\n",
        "        print(\"📊 Aggregating entities by section...\")\n",
        "\n",
        "        section_entities = {}\n",
        "        all_entities = []\n",
        "        section_stats = {}\n",
        "\n",
        "        for analysis in analyses:\n",
        "            if not analysis or analysis.get('error'):\n",
        "                continue\n",
        "\n",
        "            section_title = analysis.get('section_title', 'Unknown Section')\n",
        "\n",
        "            # Initialize section if not exists\n",
        "            if section_title not in section_entities:\n",
        "                section_entities[section_title] = {\n",
        "                    'medication_names': set(), 'dosages': set(), 'indications': set(),\n",
        "                    'contraindications': set(), 'side_effects': set(), 'manufacturers': set(),\n",
        "                    'storage_conditions': set(), 'administration_info': set()\n",
        "                }\n",
        "                section_stats[section_title] = {'sentences': 0, 'entities': 0}\n",
        "\n",
        "            section_stats[section_title]['sentences'] += 1\n",
        "\n",
        "            entities = analysis.get('entities', [])\n",
        "            if not entities:\n",
        "                continue\n",
        "\n",
        "            section_stats[section_title]['entities'] += len(entities)\n",
        "\n",
        "            for entity in entities:\n",
        "                # --- START: FIX ---\n",
        "                # 1. Check if the entity is a valid dictionary\n",
        "                if not isinstance(entity, dict):\n",
        "                    continue\n",
        "\n",
        "                # 2. Get the type and value, which could be None\n",
        "                entity_type = entity.get('type')\n",
        "                entity_value = entity.get('value')\n",
        "\n",
        "                # 3. Ensure both type and value are not None or empty before stripping\n",
        "                if not entity_type or not entity_value:\n",
        "                    continue\n",
        "\n",
        "                entity_type = entity_type.strip()\n",
        "                entity_value = str(entity_value).strip() # Convert to string to be safe\n",
        "                # --- END: FIX ---\n",
        "\n",
        "                type_mapping = {\n",
        "                    'medication_name': 'medication_names', 'dosage': 'dosages',\n",
        "                    'indication': 'indications', 'contraindication': 'contraindications',\n",
        "                    'side_effect': 'side_effects', 'manufacturer': 'manufacturers',\n",
        "                    'storage': 'storage_conditions', 'administration': 'administration_info'\n",
        "                }\n",
        "\n",
        "                if entity_type in type_mapping:\n",
        "                    collection_key = type_mapping[entity_type]\n",
        "                    section_entities[section_title][collection_key].add(entity_value)\n",
        "\n",
        "                    all_entities.append({\n",
        "                        'type': entity_type, 'value': entity_value,\n",
        "                        'section': section_title,\n",
        "                        'confidence': entity.get('confidence', 'medium'),\n",
        "                        'sentence_index': analysis.get('sentence_index', -1)\n",
        "                    })\n",
        "\n",
        "        # Convert sets to lists for JSON serialization\n",
        "        for section in section_entities:\n",
        "            for entity_type in section_entities[section]:\n",
        "                section_entities[section][entity_type] = list(section_entities[section][entity_type])\n",
        "\n",
        "        result = {\n",
        "            'entities_by_section': section_entities, 'all_entities': all_entities,\n",
        "            'section_statistics': section_stats, 'total_entities': len(all_entities),\n",
        "            'sections_processed': len(section_entities)\n",
        "        }\n",
        "\n",
        "        print(f\"✅ Aggregated {len(all_entities)} entities across {len(section_entities)} sections\")\n",
        "        return result\n",
        "\n",
        "    def process_document(self, pdf_path: str) -> bool:\n",
        "        \"\"\"Main document processing with section awareness\"\"\"\n",
        "        print(f\"📄 Processing document with section-aware analysis: {pdf_path}\")\n",
        "\n",
        "        if not Path(pdf_path).exists():\n",
        "            print(f\"❌ File not found: {pdf_path}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Extract text\n",
        "            print(\"📖 Extracting text from PDF...\")\n",
        "            self.raw_content = self.extract_pdf_content(pdf_path)\n",
        "\n",
        "            if not self.raw_content:\n",
        "                print(\"❌ No text content extracted\")\n",
        "                return False\n",
        "\n",
        "            print(f\"✅ Extracted {len(self.raw_content)} characters\")\n",
        "\n",
        "            # Split with section awareness\n",
        "            sentence_data_list = self.smart_sentence_split_with_sections(self.raw_content)\n",
        "\n",
        "            # Process sentences with context\n",
        "            analyses = self.process_sentences_with_context(sentence_data_list)\n",
        "\n",
        "            # Aggregate by sections\n",
        "            aggregated_data = self.aggregate_entities_by_section(analyses)\n",
        "\n",
        "            # Compile results\n",
        "            self.structured_data = {\n",
        "                \"metadata\": {\n",
        "                    \"file_path\": pdf_path,\n",
        "                    \"file_name\": Path(pdf_path).name,\n",
        "                    \"processing_date\": datetime.now().isoformat(),\n",
        "                    \"total_text_length\": len(self.raw_content),\n",
        "                    \"model_used\": self.model_name,\n",
        "                    \"extraction_method\": \"section_aware_sentence_analysis\"\n",
        "                },\n",
        "                \"sentence_analyses\": analyses,\n",
        "                \"section_entities\": aggregated_data,\n",
        "                \"processing_statistics\": {\n",
        "                    \"total_sentences\": len(sentence_data_list),\n",
        "                    \"sentences_with_entities\": len([a for a in analyses if a.get('key_info_found')]),\n",
        "                    \"total_entities_found\": aggregated_data.get('total_entities', 0),\n",
        "                    \"sections_identified\": aggregated_data.get('sections_processed', 0)\n",
        "                }\n",
        "            }\n",
        "\n",
        "            self.document_loaded = True\n",
        "            print(\"✅ Section-aware document processing completed!\")\n",
        "            self._show_processing_summary()\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing document: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    def _show_processing_summary(self):\n",
        "        \"\"\"Show processing summary with section information\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"📊 SECTION-AWARE PROCESSING SUMMARY\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        metadata = self.structured_data.get(\"metadata\", {})\n",
        "        stats = self.structured_data.get(\"processing_statistics\", {})\n",
        "        section_data = self.structured_data.get(\"section_entities\", {})\n",
        "\n",
        "        print(f\"📁 File: {metadata.get('file_name', 'Unknown')}\")\n",
        "        print(f\"📝 Text length: {metadata.get('total_text_length', 0):,} characters\")\n",
        "        print(f\"✂️ Total sentences: {stats.get('total_sentences', 0)}\")\n",
        "        print(f\"🏷️ Sentences with entities: {stats.get('sentences_with_entities', 0)}\")\n",
        "        print(f\"🔢 Total entities found: {stats.get('total_entities_found', 0)}\")\n",
        "        print(f\"📋 Sections identified: {stats.get('sections_identified', 0)}\")\n",
        "\n",
        "        # Show section statistics\n",
        "        section_stats = section_data.get('section_statistics', {})\n",
        "        if section_stats:\n",
        "            print(f\"\\n📊 Entity Distribution by Section:\")\n",
        "            for section, stat in section_stats.items():\n",
        "                print(f\"   {section}: {stat['entities']} entities from {stat['sentences']} sentences\")\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    def save_results(self, output_path: Optional[str] = None) -> str:\n",
        "        \"\"\"Save processing results\"\"\"\n",
        "        if not self.document_loaded:\n",
        "            raise Exception(\"No document processed\")\n",
        "\n",
        "        if not output_path:\n",
        "            file_name = self.structured_data[\"metadata\"][\"file_name\"]\n",
        "            pdf_name = Path(file_name).stem\n",
        "            output_path = f\"{pdf_name}_section_aware_analysis.json\"\n",
        "\n",
        "        output_file = Path(output_path)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.structured_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\n📄 Results saved to: {output_file}\")\n",
        "        print(f\"📊 File size: {output_file.stat().st_size:,} bytes\")\n",
        "        return str(output_file)\n",
        "\n",
        "    def query_document(self, question: str) -> str:\n",
        "        \"\"\"Query with section context\"\"\"\n",
        "        if not self.document_loaded:\n",
        "            return \"❌ No document processed.\"\n",
        "\n",
        "        section_entities = self.structured_data.get(\"section_entities\", {}).get(\"entities_by_section\", {})\n",
        "\n",
        "        context_parts = [\"MEDICATION INFORMATION BY SECTION:\\n\"]\n",
        "\n",
        "        for section, entities in section_entities.items():\n",
        "            if any(entities.values()):  # Only show sections with entities\n",
        "                context_parts.append(f\"[{section}]\")\n",
        "                for entity_type, values in entities.items():\n",
        "                    if values:\n",
        "                        context_parts.append(f\"  {entity_type}: {', '.join(values[:3])}\")\n",
        "                context_parts.append(\"\")\n",
        "\n",
        "        context = \"\\n\".join(context_parts)\n",
        "\n",
        "        query_prompt = f\"\"\"Answer about this medication based on the section-organized information.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Available Information:\n",
        "{context[:4000]}\n",
        "\n",
        "Provide a clear answer in Portuguese, mentioning the relevant sections when appropriate.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.call_ollama_raw(query_prompt)\n",
        "            return response.strip() if response else \"No response received\"\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error: {e}\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"Demonstrate section-aware processing\"\"\"\n",
        "    parser = SectionAwarePharmaParser(model_name=\"llama3:8b\")\n",
        "\n",
        "    pdf_path = \"bula_1755192077396.pdf\"\n",
        "\n",
        "    print(\"🎯 Section-Aware Pharmaceutical Document Parser\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if parser.process_document(pdf_path):\n",
        "        results_file = parser.save_results()\n",
        "\n",
        "        print(f\"\\n🔍 Testing section-aware querying...\")\n",
        "        test_questions = [\n",
        "            \"Qual é o nome do medicamento e sua concentração?\",\n",
        "            \"Quais são as contraindicações principais?\",\n",
        "            \"Como deve ser administrado?\",\n",
        "            \"Quais são os efeitos adversos mais comuns?\",\n",
        "            \"Quem é o fabricante?\"\n",
        "        ]\n",
        "\n",
        "        for question in test_questions:\n",
        "            print(f\"\\n❓ {question}\")\n",
        "            answer = parser.query_document(question)\n",
        "            print(f\"💡 {answer}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUhvJFf73fqP",
        "outputId": "ded5c664-a150-4942-aa9d-ff4975dbb2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up Ollama model: llama3:8b\n",
            "✅ Ollama CLI found\n",
            "Pulling model llama3:8b...\n",
            "✅ Model llama3:8b ready\n",
            "🎯 Section-Aware Pharmaceutical Document Parser\n",
            "======================================================================\n",
            "📄 Processing document with section-aware analysis: bula_1755192077396.pdf\n",
            "📖 Extracting text from PDF...\n",
            "✅ Extracted 11936 characters\n",
            "📋 Splitting text with section tracking...\n",
            "📍 Section detected: primary_section - IDENTIFICAÇÃO DO MEDICAMENTO\n",
            "📍 Section detected: presentations - APRESENTAÇÕES\n",
            "📍 Section detected: caps_header - USO ORAL\n",
            "📍 Section detected: caps_header - USO ADULTO E PEDIÁTRICO ACIMA DE 6 ANOS DE IDADE\n",
            "📍 Section detected: composition - COMPOSIÇÃO\n",
            "📍 Section detected: primary_section - INFORMAÇÕES AO PACIENTE\n",
            "📍 Section detected: numbered_section - PARA QUE ESTE MEDICAMENTO É INDICADO?\n",
            "📍 Section detected: numbered_section - COMO ESTE MEDICAMENTO FUNCIONA?\n",
            "📍 Section detected: numbered_section - QUANDO NÃO DEVO USAR ESTE MEDICAMENTO?\n",
            "📍 Section detected: numbered_section - O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "📍 Section detected: caps_header - QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "📍 Section detected: numbered_section - ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "📍 Section detected: numbered_section - COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "📍 Section detected: numbered_section - O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?\n",
            "📍 Section detected: numbered_section - QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "📍 Section detected: numbered_section - O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE\n",
            "📍 Section detected: caps_header - MEDICAMENTO?\n",
            "📍 Section detected: primary_section - DIZERES LEGAIS\n",
            "📍 Section detected: caps_header - VENDA SOB PRESCRIÇÃO MÉDICA\n",
            "✅ Found 99 total items (80 content sentences)\n",
            "📊 Sections identified: 20\n",
            "🔍 Analyzing 80 sentences with section context...\n",
            "Processing 1/80 in [Document Start]: ezetimiba Sandoz do Brasil Ind. Farm. Ltda. Bula do Paciente...\n",
            "  ✅ Found 3 entities\n",
            "Processing 2/80 in [IDENTIFICAÇÃO DO MEDICAMENTO]: ezetimiba Medicamento genérico, Lei nº 9.787, de 1999...\n",
            "  ✅ Found 2 entities\n",
            "Processing 3/80 in [APRESENTAÇÕES]: ezetimiba comprimido 10 mg. Embalagem contendo 30 ou 60 comp...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 4/80 in [COMPOSIÇÃO]: Cada comprimido de 10 mg contém: ezetimiba.....................\n",
            "  ✅ Found 2 entities\n",
            "Processing 5/80 in [COMPOSIÇÃO]: 10 mg excipientes q.s.p. ....................\n",
            "  ✅ Found 2 entities\n",
            "Processing 6/80 in [COMPOSIÇÃO]: 1 comprimido (lactose monoidratada, hipromelose, croscarmelo...\n",
            "  ✅ Found 2 entities\n",
            "Processing 7/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: A ezetimiba é indicada para reduzir a quantidade de colester...\n",
            "  ✅ Found 2 entities\n",
            "Processing 8/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: O colesterol é uma das várias substâncias gordurosas encontr...\n",
            "  ✅ Found 2 entities\n",
            "Processing 9/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: O colesterol total é composto principalmente de colesterol L...\n",
            "  ✅ Found 8 entities\n",
            "Processing 10/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: Eventualmente, essas placas podem causar estreitamento das a...\n",
            "  ✅ Found 3 entities\n",
            "Processing 11/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: Esse bloqueio ao fluxo sanguíneo pode causar ataque cardíaco...\n",
            "  ✅ Found 2 entities\n",
            "Processing 12/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: O colesterol HDL, por sua vez, é frequentemente chamado de “...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": null, \"confidence\": \"low\"},\n",
            "    {\"type\": ...\n",
            "Processing 13/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: Outra forma de gordura no sangue que pode aumentar o risco d...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"\", \"confidence\": \"low\"},\n",
            "    {\"type\": \"i...\n",
            "Processing 14/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: Se você tem sitosterolemia, seu médico prescreveu ezetimiba ...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 15/80 in [COMO ESTE MEDICAMENTO FUNCIONA?]: A ezetimiba age ao reduzir a absorção do colesterol no intes...\n",
            "  ✅ Found 8 entities\n",
            "Processing 16/80 in [COMO ESTE MEDICAMENTO FUNCIONA?]: Portanto, ezetimiba aumenta o efeito redutor do colesterol d...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 17/80 in [COMO ESTE MEDICAMENTO FUNCIONA?]: O colesterol alto pode ser tratado de duas formas principais...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"Medicamentos redutores do colesterol\", \"...\n",
            "Processing 18/80 in [COMO ESTE MEDICAMENTO FUNCIONA?]: Seu médico prescreveu ezetimiba para ajudar a reduzir o seu ...\n",
            "  ✅ Found 2 entities\n",
            "Processing 19/80 in [QUANDO NÃO DEVO USAR ESTE MEDICAMENTO?]: Pacientes com hipersensibilidade (alérgicos) a ezetimiba ou ...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 20/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: É importante que continue a tomar ezetimiba diariamente conf...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 21/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Mesmo tomando medicamentos para tratar o colesterol alto, é ...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"medicamentos para tratar o colesterol al...\n",
            "Processing 22/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Além disso, é importante que você conheça seus níveis atuais...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"\", \"confidence\": \"low\"},\n",
            "    {\"type\": \"i...\n",
            "Processing 23/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Gravidez e Amamentação: se estiver grávida ou planeja engrav...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 24/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Se estiver amamentando, ezetimiba pode passar do seu leite p...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 25/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Este medicamento não deve ser utilizado por mulheres grávida...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"...\", \"confidence\": \"low\"},\n",
            "    {\"type\":...\n",
            "Processing 26/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Idosos: não há precauções especiais....\n",
            "  ✅ Found 2 entities\n",
            "Processing 27/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: ezetimiba – VP06 Uso pediátrico: A ezetimiba não é recomenda...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 28/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Dirigir ou Operar Máquinas: foram relatados efeitos adversos...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 29/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: As respostas individuais a ezetimiba podem variar (veja o it...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 30/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: QUAIS OS MALES...\n",
            "  ✅ Found 1 entities\n",
            "Processing 31/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Problemas Clínicos ou Alergias: informe ao seu médico quaisq...\n",
            "  ✅ Found 2 entities\n",
            "Processing 32/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Interações Medicamentosas: Você deve sempre informar seu méd...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"\", \"confidence\": \"low\"},\n",
            "    {\"type\": \"m...\n",
            "Processing 33/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Informe ao seu médico ou cirurgião-dentista se você está faz...\n",
            "  ✅ Found 2 entities\n",
            "Processing 34/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Não use medicamento sem o conhecimento do seu médico....\n",
            "  ✅ Found 2 entities\n",
            "Processing 35/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Pode ser perigoso para a sua saúde....\n",
            "  ✅ Found 1 entities\n",
            "Processing 36/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Esse medicamento não é recomendado para crianças com menos d...\n",
            "  ✅ Found 3 entities\n",
            "Processing 37/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Mantenha em temperatura ambiente (temperatura entre 15 e 30º...\n",
            "  ✅ Found 2 entities\n",
            "Processing 38/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Proteger da umidade....\n",
            "  ✅ Found 1 entities\n",
            "Processing 39/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Número de lote e datas de fabricação e validade: vide embala...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"batch_number\", \"value\": \"número de lote\", \"confidence\": \"high\"},\n",
            "   ...\n",
            "Processing 40/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Não use medicamento com o prazo de validade vencido....\n",
            "  ✅ Found 2 entities\n",
            "Processing 41/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Guarde-o em sua embalagem original....\n",
            "  ✅ Found 1 entities\n",
            "Processing 42/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Aparência: A ezetimiba é um comprimido oval branco a quase b...\n",
            "  ✅ Found 8 entities\n",
            "Processing 43/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Caso ele esteja no prazo de validade e você observe alguma m...\n",
            "  ✅ Found 2 entities\n",
            "Processing 44/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Todo medicamento deve ser mantido fora do alcance das crianç...\n",
            "  ✅ Found 2 entities\n",
            "Processing 45/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Adultos e crianças acima de 6 anos de idade: Tome um comprim...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 46/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Seu médico pode ter falado para você tomar ezetimiba com out...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 47/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Se seu médico prescreveu ezetimiba com colestiramina (um seq...\n",
            "  ✅ Found 8 entities\n",
            "Processing 48/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: A ezetimiba deve ser tomada conforme seu médico orientou....\n",
            "  ✅ Found 2 entities\n",
            "Processing 49/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Continue a tomar outros medicamentos redutores de colesterol...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"medicamentos redutores de colesterol\", \"...\n",
            "Processing 50/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Siga a orientação de seu médico, respeitando sempre os horár...\n",
            "  ✅ Found 8 entities\n",
            "Processing 51/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Não interrompa o tratamento sem o conhecimento do seu médico...\n",
            "  ✅ Found 8 entities\n",
            "Processing 52/80 in [O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?]: Tente tomar ezetimiba conforme prescrito....\n",
            "  ✅ Found 2 entities\n",
            "Processing 53/80 in [O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?]: Entretanto, se esquecer de tomar uma dose, reinicie o esquem...\n",
            "  ✅ Found 8 entities\n",
            "Processing 54/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Nos estudos clínicos, a ezetimiba foi em geral bem tolerada....\n",
            "  ✅ Found 2 entities\n",
            "Processing 55/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Os efeitos adversos geralmente foram leves e semelhantes em ...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"side_effect\", \"value\": \"efeitos adversos\", \"confidence\": \"high\"},\n",
            "  ...\n",
            "Processing 56/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Em geral, os efeitos adversos não provocaram a interrupção d...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 57/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Quando ezetimiba foi usada isoladamente, foram relatados os ...\n",
            "  ✅ Found 5 entities\n",
            "Processing 58/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Incomuns: elevações nos exames de sangue da função hepática ...\n",
            "  ✅ Found 12 entities\n",
            "Processing 59/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Além disso, quando tomado com uma estatina, foram relatados ...\n",
            "  ✅ Found 4 entities\n",
            "Processing 60/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Incomuns: sensação de formigamento; boca seca; coceira; erup...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"side_effect\", \"value\": \"sensação de formigamento\", \"confidence\": \"hi...\n",
            "Processing 61/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Ao ser utilizado com fenofibratos, o seguinte efeito adverso...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"fenofibratos\", \"confidence\": \"high\"},\n",
            "  ...\n",
            "Processing 62/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Além disso, foram relatados os seguintes efeitos adversos no...\n",
            "  ✅ Found 14 entities\n",
            "Processing 63/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: ezetimiba – VP06 Procure seu médico imediatamente se sentir ...\n",
            "  ✅ Found 2 entities\n",
            "Processing 64/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Embora raramente, esses problemas musculares podem ser grave...\n",
            "  ✅ Found 3 entities\n",
            "Processing 65/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Se ezetimiba foi prescrita para ser tomado com uma estatina,...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 66/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Informe ao seu médico, cirurgião-dentista ou farmacêutico o ...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"do medicamento\", \"confidence\": \"high\"},\n",
            "...\n",
            "Processing 67/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Informe também à empresa através do seu serviço de atendimen...\n",
            "  ✅ Found 1 entities\n",
            "Processing 68/80 in [MEDICAMENTO?]: Tome ezetimiba apenas conforme prescrito....\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"ezetimiba\", \"confidence\": \"high\"},\n",
            "    {...\n",
            "Processing 69/80 in [MEDICAMENTO?]: Se tomar mais ezetimiba do que o prescrito, entre em contato...\n",
            "  ✅ Found 2 entities\n",
            "Processing 70/80 in [MEDICAMENTO?]: Em caso de uso de grande quantidade deste medicamento, procu...\n",
            "  ✅ Found 3 entities\n",
            "Processing 71/80 in [MEDICAMENTO?]: Ligue para 0800 722 6001, se você precisar de mais orientaçõ...\n",
            "Processing 72/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: Reg. M.S.: 1.0047....\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"medication_name\", \"value\": \"\", \"confidence\": \"low\"},\n",
            "    {\"type\": \"r...\n",
            "Processing 73/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: 0538 Farm. Resp.: Cláudia Larissa S. Montanher CRF-PR nº 17....\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"manufacturer\", \"value\": \"Farm.\", \"confidence\": \"high\"},\n",
            "    {\"type\":...\n",
            "Processing 74/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: 379 Esta bula foi aprovada pela Anvisa em 21/07/2023....\n",
            "Processing 75/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: Fabricado por: Lek Pharmaceuticals, d.d. Ljubljana - Eslovên...\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"manufacturer\", \"value\": \"Lek Pharmaceuticals, d.d.\", \"confidence\": \"...\n",
            "Processing 76/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: 647/0001-16 Indústria Brasileira Ou Fabricado por: Lek Pharm...\n",
            "  ✅ Found 2 entities\n",
            "Processing 77/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: 647/0001-16 Indústria Brasileira ezetimiba – VP06 Histórico ...\n",
            "  ✅ Found 4 entities\n",
            "Processing 78/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: RDC 60/12 -6....\n",
            "JSON parse error: {\n",
            "  \"entities\": [\n",
            "    {\"type\": \"regulatory_document\", \"value\": \"RDC 60/12\", \"confidence\": \"high\"},\n",
            " ...\n",
            "Processing 79/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: Como devo usar este medicamento?...\n",
            "Processing 80/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: Notificação de Inclusão de local de Alteração de 10 mg 15/04...\n",
            "  ✅ Found 2 entities\n",
            "✅ Completed analysis: 45/80 sentences with entities\n",
            "📊 Aggregating entities by section...\n",
            "✅ Aggregated 105 entities across 13 sections\n",
            "✅ Section-aware document processing completed!\n",
            "\n",
            "======================================================================\n",
            "📊 SECTION-AWARE PROCESSING SUMMARY\n",
            "======================================================================\n",
            "📁 File: bula_1755192077396.pdf\n",
            "📝 Text length: 11,936 characters\n",
            "✂️ Total sentences: 80\n",
            "🏷️ Sentences with entities: 45\n",
            "🔢 Total entities found: 105\n",
            "📋 Sections identified: 13\n",
            "\n",
            "📊 Entity Distribution by Section:\n",
            "   Document Start: 3 entities from 1 sentences\n",
            "   IDENTIFICAÇÃO DO MEDICAMENTO: 2 entities from 1 sentences\n",
            "   COMPOSIÇÃO: 6 entities from 3 sentences\n",
            "   PARA QUE ESTE MEDICAMENTO É INDICADO?: 17 entities from 5 sentences\n",
            "   COMO ESTE MEDICAMENTO FUNCIONA?: 10 entities from 2 sentences\n",
            "   O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?: 3 entities from 2 sentences\n",
            "   QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).: 10 entities from 5 sentences\n",
            "   ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?: 18 entities from 7 sentences\n",
            "   COMO DEVO USAR ESTE MEDICAMENTO?: 26 entities from 4 sentences\n",
            "   O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?: 10 entities from 2 sentences\n",
            "   QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?: 43 entities from 8 sentences\n",
            "   MEDICAMENTO?: 5 entities from 3 sentences\n",
            "   VENDA SOB PRESCRIÇÃO MÉDICA: 8 entities from 5 sentences\n",
            "======================================================================\n",
            "\n",
            "📄 Results saved to: bula_1755192077396_section_aware_analysis.json\n",
            "📊 File size: 89,347 bytes\n",
            "\n",
            "🔍 Testing section-aware querying...\n",
            "\n",
            "❓ Qual é o nome do medicamento e sua concentração?\n",
            "💡 O nome do medicamento é Ezetimiba e sua concentração é de 10 mg por comprimido (ver seção [COMPOSIÇÃO]).\n",
            "--------------------------------------------------\n",
            "\n",
            "❓ Quais são as contraindicações principais?\n",
            "💡 As contraindicações principais para o uso do medicamento ezetimiba são:\n",
            "\n",
            "* Não há precauções especiais (§ O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?).\n",
            "* Uso de outro medicamento, crianças com menos de 6 anos de idade e problemas clínicos ou alergias (§ QUE ESTE MEDICAMENTO PODE ME CAUSAR?\").\n",
            "* Any other bile acid sequestrant (§ COMO DEVO USAR ESTE MEDICAMENTO?).\n",
            "* Uso excessivo do medicamento (§ MEDICAMENTO?).\n",
            "--------------------------------------------------\n",
            "\n",
            "❓ Como deve ser administrado?\n",
            "💡 Para administrar o medicamento ezetimiba corretamente, você deve seguir as seguintes instruções:\n",
            "\n",
            "* Como usar este medicamento? [COMO DEVO USAR ESTE MEDICAMENTO?] - Tome um comprimido por dia, conforme prescrito.\n",
            "* O que devo fazer quando eu me esquecer de usar este medicamento? [O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?] - Tome um comprimido por dia.\n",
            "\n",
            "Além disso, é importante lembrar que o medicamento deve ser tomado com pelo menos 2 horas antes ou 4 horas após a ingestão de outro medicamento chamado sequestrante de ácido bilárquico [COMO DEVO USAR ESTE MEDICAMENTO?].\n",
            "--------------------------------------------------\n",
            "\n",
            "❓ Quais são os efeitos adversos mais comuns?\n",
            "💡 Os efeitos adversos mais comuns do medicamento ezetimiba incluem:\n",
            "\n",
            "* Artérias constritas (seção \"COMO ESTE MEDICAMENTO FUNCIONA?\")\n",
            "* Redução ou bloqueio do fluxo sanguíneo para órgãos vitais, ataque cardíaco ou derrame (seção \"PARA QUE ESTE MEDICAMENTO É INDICADO?\")\n",
            "* Indigestão, dor nas articulações e depressão (seção \"QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\")\n",
            "\n",
            "É importante lembrar que esses efeitos adversos podem ocorrer em casos mais raros e podem variar dependendo da dose e do período de uso do medicamento. Além disso, é fundamental seguir as orientações do seu médico e usar o medicamento de acordo com as instruções prescritas.\n",
            "--------------------------------------------------\n",
            "\n",
            "❓ Quem é o fabricante?\n",
            "💡 O fabricante desse medicamento é a Do Brasil Ind. Farm. Ltda., como mencionado na seção \"manufacturers\".\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pymupdf4llm pdfplumber pandas requests -q\n",
        "\n",
        "# Download and install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start the ollama server process in the background\n",
        "# Its output will be redirected to a log file\n",
        "server_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    stdout=open(\"ollama_server.log\", \"w\"),\n",
        "    stderr=subprocess.STDOUT\n",
        ")\n",
        "\n",
        "print(\"✅ Ollama server started in the background.\")\n",
        "time.sleep(5) # Give the server a moment to initialize\n",
        "\n",
        "# --- CHANGE MADE HERE ---\n",
        "# Pull the Llama 3 8B model instead of the 3B version.\n",
        "print(\"📥 Pulling the Llama 3 8B model. This may take a few minutes...\")\n",
        "!ollama pull llama3:8b\n",
        "print(\"✅ Model download complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXk19v7Tf_gf",
        "outputId": "cd9e6bde-8b44-40a0-955e-fdb066b71bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "✅ Ollama server started in the background.\n",
            "📥 Pulling the Llama 3 8B model. This may take a few minutes...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "✅ Model download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Enhanced Section-Aware Pharmaceutical Document Parser\n",
        "Improved entity extraction with validation, retry logic, and better prompting\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import re\n",
        "import subprocess\n",
        "import shlex\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from datetime import datetime\n",
        "import pymupdf4llm\n",
        "import pdfplumber\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "class ConfidenceLevel(Enum):\n",
        "    HIGH = \"high\"\n",
        "    MEDIUM = \"medium\"\n",
        "    LOW = \"low\"\n",
        "\n",
        "@dataclass\n",
        "class Entity:\n",
        "    \"\"\"Structured entity with validation\"\"\"\n",
        "    type: str\n",
        "    value: str\n",
        "    confidence: ConfidenceLevel\n",
        "    context: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if not self.value or not self.value.strip():\n",
        "            raise ValueError(\"Entity value cannot be empty\")\n",
        "        if not self.type or not self.type.strip():\n",
        "            raise ValueError(\"Entity type cannot be empty\")\n",
        "\n",
        "        self.value = self.value.strip()\n",
        "        self.type = self.type.strip()\n",
        "\n",
        "class EnhancedSectionAwarePharmaParser:\n",
        "    def __init__(self, model_name: str = \"llama3:8b\", max_retries: int = 2):\n",
        "        \"\"\"\n",
        "        Initialize enhanced parser with retry logic\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.max_retries = max_retries\n",
        "        self.raw_content = \"\"\n",
        "        self.structured_data = {}\n",
        "        self.document_loaded = False\n",
        "\n",
        "        # Enhanced entity types for pharmaceutical documents\n",
        "        self.entity_types = {\n",
        "            'medication_name': {\n",
        "                'description': 'Active ingredient or brand name of the medication',\n",
        "                'examples': ['ezetimiba', 'Atorvastatina', 'Sinvastatina']\n",
        "            },\n",
        "            'dosage_strength': {\n",
        "                'description': 'Concentration or strength of the medication',\n",
        "                'examples': ['10 mg', '20 mg/ml', '500 mcg']\n",
        "            },\n",
        "            'posology': {\n",
        "                'description': 'How and when to take the medication',\n",
        "                'examples': ['1 comprimido ao dia', 'tomar com alimentos', 'antes das refeições']\n",
        "            },\n",
        "            'indication': {\n",
        "                'description': 'Medical condition the medication treats',\n",
        "                'examples': ['hipercolesterolemia', 'redução do colesterol', 'prevenção cardiovascular']\n",
        "            },\n",
        "            'contraindication': {\n",
        "                'description': 'Conditions where medication should not be used',\n",
        "                'examples': ['gravidez', 'insuficiência hepática', 'hipersensibilidade']\n",
        "            },\n",
        "            'mechanism_of_action': {\n",
        "                'description': 'How the medication works in the body',\n",
        "                'examples': ['inibição da absorção do colesterol', 'bloqueio de receptores']\n",
        "            },\n",
        "            'adverse_effect': {\n",
        "                'description': 'Side effects or unwanted reactions',\n",
        "                'examples': ['dor de cabeça', 'náusea', 'dor muscular']\n",
        "            },\n",
        "            'adverse_effect_frequency': {\n",
        "                'description': 'How common side effects are',\n",
        "                'examples': ['muito comum', 'raro', 'muito raro']\n",
        "            },\n",
        "            'drug_interaction': {\n",
        "                'description': 'Other medications that interact with this drug',\n",
        "                'examples': ['warfarina', 'ciclosporina', 'fibratos']\n",
        "            },\n",
        "            'patient_population': {\n",
        "                'description': 'Specific patient groups mentioned',\n",
        "                'examples': ['idosos', 'crianças', 'pacientes com diabetes']\n",
        "            },\n",
        "            'manufacturer': {\n",
        "                'description': 'Company that makes the medication',\n",
        "                'examples': ['Sandoz', 'EMS', 'Medley']\n",
        "            },\n",
        "            'storage_condition': {\n",
        "                'description': 'How to store the medication',\n",
        "                'examples': ['temperatura ambiente', 'proteger da luz', 'geladeira']\n",
        "            },\n",
        "            'presentation': {\n",
        "                'description': 'How the medication is packaged',\n",
        "                'examples': ['comprimidos', 'caixa com 30 unidades', 'frasco de 100ml']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Common pharmaceutical abbreviations\n",
        "        self.pharma_abbreviations = {\n",
        "            'mg', 'ml', 'mcg', 'kg', 'g', 'l', 'dl', 'mmol', 'mol',\n",
        "            'q.s.p', 'c.q.s', 'q.s', 'c.s.p',\n",
        "            'ltda', 'ltd', 'inc', 'corp', 'sa', 'co',\n",
        "            'dr', 'dra', 'prof', 'sr', 'sra',\n",
        "            'etc', 'ex', 'vs', 'e.g', 'i.e',\n",
        "            'cnpj', 'cpf', 'rg', 'crf', 'crm',\n",
        "            'anvisa', 'ms', 'rdc', 'vp', 'vps',\n",
        "            'd.d', 'p.ex', 'n°', 'nº'\n",
        "        }\n",
        "\n",
        "        # Brazilian pharmaceutical document section patterns\n",
        "        self.section_patterns = {\n",
        "            r'^\\s*I+\\)\\s*(.+)$': 'primary_section',\n",
        "            r'^\\s*\\d+\\.\\s*(.+)$': 'numbered_section',\n",
        "            r'^\\s*(IDENTIFICAÇÃO|IDENTIFICACAO)\\s*(DO\\s*MEDICAMENTO)?\\s*$': 'identification',\n",
        "            r'^\\s*(INFORMAÇÕES|INFORMACOES)\\s*(AO\\s*PACIENTE)?\\s*$': 'patient_info',\n",
        "            r'^\\s*(COMPOSIÇÃO|COMPOSICAO)\\s*$': 'composition',\n",
        "            r'^\\s*(APRESENTAÇÕES|APRESENTACOES)\\s*$': 'presentations',\n",
        "            r'^\\s*(INDICAÇÕES|INDICACOES)\\s*$': 'indications',\n",
        "            r'^\\s*(CONTRAINDICAÇÕES|CONTRAINDICACOES)\\s*$': 'contraindications',\n",
        "            r'^\\s*(PRECAUÇÕES|PRECAUCOES)\\s*$': 'precautions',\n",
        "            r'^\\s*(REAÇÕES\\s*ADVERSAS|REACOES\\s*ADVERSAS|EFEITOS\\s*ADVERSOS)\\s*$': 'adverse_effects',\n",
        "            r'^\\s*(INTERAÇÕES|INTERACOES)\\s*(MEDICAMENTOSAS)?\\s*$': 'drug_interactions',\n",
        "            r'^\\s*(POSOLOGIA|DOSAGEM)\\s*$': 'dosage',\n",
        "            r'^\\s*(SUPERDOSAGEM|SUPERDOSE)\\s*$': 'overdose',\n",
        "            r'^\\s*ARMAZENAMENTO\\s*$': 'storage',\n",
        "            r'^\\s*DIZERES\\s*LEGAIS\\s*$': 'legal_info',\n",
        "            r'^\\s*\\d+\\.\\s*(PARA\\s*QUE|O\\s*QUE|COMO|QUANDO|ONDE|QUAIS)\\s*.*\\?\\s*$': 'question_header'\n",
        "        }\n",
        "\n",
        "        self.setup_ollama()\n",
        "\n",
        "    def setup_ollama(self):\n",
        "        \"\"\"Setup Ollama model automatically\"\"\"\n",
        "        print(f\"Setting up Ollama model: {self.model_name}\")\n",
        "        try:\n",
        "            subprocess.run([\"ollama\", \"--version\"], capture_output=True, check=True)\n",
        "            print(\"Ollama CLI found\")\n",
        "        except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "            raise RuntimeError(\"Ollama CLI not found. Please install Ollama first.\")\n",
        "\n",
        "        try:\n",
        "            print(f\"Pulling model {self.model_name}...\")\n",
        "            result = subprocess.run(\n",
        "                [\"ollama\", \"pull\", self.model_name],\n",
        "                capture_output=True, text=True, timeout=300\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(f\"Model {self.model_name} ready\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error with model setup: {e}\")\n",
        "\n",
        "    def call_ollama_raw(self, prompt: str, extra_flags: str = \"\") -> str:\n",
        "        \"\"\"Call ollama with exact prompt\"\"\"\n",
        "        cmd = [\"ollama\", \"run\", self.model_name]\n",
        "        if extra_flags:\n",
        "            cmd += shlex.split(extra_flags)\n",
        "\n",
        "        try:\n",
        "            proc = subprocess.run(\n",
        "                cmd, input=prompt, text=True, capture_output=True, timeout=120\n",
        "            )\n",
        "            return proc.stdout.strip() or proc.stderr.strip()\n",
        "        except subprocess.TimeoutExpired:\n",
        "            raise RuntimeError(\"Ollama call timed out\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error calling ollama: {e}\")\n",
        "\n",
        "    def create_enhanced_prompt(self, sentence_data: Dict) -> str:\n",
        "        \"\"\"Create an enhanced prompt with detailed entity descriptions and examples\"\"\"\n",
        "        sentence = sentence_data['sentence']\n",
        "        section_type = sentence_data['section_type']\n",
        "        section_title = sentence_data['section_title']\n",
        "\n",
        "        # Build entity type documentation\n",
        "        entity_docs = []\n",
        "        for entity_type, info in self.entity_types.items():\n",
        "            examples_str = ', '.join(f'\"{ex}\"' for ex in info['examples'][:2])\n",
        "            entity_docs.append(f'- {entity_type}: {info[\"description\"]} (examples: {examples_str})')\n",
        "\n",
        "        entity_documentation = '\\n'.join(entity_docs)\n",
        "\n",
        "        prompt = f\"\"\"You are analyzing a sentence from a Brazilian pharmaceutical document (bula).\n",
        "\n",
        "SECTION CONTEXT: This sentence is from \"{section_title}\" ({section_type})\n",
        "\n",
        "ENTITY TYPES TO EXTRACT:\n",
        "{entity_documentation}\n",
        "\n",
        "EXTRACTION RULES:\n",
        "1. Only extract entities that are EXPLICITLY mentioned in the sentence\n",
        "2. Do not infer or assume information not directly stated\n",
        "3. Extract the exact text, don't paraphrase\n",
        "4. Set confidence based on clarity: \"high\" for clear/direct mentions, \"medium\" for somewhat clear, \"low\" for uncertain\n",
        "5. If no entities found, return empty entities array\n",
        "\n",
        "SENTENCE TO ANALYZE: \"{sentence}\"\n",
        "\n",
        "RESPOND WITH VALID JSON ONLY (no explanations, no markdown):\n",
        "{{\n",
        "  \"entities\": [\n",
        "    {{\"type\": \"entity_type\", \"value\": \"exact_text_from_sentence\", \"confidence\": \"high|medium|low\"}}\n",
        "  ],\n",
        "  \"section_relevance\": \"high|medium|low\",\n",
        "  \"key_info_found\": true/false\n",
        "}}\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def validate_and_clean_entities(self, entities: List[Dict]) -> List[Entity]:\n",
        "        \"\"\"Validate and clean extracted entities\"\"\"\n",
        "        validated_entities = []\n",
        "\n",
        "        for entity_dict in entities:\n",
        "            try:\n",
        "                # Basic validation\n",
        "                if not isinstance(entity_dict, dict):\n",
        "                    continue\n",
        "\n",
        "                entity_type = entity_dict.get('type', '').strip()\n",
        "                entity_value = entity_dict.get('value', '').strip()\n",
        "                confidence_str = entity_dict.get('confidence', 'medium').strip().lower()\n",
        "\n",
        "                # Skip empty or invalid entities\n",
        "                if not entity_type or not entity_value:\n",
        "                    continue\n",
        "\n",
        "                # Validate entity type\n",
        "                if entity_type not in self.entity_types:\n",
        "                    continue\n",
        "\n",
        "                # Clean and validate confidence\n",
        "                try:\n",
        "                    confidence = ConfidenceLevel(confidence_str)\n",
        "                except ValueError:\n",
        "                    confidence = ConfidenceLevel.MEDIUM\n",
        "\n",
        "                # Additional cleaning\n",
        "                entity_value = self.clean_entity_value(entity_value)\n",
        "                if not entity_value:\n",
        "                    continue\n",
        "\n",
        "                # Create validated entity\n",
        "                entity = Entity(\n",
        "                    type=entity_type,\n",
        "                    value=entity_value,\n",
        "                    confidence=confidence\n",
        "                )\n",
        "\n",
        "                validated_entities.append(entity)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error validating entity {entity_dict}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return validated_entities\n",
        "\n",
        "    def clean_entity_value(self, value: str) -> str:\n",
        "        \"\"\"Clean entity values\"\"\"\n",
        "        if not value:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        cleaned = re.sub(r'\\s+', ' ', value.strip())\n",
        "\n",
        "        # Remove trailing periods (except for abbreviations)\n",
        "        if cleaned.endswith('.') and len(cleaned) > 4:\n",
        "            cleaned = cleaned.rstrip('.')\n",
        "\n",
        "        # Remove quotation marks\n",
        "        cleaned = cleaned.strip('\"\\'')\n",
        "\n",
        "        # Skip very short or meaningless values\n",
        "        if len(cleaned) < 2:\n",
        "            return \"\"\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    def parse_json_response_with_retry(self, response: str) -> Optional[Dict]:\n",
        "        \"\"\"Enhanced JSON parsing with better error handling\"\"\"\n",
        "        if not response or not response.strip():\n",
        "            return None\n",
        "\n",
        "        cleaned = response.strip()\n",
        "\n",
        "        # Remove markdown code blocks\n",
        "        if \"```json\" in cleaned:\n",
        "            start = cleaned.find(\"```json\") + 7\n",
        "            end = cleaned.rfind(\"```\")\n",
        "            if start < end:\n",
        "                cleaned = cleaned[start:end].strip()\n",
        "        elif \"```\" in cleaned:\n",
        "            start = cleaned.find(\"```\") + 3\n",
        "            end = cleaned.rfind(\"```\")\n",
        "            if start < end:\n",
        "                cleaned = cleaned[start:end].strip()\n",
        "\n",
        "        # Find JSON boundaries\n",
        "        json_start = cleaned.find('{')\n",
        "        json_end = cleaned.rfind('}') + 1\n",
        "\n",
        "        if json_start != -1 and json_end > json_start:\n",
        "            cleaned = cleaned[json_start:json_end]\n",
        "\n",
        "        try:\n",
        "            parsed = json.loads(cleaned)\n",
        "\n",
        "            # Validate structure\n",
        "            if not isinstance(parsed, dict):\n",
        "                return None\n",
        "\n",
        "            if 'entities' not in parsed:\n",
        "                return None\n",
        "\n",
        "            if not isinstance(parsed['entities'], list):\n",
        "                return None\n",
        "\n",
        "            return parsed\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            try:\n",
        "                # Try to fix common JSON issues\n",
        "                fixed = re.sub(r',(\\s*[}\\]])', r'\\1', cleaned)  # Remove trailing commas\n",
        "                fixed = re.sub(r'(\\w+):', r'\"\\1\":', fixed)  # Add quotes to keys\n",
        "                fixed = re.sub(r': *([^\",\\[\\{][^,}\\]]*)', r': \"\\1\"', fixed)  # Add quotes to values\n",
        "                return json.loads(fixed)\n",
        "            except:\n",
        "                return None\n",
        "\n",
        "    def analyze_sentence_with_retries(self, sentence_data: Dict, index: int) -> Dict:\n",
        "        \"\"\"Analyze sentence with retry logic for failed extractions\"\"\"\n",
        "\n",
        "        for attempt in range(self.max_retries + 1):\n",
        "            try:\n",
        "                if attempt == 0:\n",
        "                    prompt = self.create_enhanced_prompt(sentence_data)\n",
        "                else:\n",
        "                    # Modified prompt for retry\n",
        "                    original_prompt = self.create_enhanced_prompt(sentence_data)\n",
        "                    prompt = f\"{original_prompt}\\n\\nNOTE: Previous attempt failed. Please ensure your response is valid JSON with the exact structure shown above.\"\n",
        "\n",
        "                response = self.call_ollama_raw(prompt)\n",
        "                result = self.parse_json_response_with_retry(response)\n",
        "\n",
        "                if result and isinstance(result, dict) and 'entities' in result:\n",
        "                    # Validate and clean entities\n",
        "                    raw_entities = result.get('entities', [])\n",
        "                    validated_entities = self.validate_and_clean_entities(raw_entities)\n",
        "\n",
        "                    # Convert back to dict format for consistency\n",
        "                    cleaned_entities = []\n",
        "                    for entity in validated_entities:\n",
        "                        cleaned_entities.append({\n",
        "                            'type': entity.type,\n",
        "                            'value': entity.value,\n",
        "                            'confidence': entity.confidence.value\n",
        "                        })\n",
        "\n",
        "                    # Build final result\n",
        "                    final_result = {\n",
        "                        'entities': cleaned_entities,\n",
        "                        'section_relevance': result.get('section_relevance', 'medium'),\n",
        "                        'key_info_found': len(cleaned_entities) > 0,\n",
        "                        'sentence_index': index,\n",
        "                        'original_sentence': sentence_data.get('sentence', ''),\n",
        "                        'section_type': sentence_data.get('section_type', 'unknown'),\n",
        "                        'section_title': sentence_data.get('section_title', 'Unknown Section'),\n",
        "                        'line_number': sentence_data.get('line_number', 0),\n",
        "                        'extraction_attempts': attempt + 1\n",
        "                    }\n",
        "\n",
        "                    if len(cleaned_entities) > 0:\n",
        "                        print(f\"  Success on attempt {attempt + 1}: {len(cleaned_entities)} entities\")\n",
        "\n",
        "                    return final_result\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == self.max_retries:\n",
        "                    break\n",
        "                continue\n",
        "\n",
        "        # All attempts failed\n",
        "        print(f\"  All {self.max_retries + 1} attempts failed\")\n",
        "        return self._create_empty_result(sentence_data, index, 'extraction_failed_all_attempts')\n",
        "\n",
        "    def extract_pdf_content(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text content from PDF\"\"\"\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                all_text = []\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        all_text.append(page_text)\n",
        "                if all_text:\n",
        "                    return \"\\n\\n\".join(all_text)\n",
        "        except Exception as e:\n",
        "            print(f\"pdfplumber failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            return pymupdf4llm.to_markdown(pdf_path)\n",
        "        except Exception as e:\n",
        "            print(f\"pymupdf4llm failed: {e}\")\n",
        "            raise Exception(\"All extraction methods failed\")\n",
        "\n",
        "    def detect_section_header(self, text: str) -> Tuple[Optional[str], Optional[str]]:\n",
        "        \"\"\"Detect if text is a section header\"\"\"\n",
        "        text_clean = text.strip()\n",
        "\n",
        "        if len(text_clean) < 3:\n",
        "            return None, None\n",
        "\n",
        "        for pattern, section_type in self.section_patterns.items():\n",
        "            match = re.match(pattern, text_clean, re.IGNORECASE)\n",
        "            if match:\n",
        "                if section_type == 'primary_section' or section_type == 'numbered_section':\n",
        "                    section_title = match.group(1).strip()\n",
        "                else:\n",
        "                    section_title = text_clean\n",
        "                return section_type, section_title\n",
        "\n",
        "        if (text_clean.isupper() and\n",
        "            len(text_clean) > 5 and\n",
        "            len(text_clean) < 100 and\n",
        "            not re.search(r'\\d{2,}', text_clean)):\n",
        "            return 'caps_header', text_clean\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def is_likely_abbreviation(self, text: str) -> bool:\n",
        "        \"\"\"Check if text ending with period is likely an abbreviation\"\"\"\n",
        "        if not text or len(text) < 2:\n",
        "            return False\n",
        "\n",
        "        word = text.rstrip('.').lower()\n",
        "\n",
        "        if word in self.pharma_abbreviations:\n",
        "            return True\n",
        "\n",
        "        patterns = [\n",
        "            r'^[a-z]{1,4}$',\n",
        "            r'^[A-Z]{2,6}$',\n",
        "            r'^[A-Z][a-z]{1,3}$',\n",
        "            r'^\\d+[a-z]+$',\n",
        "            r'^[a-z]\\.[a-z]',\n",
        "            r'[0-9]$'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            if re.match(pattern, word):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def smart_sentence_split_with_sections(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Split text into sentences with section awareness\"\"\"\n",
        "        print(\"Splitting text with section tracking...\")\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        current_section_type = 'unknown'\n",
        "        current_section_title = 'Document Start'\n",
        "        sentence_data = []\n",
        "        current_sentence = \"\"\n",
        "\n",
        "        for line_num, line in enumerate(lines):\n",
        "            line = line.strip()\n",
        "\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            section_type, section_title = self.detect_section_header(line)\n",
        "\n",
        "            if section_type and section_title:\n",
        "                if current_sentence.strip():\n",
        "                    sentences = self._split_sentence_safely(current_sentence)\n",
        "                    for sent in sentences:\n",
        "                        if sent.strip() and len(sent.strip()) > 10:\n",
        "                            sentence_data.append({\n",
        "                                'sentence': sent.strip(),\n",
        "                                'section_type': current_section_type,\n",
        "                                'section_title': current_section_title,\n",
        "                                'line_number': line_num,\n",
        "                                'is_header': False\n",
        "                            })\n",
        "                    current_sentence = \"\"\n",
        "\n",
        "                current_section_type = section_type\n",
        "                current_section_title = section_title\n",
        "\n",
        "                sentence_data.append({\n",
        "                    'sentence': line,\n",
        "                    'section_type': section_type,\n",
        "                    'section_title': section_title,\n",
        "                    'line_number': line_num,\n",
        "                    'is_header': True\n",
        "                })\n",
        "\n",
        "                print(f\"Section detected: {section_type} - {section_title}\")\n",
        "\n",
        "            else:\n",
        "                if current_sentence:\n",
        "                    current_sentence += \" \" + line\n",
        "                else:\n",
        "                    current_sentence = line\n",
        "\n",
        "        if current_sentence.strip():\n",
        "            sentences = self._split_sentence_safely(current_sentence)\n",
        "            for sent in sentences:\n",
        "                if sent.strip() and len(sent.strip()) > 10:\n",
        "                    sentence_data.append({\n",
        "                        'sentence': sent.strip(),\n",
        "                        'section_type': current_section_type,\n",
        "                        'section_title': current_section_title,\n",
        "                        'line_number': len(lines),\n",
        "                        'is_header': False\n",
        "                    })\n",
        "\n",
        "        content_sentences = [s for s in sentence_data if not s['is_header']]\n",
        "        print(f\"Found {len(sentence_data)} total items ({len(content_sentences)} content sentences)\")\n",
        "\n",
        "        return content_sentences\n",
        "\n",
        "    def _split_sentence_safely(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into sentences with abbreviation awareness\"\"\"\n",
        "        sentences = []\n",
        "        current_sentence = \"\"\n",
        "\n",
        "        parts = re.split(r'([.!?]+)', text)\n",
        "\n",
        "        i = 0\n",
        "        while i < len(parts):\n",
        "            if i % 2 == 0:\n",
        "                current_sentence += parts[i]\n",
        "            else:\n",
        "                punctuation = parts[i]\n",
        "                current_sentence += punctuation\n",
        "\n",
        "                if '.' in punctuation:\n",
        "                    words = current_sentence.split()\n",
        "                    if words:\n",
        "                        last_word = words[-1]\n",
        "                        if not self.is_likely_abbreviation(last_word):\n",
        "                            if current_sentence.strip():\n",
        "                                sentences.append(current_sentence.strip())\n",
        "                            current_sentence = \"\"\n",
        "                    else:\n",
        "                        if current_sentence.strip():\n",
        "                            sentences.append(current_sentence.strip())\n",
        "                        current_sentence = \"\"\n",
        "                else:\n",
        "                    if current_sentence.strip():\n",
        "                        sentences.append(current_sentence.strip())\n",
        "                    current_sentence = \"\"\n",
        "            i += 1\n",
        "\n",
        "        if current_sentence.strip():\n",
        "            sentences.append(current_sentence.strip())\n",
        "\n",
        "        return [s for s in sentences if s.strip() and len(s.strip()) > 10]\n",
        "\n",
        "    def _create_empty_result(self, sentence_data: Dict, index: int, error_type: str = None) -> Dict:\n",
        "        \"\"\"Create empty result structure\"\"\"\n",
        "        return {\n",
        "            'entities': [],\n",
        "            'section_relevance': 'low',\n",
        "            'key_info_found': False,\n",
        "            'sentence_index': index,\n",
        "            'original_sentence': sentence_data.get('sentence', ''),\n",
        "            'section_type': sentence_data.get('section_type', 'unknown'),\n",
        "            'section_title': sentence_data.get('section_title', 'Unknown Section'),\n",
        "            'line_number': sentence_data.get('line_number', 0),\n",
        "            'error': error_type,\n",
        "            'extraction_attempts': self.max_retries + 1\n",
        "        }\n",
        "\n",
        "    def process_sentences_with_enhanced_extraction(self, sentence_data_list: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Process all sentences with enhanced extraction\"\"\"\n",
        "        print(f\"Analyzing {len(sentence_data_list)} sentences with enhanced extraction...\")\n",
        "\n",
        "        analyses = []\n",
        "        successful_analyses = 0\n",
        "        total_entities = 0\n",
        "\n",
        "        for i, sentence_data in enumerate(sentence_data_list):\n",
        "            sentence = sentence_data.get('sentence', '')[:60]\n",
        "            section = sentence_data.get('section_title', 'Unknown')\n",
        "\n",
        "            print(f\"Processing {i+1}/{len(sentence_data_list)} in [{section}]: {sentence}...\")\n",
        "\n",
        "            analysis = self.analyze_sentence_with_retries(sentence_data, i)\n",
        "            analyses.append(analysis)\n",
        "\n",
        "            if analysis.get('key_info_found') and not analysis.get('error'):\n",
        "                successful_analyses += 1\n",
        "                entity_count = len(analysis.get('entities', []))\n",
        "                total_entities += entity_count\n",
        "\n",
        "            import time\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        print(f\"Completed: {successful_analyses}/{len(sentence_data_list)} sentences with entities\")\n",
        "        print(f\"Total entities extracted: {total_entities}\")\n",
        "\n",
        "        return analyses\n",
        "\n",
        "    def aggregate_entities_by_section(self, analyses: List[Dict]) -> Dict:\n",
        "        \"\"\"Aggregate entities by section with improved processing\"\"\"\n",
        "        print(\"Aggregating entities by section...\")\n",
        "\n",
        "        section_entities = {}\n",
        "        all_entities = []\n",
        "        section_stats = {}\n",
        "\n",
        "        for analysis in analyses:\n",
        "            if not analysis or analysis.get('error'):\n",
        "                continue\n",
        "\n",
        "            section_title = analysis.get('section_title', 'Unknown Section')\n",
        "\n",
        "            if section_title not in section_entities:\n",
        "                section_entities[section_title] = {}\n",
        "                for entity_type in self.entity_types.keys():\n",
        "                    section_entities[section_title][entity_type] = set()\n",
        "                section_stats[section_title] = {'sentences': 0, 'entities': 0}\n",
        "\n",
        "            section_stats[section_title]['sentences'] += 1\n",
        "\n",
        "            entities = analysis.get('entities', [])\n",
        "            if not entities:\n",
        "                continue\n",
        "\n",
        "            section_stats[section_title]['entities'] += len(entities)\n",
        "\n",
        "            for entity in entities:\n",
        "                if not isinstance(entity, dict):\n",
        "                    continue\n",
        "\n",
        "                entity_type = entity.get('type', '').strip()\n",
        "                entity_value = entity.get('value', '').strip()\n",
        "\n",
        "                if not entity_type or not entity_value:\n",
        "                    continue\n",
        "\n",
        "                if entity_type in self.entity_types:\n",
        "                    section_entities[section_title][entity_type].add(entity_value)\n",
        "\n",
        "                    all_entities.append({\n",
        "                        'type': entity_type,\n",
        "                        'value': entity_value,\n",
        "                        'section': section_title,\n",
        "                        'confidence': entity.get('confidence', 'medium'),\n",
        "                        'sentence_index': analysis.get('sentence_index', -1)\n",
        "                    })\n",
        "\n",
        "        # Convert sets to lists for JSON serialization\n",
        "        for section in section_entities:\n",
        "            for entity_type in section_entities[section]:\n",
        "                section_entities[section][entity_type] = list(section_entities[section][entity_type])\n",
        "\n",
        "        result = {\n",
        "            'entities_by_section': section_entities,\n",
        "            'all_entities': all_entities,\n",
        "            'section_statistics': section_stats,\n",
        "            'total_entities': len(all_entities),\n",
        "            'sections_processed': len(section_entities)\n",
        "        }\n",
        "\n",
        "        print(f\"Aggregated {len(all_entities)} entities across {len(section_entities)} sections\")\n",
        "        return result\n",
        "\n",
        "    def process_document(self, pdf_path: str) -> bool:\n",
        "        \"\"\"Main document processing with enhanced extraction\"\"\"\n",
        "        print(f\"Processing document with enhanced extraction: {pdf_path}\")\n",
        "\n",
        "        if not Path(pdf_path).exists():\n",
        "            print(f\"File not found: {pdf_path}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            print(\"Extracting text from PDF...\")\n",
        "            self.raw_content = self.extract_pdf_content(pdf_path)\n",
        "\n",
        "            if not self.raw_content:\n",
        "                print(\"No text content extracted\")\n",
        "                return False\n",
        "\n",
        "            print(f\"Extracted {len(self.raw_content)} characters\")\n",
        "\n",
        "            sentence_data_list = self.smart_sentence_split_with_sections(self.raw_content)\n",
        "            analyses = self.process_sentences_with_enhanced_extraction(sentence_data_list)\n",
        "            aggregated_data = self.aggregate_entities_by_section(analyses)\n",
        "\n",
        "            self.structured_data = {\n",
        "                \"metadata\": {\n",
        "                    \"file_path\": pdf_path,\n",
        "                    \"file_name\": Path(pdf_path).name,\n",
        "                    \"processing_date\": datetime.now().isoformat(),\n",
        "                    \"total_text_length\": len(self.raw_content),\n",
        "                    \"model_used\": self.model_name,\n",
        "                    \"extraction_method\": \"enhanced_section_aware_extraction\",\n",
        "                    \"max_retries\": self.max_retries\n",
        "                },\n",
        "                \"sentence_analyses\": analyses,\n",
        "                \"section_entities\": aggregated_data,\n",
        "                \"processing_statistics\": {\n",
        "                    \"total_sentences\": len(sentence_data_list),\n",
        "                    \"sentences_with_entities\": len([a for a in analyses if a.get('key_info_found')]),\n",
        "                    \"total_entities_found\": aggregated_data.get('total_entities', 0),\n",
        "                    \"sections_identified\": aggregated_data.get('sections_processed', 0),\n",
        "                    \"extraction_success_rate\": len([a for a in analyses if a.get('key_info_found')]) / len(sentence_data_list) * 100 if sentence_data_list else 0\n",
        "                }\n",
        "            }\n",
        "\n",
        "            self.document_loaded = True\n",
        "            print(\"Enhanced document processing completed!\")\n",
        "            self._show_processing_summary()\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing document: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    def _show_processing_summary(self):\n",
        "        \"\"\"Show enhanced processing summary\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ENHANCED PROCESSING SUMMARY\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        metadata = self.structured_data.get(\"metadata\", {})\n",
        "        stats = self.structured_data.get(\"processing_statistics\", {})\n",
        "        section_data = self.structured_data.get(\"section_entities\", {})\n",
        "\n",
        "        print(f\"File: {metadata.get('file_name', 'Unknown')}\")\n",
        "        print(f\"Text length: {metadata.get('total_text_length', 0):,} characters\")\n",
        "        print(f\"Total sentences: {stats.get('total_sentences', 0)}\")\n",
        "        print(f\"Sentences with entities: {stats.get('sentences_with_entities', 0)}\")\n",
        "        print(f\"Total entities found: {stats.get('total_entities_found', 0)}\")\n",
        "        print(f\"Sections identified: {stats.get('sections_identified', 0)}\")\n",
        "        print(f\"Success rate: {stats.get('extraction_success_rate', 0):.1f}%\")\n",
        "\n",
        "        section_stats = section_data.get('section_statistics', {})\n",
        "        if section_stats:\n",
        "            print(f\"\\nEntity Distribution by Section:\")\n",
        "            for section, stat in section_stats.items():\n",
        "                print(f\"   {section}: {stat['entities']} entities from {stat['sentences']} sentences\")\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    def save_results(self, output_path: Optional[str] = None) -> str:\n",
        "        \"\"\"Save enhanced processing results\"\"\"\n",
        "        if not self.document_loaded:\n",
        "            raise Exception(\"No document processed\")\n",
        "\n",
        "        if not output_path:\n",
        "            file_name = self.structured_data[\"metadata\"][\"file_name\"]\n",
        "            pdf_name = Path(file_name).stem\n",
        "            output_path = f\"{pdf_name}_enhanced_analysis.json\"\n",
        "\n",
        "        output_file = Path(output_path)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.structured_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\nResults saved to: {output_file}\")\n",
        "        print(f\"File size: {output_file.stat().st_size:,} bytes\")\n",
        "        return str(output_file)\n",
        "\n",
        "    def query_document(self, question: str) -> str:\n",
        "        \"\"\"Query with enhanced section context\"\"\"\n",
        "        if not self.document_loaded:\n",
        "            return \"No document processed.\"\n",
        "\n",
        "        section_entities = self.structured_data.get(\"section_entities\", {}).get(\"entities_by_section\", {})\n",
        "\n",
        "        context_parts = [\"MEDICATION INFORMATION BY SECTION:\\n\"]\n",
        "\n",
        "        for section, entities in section_entities.items():\n",
        "            if any(entities.values()):\n",
        "                context_parts.append(f\"[{section}]\")\n",
        "                for entity_type, values in entities.items():\n",
        "                    if values:\n",
        "                        # Show more entities for better context\n",
        "                        display_values = values[:5] if len(values) > 5 else values\n",
        "                        context_parts.append(f\"  {entity_type}: {', '.join(display_values)}\")\n",
        "                context_parts.append(\"\")\n",
        "\n",
        "        context = \"\\n\".join(context_parts)\n",
        "\n",
        "        query_prompt = f\"\"\"Answer about this medication based on the detailed section-organized information.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Available Information:\n",
        "{context[:6000]}\n",
        "\n",
        "Provide a comprehensive answer in Portuguese, mentioning the relevant sections and specific details when appropriate. Be precise and cite the information source.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.call_ollama_raw(query_prompt)\n",
        "            return response.strip() if response else \"No response received\"\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "    def get_entity_statistics(self) -> Dict:\n",
        "        \"\"\"Get detailed statistics about extracted entities\"\"\"\n",
        "        if not self.document_loaded:\n",
        "            return {}\n",
        "\n",
        "        all_entities = self.structured_data.get(\"section_entities\", {}).get(\"all_entities\", [])\n",
        "\n",
        "        # Count by type\n",
        "        type_counts = {}\n",
        "        for entity in all_entities:\n",
        "            entity_type = entity.get('type', 'unknown')\n",
        "            type_counts[entity_type] = type_counts.get(entity_type, 0) + 1\n",
        "\n",
        "        # Count by confidence\n",
        "        confidence_counts = {}\n",
        "        for entity in all_entities:\n",
        "            confidence = entity.get('confidence', 'medium')\n",
        "            confidence_counts[confidence] = confidence_counts.get(confidence, 0) + 1\n",
        "\n",
        "        # Count by section\n",
        "        section_counts = {}\n",
        "        for entity in all_entities:\n",
        "            section = entity.get('section', 'unknown')\n",
        "            section_counts[section] = section_counts.get(section, 0) + 1\n",
        "\n",
        "        return {\n",
        "            'total_entities': len(all_entities),\n",
        "            'by_type': type_counts,\n",
        "            'by_confidence': confidence_counts,\n",
        "            'by_section': section_counts\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    \"\"\"Demonstrate enhanced entity extraction\"\"\"\n",
        "    parser = EnhancedSectionAwarePharmaParser(model_name=\"llama3:8b\", max_retries=2)\n",
        "\n",
        "    pdf_path = \"bula_1755192077396.pdf\"\n",
        "\n",
        "    print(\"Enhanced Pharmaceutical Document Parser\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if parser.process_document(pdf_path):\n",
        "        results_file = parser.save_results()\n",
        "\n",
        "        # Show entity statistics\n",
        "        stats = parser.get_entity_statistics()\n",
        "        print(f\"\\nENTITY EXTRACTION STATISTICS:\")\n",
        "        print(f\"Total entities: {stats.get('total_entities', 0)}\")\n",
        "\n",
        "        print(\"\\nBy type:\")\n",
        "        for entity_type, count in sorted(stats.get('by_type', {}).items()):\n",
        "            print(f\"  {entity_type}: {count}\")\n",
        "\n",
        "        print(\"\\nBy confidence:\")\n",
        "        for confidence, count in sorted(stats.get('by_confidence', {}).items()):\n",
        "            print(f\"  {confidence}: {count}\")\n",
        "\n",
        "        print(f\"\\nTesting enhanced querying...\")\n",
        "        test_questions = [\n",
        "            \"Qual é o nome do medicamento e sua concentração?\",\n",
        "            \"Quais são as contraindicações principais?\",\n",
        "            \"Como deve ser administrado e qual a posologia?\",\n",
        "            \"Quais são os efeitos adversos e sua frequência?\",\n",
        "            \"Quem é o fabricante e como armazenar?\",\n",
        "            \"Qual é o mecanismo de ação do medicamento?\",\n",
        "            \"Há interações medicamentosas importantes?\"\n",
        "        ]\n",
        "\n",
        "        for question in test_questions:\n",
        "            print(f\"\\n{question}\")\n",
        "            answer = parser.query_document(question)\n",
        "            print(f\"{answer}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVMIvqEklSGs",
        "outputId": "7afcbecd-e0cd-43f7-b4aa-75e41ce833de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up Ollama model: llama3:8b\n",
            "Ollama CLI found\n",
            "Pulling model llama3:8b...\n",
            "Model llama3:8b ready\n",
            "Enhanced Pharmaceutical Document Parser\n",
            "======================================================================\n",
            "Processing document with enhanced extraction: bula_1755192077396.pdf\n",
            "Extracting text from PDF...\n",
            "Extracted 11936 characters\n",
            "Splitting text with section tracking...\n",
            "Section detected: primary_section - IDENTIFICAÇÃO DO MEDICAMENTO\n",
            "Section detected: presentations - APRESENTAÇÕES\n",
            "Section detected: caps_header - USO ORAL\n",
            "Section detected: caps_header - USO ADULTO E PEDIÁTRICO ACIMA DE 6 ANOS DE IDADE\n",
            "Section detected: composition - COMPOSIÇÃO\n",
            "Section detected: primary_section - INFORMAÇÕES AO PACIENTE\n",
            "Section detected: numbered_section - PARA QUE ESTE MEDICAMENTO É INDICADO?\n",
            "Section detected: numbered_section - COMO ESTE MEDICAMENTO FUNCIONA?\n",
            "Section detected: numbered_section - QUANDO NÃO DEVO USAR ESTE MEDICAMENTO?\n",
            "Section detected: numbered_section - O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\n",
            "Section detected: caps_header - QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).\n",
            "Section detected: numbered_section - ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\n",
            "Section detected: numbered_section - COMO DEVO USAR ESTE MEDICAMENTO?\n",
            "Section detected: numbered_section - O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?\n",
            "Section detected: numbered_section - QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\n",
            "Section detected: numbered_section - O QUE FAZER SE ALGUÉM USAR UMA QUANTIDADE MAIOR DO QUE A INDICADA DESTE\n",
            "Section detected: caps_header - MEDICAMENTO?\n",
            "Section detected: primary_section - DIZERES LEGAIS\n",
            "Section detected: caps_header - VENDA SOB PRESCRIÇÃO MÉDICA\n",
            "Found 99 total items (80 content sentences)\n",
            "Analyzing 80 sentences with enhanced extraction...\n",
            "Processing 1/80 in [Document Start]: ezetimiba Sandoz do Brasil Ind. Farm. Ltda. Bula do Paciente...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 2/80 in [IDENTIFICAÇÃO DO MEDICAMENTO]: ezetimiba Medicamento genérico, Lei nº 9.787, de 1999...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 3/80 in [APRESENTAÇÕES]: ezetimiba comprimido 10 mg. Embalagem contendo 30 ou 60 comp...\n",
            "  Success on attempt 1: 4 entities\n",
            "Processing 4/80 in [COMPOSIÇÃO]: Cada comprimido de 10 mg contém: ezetimiba.....................\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 5/80 in [COMPOSIÇÃO]: 10 mg excipientes q.s.p. ....................\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 6/80 in [COMPOSIÇÃO]: 1 comprimido (lactose monoidratada, hipromelose, croscarmelo...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 7/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: A ezetimiba é indicada para reduzir a quantidade de colester...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 8/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: O colesterol é uma das várias substâncias gordurosas encontr...\n",
            "Processing 9/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: O colesterol total é composto principalmente de colesterol L...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 10/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: Eventualmente, essas placas podem causar estreitamento das a...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 11/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: Esse bloqueio ao fluxo sanguíneo pode causar ataque cardíaco...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 12/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: O colesterol HDL, por sua vez, é frequentemente chamado de “...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 13/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: Outra forma de gordura no sangue que pode aumentar o risco d...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 14/80 in [PARA QUE ESTE MEDICAMENTO É INDICADO?]: Se você tem sitosterolemia, seu médico prescreveu ezetimiba ...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 15/80 in [COMO ESTE MEDICAMENTO FUNCIONA?]: A ezetimiba age ao reduzir a absorção do colesterol no intes...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 16/80 in [COMO ESTE MEDICAMENTO FUNCIONA?]: Portanto, ezetimiba aumenta o efeito redutor do colesterol d...\n",
            "  Success on attempt 1: 3 entities\n",
            "Processing 17/80 in [COMO ESTE MEDICAMENTO FUNCIONA?]: O colesterol alto pode ser tratado de duas formas principais...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 18/80 in [COMO ESTE MEDICAMENTO FUNCIONA?]: Seu médico prescreveu ezetimiba para ajudar a reduzir o seu ...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 19/80 in [QUANDO NÃO DEVO USAR ESTE MEDICAMENTO?]: Pacientes com hipersensibilidade (alérgicos) a ezetimiba ou ...\n",
            "  Success on attempt 2: 2 entities\n",
            "Processing 20/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: É importante que continue a tomar ezetimiba diariamente conf...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 21/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Mesmo tomando medicamentos para tratar o colesterol alto, é ...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 22/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Além disso, é importante que você conheça seus níveis atuais...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 23/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Gravidez e Amamentação: se estiver grávida ou planeja engrav...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 24/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Se estiver amamentando, ezetimiba pode passar do seu leite p...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 25/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Este medicamento não deve ser utilizado por mulheres grávida...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 26/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Idosos: não há precauções especiais....\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 27/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: ezetimiba – VP06 Uso pediátrico: A ezetimiba não é recomenda...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 28/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: Dirigir ou Operar Máquinas: foram relatados efeitos adversos...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 29/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: As respostas individuais a ezetimiba podem variar (veja o it...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 30/80 in [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?]: QUAIS OS MALES...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 31/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Problemas Clínicos ou Alergias: informe ao seu médico quaisq...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 32/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Interações Medicamentosas: Você deve sempre informar seu méd...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 33/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Informe ao seu médico ou cirurgião-dentista se você está faz...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 34/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Não use medicamento sem o conhecimento do seu médico....\n",
            "Processing 35/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Pode ser perigoso para a sua saúde....\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 36/80 in [QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).]: Esse medicamento não é recomendado para crianças com menos d...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 37/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Mantenha em temperatura ambiente (temperatura entre 15 e 30º...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 38/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Proteger da umidade....\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 39/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Número de lote e datas de fabricação e validade: vide embala...\n",
            "Processing 40/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Não use medicamento com o prazo de validade vencido....\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 41/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Guarde-o em sua embalagem original....\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 42/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Aparência: A ezetimiba é um comprimido oval branco a quase b...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 43/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Caso ele esteja no prazo de validade e você observe alguma m...\n",
            "Processing 44/80 in [ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?]: Todo medicamento deve ser mantido fora do alcance das crianç...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 45/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Adultos e crianças acima de 6 anos de idade: Tome um comprim...\n",
            "  Success on attempt 1: 4 entities\n",
            "Processing 46/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Seu médico pode ter falado para você tomar ezetimiba com out...\n",
            "  Success on attempt 1: 3 entities\n",
            "Processing 47/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Se seu médico prescreveu ezetimiba com colestiramina (um seq...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 48/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: A ezetimiba deve ser tomada conforme seu médico orientou....\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 49/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Continue a tomar outros medicamentos redutores de colesterol...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 50/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Siga a orientação de seu médico, respeitando sempre os horár...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 51/80 in [COMO DEVO USAR ESTE MEDICAMENTO?]: Não interrompa o tratamento sem o conhecimento do seu médico...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 52/80 in [O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?]: Tente tomar ezetimiba conforme prescrito....\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 53/80 in [O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?]: Entretanto, se esquecer de tomar uma dose, reinicie o esquem...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 54/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Nos estudos clínicos, a ezetimiba foi em geral bem tolerada....\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 55/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Os efeitos adversos geralmente foram leves e semelhantes em ...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 56/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Em geral, os efeitos adversos não provocaram a interrupção d...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 57/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Quando ezetimiba foi usada isoladamente, foram relatados os ...\n",
            "  Success on attempt 1: 4 entities\n",
            "Processing 58/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Incomuns: elevações nos exames de sangue da função hepática ...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 59/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Além disso, quando tomado com uma estatina, foram relatados ...\n",
            "  Success on attempt 1: 3 entities\n",
            "Processing 60/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Incomuns: sensação de formigamento; boca seca; coceira; erup...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 61/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Ao ser utilizado com fenofibratos, o seguinte efeito adverso...\n",
            "  Success on attempt 2: 2 entities\n",
            "Processing 62/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Além disso, foram relatados os seguintes efeitos adversos no...\n",
            "  Success on attempt 1: 14 entities\n",
            "Processing 63/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: ezetimiba – VP06 Procure seu médico imediatamente se sentir ...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 64/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Embora raramente, esses problemas musculares podem ser grave...\n",
            "  Success on attempt 1: 2 entities\n",
            "Processing 65/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Se ezetimiba foi prescrita para ser tomado com uma estatina,...\n",
            "  Success on attempt 2: 1 entities\n",
            "Processing 66/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Informe ao seu médico, cirurgião-dentista ou farmacêutico o ...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 67/80 in [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?]: Informe também à empresa através do seu serviço de atendimen...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 68/80 in [MEDICAMENTO?]: Tome ezetimiba apenas conforme prescrito....\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 69/80 in [MEDICAMENTO?]: Se tomar mais ezetimiba do que o prescrito, entre em contato...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 70/80 in [MEDICAMENTO?]: Em caso de uso de grande quantidade deste medicamento, procu...\n",
            "Processing 71/80 in [MEDICAMENTO?]: Ligue para 0800 722 6001, se você precisar de mais orientaçõ...\n",
            "Processing 72/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: Reg. M.S.: 1.0047....\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 73/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: 0538 Farm. Resp.: Cláudia Larissa S. Montanher CRF-PR nº 17....\n",
            "Processing 74/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: 379 Esta bula foi aprovada pela Anvisa em 21/07/2023....\n",
            "Processing 75/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: Fabricado por: Lek Pharmaceuticals, d.d. Ljubljana - Eslovên...\n",
            "Processing 76/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: 647/0001-16 Indústria Brasileira Ou Fabricado por: Lek Pharm...\n",
            "Processing 77/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: 647/0001-16 Indústria Brasileira ezetimiba – VP06 Histórico ...\n",
            "Processing 78/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: RDC 60/12 -6....\n",
            "Processing 79/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: Como devo usar este medicamento?...\n",
            "  Success on attempt 1: 1 entities\n",
            "Processing 80/80 in [VENDA SOB PRESCRIÇÃO MÉDICA]: Notificação de Inclusão de local de Alteração de 10 mg 15/04...\n",
            "Completed: 67/80 sentences with entities\n",
            "Total entities extracted: 115\n",
            "Aggregating entities by section...\n",
            "Aggregated 115 entities across 15 sections\n",
            "Enhanced document processing completed!\n",
            "\n",
            "======================================================================\n",
            "ENHANCED PROCESSING SUMMARY\n",
            "======================================================================\n",
            "File: bula_1755192077396.pdf\n",
            "Text length: 11,936 characters\n",
            "Total sentences: 80\n",
            "Sentences with entities: 67\n",
            "Total entities found: 115\n",
            "Sections identified: 15\n",
            "Success rate: 83.8%\n",
            "\n",
            "Entity Distribution by Section:\n",
            "   Document Start: 2 entities from 1 sentences\n",
            "   IDENTIFICAÇÃO DO MEDICAMENTO: 1 entities from 1 sentences\n",
            "   APRESENTAÇÕES: 4 entities from 1 sentences\n",
            "   COMPOSIÇÃO: 3 entities from 3 sentences\n",
            "   PARA QUE ESTE MEDICAMENTO É INDICADO?: 11 entities from 8 sentences\n",
            "   COMO ESTE MEDICAMENTO FUNCIONA?: 9 entities from 4 sentences\n",
            "   QUANDO NÃO DEVO USAR ESTE MEDICAMENTO?: 2 entities from 1 sentences\n",
            "   O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?: 13 entities from 11 sentences\n",
            "   QUE ESTE MEDICAMENTO PODE ME CAUSAR? ”).: 5 entities from 6 sentences\n",
            "   ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?: 7 entities from 8 sentences\n",
            "   COMO DEVO USAR ESTE MEDICAMENTO?: 13 entities from 7 sentences\n",
            "   O QUE DEVO FAZER QUANDO EU ME ESQUECER DE USAR ESTE MEDICAMENTO?: 3 entities from 2 sentences\n",
            "   QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?: 38 entities from 14 sentences\n",
            "   MEDICAMENTO?: 2 entities from 4 sentences\n",
            "   VENDA SOB PRESCRIÇÃO MÉDICA: 2 entities from 9 sentences\n",
            "======================================================================\n",
            "\n",
            "Results saved to: bula_1755192077396_enhanced_analysis.json\n",
            "File size: 94,254 bytes\n",
            "\n",
            "ENTITY EXTRACTION STATISTICS:\n",
            "Total entities: 115\n",
            "\n",
            "By type:\n",
            "  adverse_effect: 29\n",
            "  adverse_effect_frequency: 3\n",
            "  contraindication: 10\n",
            "  dosage_strength: 4\n",
            "  drug_interaction: 1\n",
            "  indication: 8\n",
            "  manufacturer: 3\n",
            "  mechanism_of_action: 5\n",
            "  medication_name: 35\n",
            "  patient_population: 3\n",
            "  posology: 6\n",
            "  presentation: 5\n",
            "  storage_condition: 3\n",
            "\n",
            "By confidence:\n",
            "  high: 105\n",
            "  low: 2\n",
            "  medium: 8\n",
            "\n",
            "Testing enhanced querying...\n",
            "\n",
            "Qual é o nome do medicamento e sua concentração?\n",
            "O nome do medicamento é Ezetimiba, e sua concentração é de 10 mg.\n",
            "\n",
            "Essa informação está presente na seção \"APRESENTAÇÕES\" do documento, onde é mencionado que a apresentação do medicamento é um comprimido oval branco a quase branco, em sua embalagem original, com 30 ou 60 comprimidos.\n",
            "\n",
            "É importante ressaltar que o Ezetimiba é indicado para reduzir a quantidade de colesterol e de triglicérides no sangue, doenças do coração e reduzir os níveis de esteroides vegetais em seu sangue. Além disso, ajuda a evitar o depósito de mau colesterol nas artérias e protege contra doenças do coração, estreitamento das artérias e reduzir ou bloquear o fluxo sanguíneo para órgãos vitais (seção \"PARA QUE ESTE MEDICAMENTO É INDICADO?\").\n",
            "\n",
            "No entanto, é importante lembrar que o Ezetimiba não deve ser usado por pessoas com hipersensibilidade (alérgicos) a ele ou a qualquer um de seus componentes (seção \"QUANDO NÃO DEVO USAR ESTE MEDICAMENTO?\"). Além disso, há contraindicação para uso em pacientes com menos de 6 anos de idade, estiver amamentando, mulheres grávidas e gravidez e amamentação (seção \"O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?\").\n",
            "\n",
            "Quanto aos efeitos adversos, o Ezetimiba pode causar inchaço da face, lábios, língua e/ou garganta que possa causar dificuldade para respirar ou engolir, depressão, cansaço ou fraqueza incomuns, alterações em alguns exames laboratoriais de sangue e dor de cabeça (seção \"QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?\").\n",
            "\n",
            "Para armazenamento do medicamento, é necessário mantê-lo à temperatura ambiente (entre 15 e 30°C), protegendo-o da umidade e mantendo-o fora do alcance das crianças (seção \"ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\").\n",
            "--------------------------------------------------\n",
            "\n",
            "Quais são as contraindicações principais?\n",
            "As contraindicações principais para o uso do medicamento ezetimiba são:\n",
            "\n",
            "* Hipersensibilidade (alérgicos) a ezetimiba ou a qualquer um de seus componentes, conforme informado na seção \"Quando não deve usar este medicamento?\".\n",
            "* Idade: não deve ser utilizado em crianças com menos de 6 anos de idade, mulheres grávidas ou amamentando, e homens que estejam fazendo uso da mesma forma (referência à seção \"O que devo saber antes de usar este medicamento?\").\n",
            "* Doença hepática ou problemas hepáticos [relativos ao fígado], conforme informado na seção \"Que este medicamento pode me causar?\".\n",
            "\n",
            "É importante lembrar que a contraindicação principal é a hipersensibilidade, o que é mencionado explicitamente na seção \"Quando não deve usar este medicamento?\". Além disso, é fundamental consultar o médico ou farmacêutico antes de iniciar o tratamento com esse medicamento.\n",
            "--------------------------------------------------\n",
            "\n",
            "Como deve ser administrado e qual a posologia?\n",
            "Para administrar o medicamento ezetimiba corretamente, é importante seguir as orientações do seu médico e considerar os seguintes pontos:\n",
            "\n",
            "* Como deve ser administrado: Deve-se tomar um comprimido diariamente, em qualquer horário do dia (Seção \"Como devo usar este medicamento?\" ).\n",
            "* Posologia: Não interrompa o tratamento, continue a tomar o medicamento por tempo indefinido, a menos que seu médico mande você parar. Siga a orientação de seu médico, respeitando sempre os horários, as doses e a duração do tratamento (Seção \"Como devo usar este medicamento?\" ).\n",
            "* Em caso de esquecimento: Tome o comprimido assim que lembrar, desde que não seja próximo da hora de dormir. Se for próximo da hora de dormir, espere até a próxima manhã para tomar o medicamento (Seção \"O que devo fazer quando eu me esquecer de usar este medicamento?\" ).\n",
            "* Contraindicação: Não use ezetimiba se você tem hipersensibilidade à substância ou a qualquer um de seus componentes (Seção \"Quando não devo usar este medicamento?\" ).\n",
            "\n",
            "Além disso, é importante lembrar que o medicamento pode causar efeitos adversos como inchaço da face, lábios, língua e/ou garganta que possa causar dificuldade para respirar ou engolir, depressão, cansaço ou fraqueza incomuns, alterações em alguns exames laboratoriais de sangue, dor de cabeça (Seção \"Quais os males que este medicamento pode me causar?\" ).\n",
            "\n",
            "É fundamental seguir as orientações do seu médico e armazenar o medicamento corretamente, protegendo-o da umidade e mantendo-o à temperatura ambiente (temperatura entre 15 e 30°C) (Seção \"Onde, como e por quanto tempo posso guardar este medicamento?\" ).\n",
            "--------------------------------------------------\n",
            "\n",
            "Quais são os efeitos adversos e sua frequência?\n",
            "De acordo com as informações disponíveis sobre o medicamento Ezetimiba, os efeitos adversos podem incluir:\n",
            "\n",
            "* Inchaço da face, lábios, língua e/ou garganta que possa causar dificuldade para respirar ou engolir (frequência: incomum) [Seção \"Quais os males que este medicamento pode me causar?\" - Adverse Effect]\n",
            "* Depressão, cansaço ou fraqueza incomuns (frequência: incomum) [Seção \"Quais os males que este medicamento pode me causar?\" - Adverse Effect]\n",
            "* Dor de cabeça (frequência: comum) [Seção \"Quais os males que este medicamento pode me causar?\" - Adverse Effect]\n",
            "\n",
            "Além disso, é importante mencionar que o Ezetimiba também pode aumentar o risco de ataque cardíaco ou derrame, como indicado na seção \"Apresentações\" [Seção \"Adverse Effect\"].\n",
            "\n",
            "É fundamental seguir as orientações do seu médico e respeitar os horários, doses e duração do tratamento para minimizar os efeitos adversos. Se você esquecer de usar o medicamento, procure orientação do farmacêutico ou do seu médico [Seção \"O que devo fazer quando eu me esquecer de usar este medicamento?\" - Posology].\n",
            "\n",
            "Fonte: Informações disponíveis sobre o medicamento Ezetimiba fornecidas pela Sandoz do Brasil Ind. Farm. Ltda.\n",
            "--------------------------------------------------\n",
            "\n",
            "Quem é o fabricante e como armazenar?\n",
            "O fabricante do medicamento é Sandoz do Brasil Ind. Farm. Ltda, conforme mencionado na seção \"Document Start\".\n",
            "\n",
            "Para armazenar este medicamento, você deve segui-la as instruções da seção \"ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\" : temperatura ambiente (temperatura entre 15 e 30ºC), proteger da umidade, e manter fora do alcance das crianças. Além disso, é importante verificar a validade do medicamento antes de usá-lo, pois um prazo de validade vencido pode ser perigoso para sua saúde.\n",
            "\n",
            "Fonte: Documento de Informação sobre o Medicamento, seção \"ONDE, COMO E POR QUANTO TEMPO POSSO GUARDAR ESTE MEDICAMENTO?\".\n",
            "--------------------------------------------------\n",
            "\n",
            "Qual é o mecanismo de ação do medicamento?\n",
            "O mecanismo de ação do medicamento ezetimiba é ajudar a evitar o depósito de mau colesterol nas artérias e proteger contra doenças do coração, estreitamento das artérias, reduzir ou bloquear o fluxo sanguíneo para órgãos vitais. Isso é descrito na seção \"PARA QUE ESTE MEDICAMENTO É INDICADO?\" da documentação.\n",
            "\n",
            "Além disso, a seção \"COMO ESTE MEDICAMENTO FUNCIONA?\" informa que o medicamento reduz a absorção do colesterol no intestino delgado. Isso também é mencionado na seção \"MECANISMO DE AÇÃO\" como uma forma de ajuda a evitar o depósito de mau colesterol nas artérias.\n",
            "\n",
            "Portanto, o mecanismo de ação do medicamento ezetimiba consiste em reduzir a absorção do colesterol no intestino delgado e ajudar a evitar o depósito de mau colesterol nas artérias. Fonte: Documentação do Medicamento - Seções \"APRESENTAÇÕES\", \"COMPOSIÇÃO\", \"PARA QUE ESTE MEDICAMENTO É INDICADO?\" e \"COMO ESTE MEDICAMENTO FUNCIONA?\".\n",
            "--------------------------------------------------\n",
            "\n",
            "Há interações medicamentosas importantes?\n",
            "Sim, há interações medicamentosas importantes relacionadas ao uso do medicamento ezetimiba.\n",
            "\n",
            "De acordo com o [QUAIS OS MALES QUE ESTE MEDICAMENTO PODE ME CAUSAR?] seção, esse medicamento pode causar efeitos adversos como inchaço da face, lábios, língua e/ou garganta que possa causar dificuldade para respirar ou engolir, depressão, cansaço ou fraqueza incomuns, alterações em alguns exames laboratoriais de sangue e dor de cabeça (página 8).\n",
            "\n",
            "Além disso, o [COMO DEVO USAR ESTE MEDICAMENTO?] seção destaca a importância de não interromper o tratamento e de seguir a orientação do médico para uso correto do medicamento (página 9). \n",
            "\n",
            "É fundamental lembrar que esse medicamento está contraindicado em pacientes com hipersensibilidade (alérgicos) a ezetimiba ou a qualquer um de seus componentes, além disso, é contraindicado para menores de 6 anos de idade, gestantes e amamentadoras (página 5).\n",
            "\n",
            "Ainda, é importante mencionar que o [O QUE DEVO SABER ANTES DE USAR ESTE MEDICAMENTO?] seção destaca a necessidade de informar ao seu médico sobre doença hepática ou problemas hepáticos antes de iniciar o tratamento com esse medicamento (página 6).\n",
            "\n",
            "Por fim, é fundamental lembrar que esse medicamento deve ser armazenado em temperatura ambiente (temperatura entre 15 e 30ºC), protegido da umidade e fora do alcance das crianças, e que o prazo de validade vencido é contraindicado para uso (página 10).\n",
            "\n",
            "Fonte: Documento de Informação sobre Medicamento - Ezetimiba.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Bula Entity-Relation-Value Extractor for Graph Database\n",
        "Processes JSON output from bula analysis and extracts structured data for Neo4j\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Any, Set\n",
        "from dataclasses import dataclass, asdict\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "@dataclass\n",
        "class Entity:\n",
        "    \"\"\"Represents a graph entity\"\"\"\n",
        "    id: str\n",
        "    type: str\n",
        "    properties: Dict[str, Any]\n",
        "    source_sentence: str = \"\"\n",
        "    source_section: str = \"\"\n",
        "    confidence: str = \"medium\"\n",
        "\n",
        "@dataclass\n",
        "class Relation:\n",
        "    \"\"\"Represents a graph relationship\"\"\"\n",
        "    source_entity_id: str\n",
        "    target_entity_id: str\n",
        "    relation_type: str\n",
        "    properties: Dict[str, Any] = None\n",
        "    confidence: str = \"medium\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.properties is None:\n",
        "            self.properties = {}\n",
        "\n",
        "class BulaGraphExtractor:\n",
        "    \"\"\"Extracts entities, relations, and values from bula JSON for graph database\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.entities: Dict[str, Entity] = {}\n",
        "        self.relations: List[Relation] = []\n",
        "        self.medication_id = None\n",
        "\n",
        "        # Entity type mappings for graph nodes\n",
        "        self.entity_type_mapping = {\n",
        "            'medication_name': 'Medication',\n",
        "            'dosage_strength': 'DosageStrength',\n",
        "            'posology': 'Posology',\n",
        "            'indication': 'Indication',\n",
        "            'contraindication': 'Contraindication',\n",
        "            'mechanism_of_action': 'MechanismOfAction',\n",
        "            'adverse_effect': 'AdverseEffect',\n",
        "            'adverse_effect_frequency': 'AdverseEffectFrequency',\n",
        "            'drug_interaction': 'DrugInteraction',\n",
        "            'patient_population': 'PatientPopulation',\n",
        "            'manufacturer': 'Manufacturer',\n",
        "            'storage_condition': 'StorageCondition',\n",
        "            'presentation': 'Presentation'\n",
        "        }\n",
        "\n",
        "        # Relation type mappings\n",
        "        self.relation_mappings = {\n",
        "            ('Medication', 'DosageStrength'): 'HAS_STRENGTH',\n",
        "            ('Medication', 'Posology'): 'HAS_POSOLOGY',\n",
        "            ('Medication', 'Indication'): 'INDICATED_FOR',\n",
        "            ('Medication', 'Contraindication'): 'CONTRAINDICATED_FOR',\n",
        "            ('Medication', 'MechanismOfAction'): 'WORKS_BY',\n",
        "            ('Medication', 'AdverseEffect'): 'MAY_CAUSE',\n",
        "            ('Medication', 'DrugInteraction'): 'INTERACTS_WITH',\n",
        "            ('Medication', 'PatientPopulation'): 'SUITABLE_FOR',\n",
        "            ('Medication', 'Manufacturer'): 'MANUFACTURED_BY',\n",
        "            ('Medication', 'StorageCondition'): 'REQUIRES_STORAGE',\n",
        "            ('Medication', 'Presentation'): 'AVAILABLE_AS',\n",
        "            ('AdverseEffect', 'AdverseEffectFrequency'): 'HAS_FREQUENCY',\n",
        "            ('Indication', 'PatientPopulation'): 'AFFECTS_POPULATION',\n",
        "            ('Posology', 'PatientPopulation'): 'FOR_POPULATION'\n",
        "        }\n",
        "\n",
        "    def generate_entity_id(self, entity_type: str, value: str) -> str:\n",
        "        \"\"\"Generate a unique entity ID\"\"\"\n",
        "        # Create deterministic ID based on type and value\n",
        "        clean_value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
        "        clean_value = re.sub(r'\\s+', '_', clean_value.strip())\n",
        "        return f\"{entity_type.lower()}_{clean_value}_{str(uuid.uuid4())[:8]}\"\n",
        "\n",
        "    def clean_entity_value(self, value: str) -> str:\n",
        "        \"\"\"Clean and normalize entity values\"\"\"\n",
        "        if not value or not isinstance(value, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Remove extra whitespace and normalize\n",
        "        cleaned = re.sub(r'\\s+', ' ', value.strip())\n",
        "\n",
        "        # Remove quotes\n",
        "        cleaned = cleaned.strip('\"\\'')\n",
        "\n",
        "        # Remove trailing periods for non-abbreviations\n",
        "        if cleaned.endswith('.') and len(cleaned) > 4 and not re.match(r'.*\\b[A-Z]{2,4}\\.?$', cleaned):\n",
        "            cleaned = cleaned.rstrip('.')\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    def extract_medication_entity(self, all_entities: List[Dict]) -> str:\n",
        "        \"\"\"Extract and create the main medication entity\"\"\"\n",
        "        medication_names = [e for e in all_entities if e.get('type') == 'medication_name']\n",
        "\n",
        "        if not medication_names:\n",
        "            # Create a default medication entity\n",
        "            med_id = self.generate_entity_id('medication', 'unknown_medication')\n",
        "            self.entities[med_id] = Entity(\n",
        "                id=med_id,\n",
        "                type='Medication',\n",
        "                properties={'name': 'Unknown Medication'},\n",
        "                source_sentence=\"Extracted from document\",\n",
        "                source_section=\"Document\"\n",
        "            )\n",
        "            return med_id\n",
        "\n",
        "        # Use the first medication name found\n",
        "        med_entity = medication_names[0]\n",
        "        med_name = self.clean_entity_value(med_entity.get('value', 'Unknown'))\n",
        "        med_id = self.generate_entity_id('medication', med_name)\n",
        "\n",
        "        self.entities[med_id] = Entity(\n",
        "            id=med_id,\n",
        "            type='Medication',\n",
        "            properties={\n",
        "                'name': med_name,\n",
        "                'confidence': med_entity.get('confidence', 'medium')\n",
        "            },\n",
        "            source_sentence=med_entity.get('source_sentence', ''),\n",
        "            source_section=med_entity.get('section', '')\n",
        "        )\n",
        "\n",
        "        return med_id\n",
        "\n",
        "    def extract_entities_from_json(self, json_data: Dict) -> None:\n",
        "        \"\"\"Extract entities from processed JSON data\"\"\"\n",
        "        section_entities = json_data.get('section_entities', {})\n",
        "        all_entities = section_entities.get('all_entities', [])\n",
        "\n",
        "        if not all_entities:\n",
        "            print(\"No entities found in JSON data\")\n",
        "            return\n",
        "\n",
        "        # First, create the main medication entity\n",
        "        self.medication_id = self.extract_medication_entity(all_entities)\n",
        "        print(f\"Created medication entity: {self.medication_id}\")\n",
        "\n",
        "        # Track created entities to avoid duplicates\n",
        "        created_entities = set()\n",
        "\n",
        "        # Process all other entities\n",
        "        for entity_data in all_entities:\n",
        "            entity_type = entity_data.get('type', '').strip()\n",
        "            entity_value = self.clean_entity_value(entity_data.get('value', ''))\n",
        "\n",
        "            if not entity_type or not entity_value:\n",
        "                continue\n",
        "\n",
        "            # Skip medication names as we already handled them\n",
        "            if entity_type == 'medication_name':\n",
        "                continue\n",
        "\n",
        "            # Map entity type\n",
        "            graph_entity_type = self.entity_type_mapping.get(entity_type, entity_type.title())\n",
        "\n",
        "            # Create unique identifier for this entity\n",
        "            entity_key = f\"{graph_entity_type}:{entity_value}\"\n",
        "            if entity_key in created_entities:\n",
        "                continue\n",
        "\n",
        "            created_entities.add(entity_key)\n",
        "\n",
        "            # Generate entity ID\n",
        "            entity_id = self.generate_entity_id(graph_entity_type, entity_value)\n",
        "\n",
        "            # Create entity\n",
        "            entity = Entity(\n",
        "                id=entity_id,\n",
        "                type=graph_entity_type,\n",
        "                properties={\n",
        "                    'value': entity_value,\n",
        "                    'confidence': entity_data.get('confidence', 'medium'),\n",
        "                    'original_type': entity_type\n",
        "                },\n",
        "                source_sentence=entity_data.get('source_sentence', ''),\n",
        "                source_section=entity_data.get('section', ''),\n",
        "                confidence=entity_data.get('confidence', 'medium')\n",
        "            )\n",
        "\n",
        "            self.entities[entity_id] = entity\n",
        "\n",
        "            # Create relation to medication\n",
        "            relation_type = self.relation_mappings.get(\n",
        "                ('Medication', graph_entity_type),\n",
        "                'RELATED_TO'\n",
        "            )\n",
        "\n",
        "            relation = Relation(\n",
        "                source_entity_id=self.medication_id,\n",
        "                target_entity_id=entity_id,\n",
        "                relation_type=relation_type,\n",
        "                properties={\n",
        "                    'extracted_from_section': entity_data.get('section', ''),\n",
        "                    'confidence': entity_data.get('confidence', 'medium')\n",
        "                }\n",
        "            )\n",
        "\n",
        "            self.relations.append(relation)\n",
        "\n",
        "    def create_inter_entity_relations(self) -> None:\n",
        "        \"\"\"Create relations between non-medication entities where logical\"\"\"\n",
        "        entity_list = list(self.entities.values())\n",
        "\n",
        "        for i, entity1 in enumerate(entity_list):\n",
        "            for entity2 in entity_list[i+1:]:\n",
        "                # Skip self-relations and medication relations (already handled)\n",
        "                if entity1.type == 'Medication' or entity2.type == 'Medication':\n",
        "                    continue\n",
        "\n",
        "                # Create specific inter-entity relations\n",
        "                relation_type = None\n",
        "\n",
        "                # Adverse effects can have frequencies\n",
        "                if (entity1.type == 'AdverseEffect' and entity2.type == 'AdverseEffectFrequency'):\n",
        "                    if self._are_related_by_context(entity1, entity2):\n",
        "                        relation_type = 'HAS_FREQUENCY'\n",
        "                elif (entity2.type == 'AdverseEffect' and entity1.type == 'AdverseEffectFrequency'):\n",
        "                    if self._are_related_by_context(entity1, entity2):\n",
        "                        relation_type = 'HAS_FREQUENCY'\n",
        "                        entity1, entity2 = entity2, entity1  # Swap to maintain direction\n",
        "\n",
        "                # Posology can be specific to patient populations\n",
        "                elif (entity1.type == 'Posology' and entity2.type == 'PatientPopulation'):\n",
        "                    if self._are_related_by_context(entity1, entity2):\n",
        "                        relation_type = 'FOR_POPULATION'\n",
        "                elif (entity2.type == 'Posology' and entity1.type == 'PatientPopulation'):\n",
        "                    if self._are_related_by_context(entity1, entity2):\n",
        "                        relation_type = 'FOR_POPULATION'\n",
        "                        entity1, entity2 = entity2, entity1\n",
        "\n",
        "                # Create relation if we found a logical connection\n",
        "                if relation_type:\n",
        "                    relation = Relation(\n",
        "                        source_entity_id=entity1.id,\n",
        "                        target_entity_id=entity2.id,\n",
        "                        relation_type=relation_type,\n",
        "                        properties={\n",
        "                            'inferred': True,\n",
        "                            'confidence': 'medium'\n",
        "                        }\n",
        "                    )\n",
        "                    self.relations.append(relation)\n",
        "\n",
        "    def _are_related_by_context(self, entity1: Entity, entity2: Entity) -> bool:\n",
        "        \"\"\"Check if two entities are related by appearing in similar contexts\"\"\"\n",
        "        # Simple heuristic: same section or similar source sentences\n",
        "        if entity1.source_section == entity2.source_section:\n",
        "            return True\n",
        "\n",
        "        # Check if values appear in each other's source sentences\n",
        "        if (entity1.properties.get('value', '').lower() in entity2.source_sentence.lower() or\n",
        "            entity2.properties.get('value', '').lower() in entity1.source_sentence.lower()):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def extract_from_json_file(self, json_file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Main extraction method from JSON file\"\"\"\n",
        "        print(f\"Extracting graph data from: {json_file_path}\")\n",
        "\n",
        "        # Load JSON data\n",
        "        try:\n",
        "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "                json_data = json.load(f)\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to load JSON file: {e}\")\n",
        "\n",
        "        # Extract entities and relations\n",
        "        self.extract_entities_from_json(json_data)\n",
        "        self.create_inter_entity_relations()\n",
        "\n",
        "        # Prepare output data\n",
        "        graph_data = {\n",
        "            'metadata': {\n",
        "                'source_file': json_file_path,\n",
        "                'extraction_date': datetime.now().isoformat(),\n",
        "                'total_entities': len(self.entities),\n",
        "                'total_relations': len(self.relations),\n",
        "                'medication_id': self.medication_id\n",
        "            },\n",
        "            'entities': [asdict(entity) for entity in self.entities.values()],\n",
        "            'relations': [asdict(relation) for relation in self.relations],\n",
        "            'statistics': self._generate_statistics()\n",
        "        }\n",
        "\n",
        "        return graph_data\n",
        "\n",
        "    def _generate_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate statistics about extracted data\"\"\"\n",
        "        entity_type_counts = {}\n",
        "        relation_type_counts = {}\n",
        "        confidence_distribution = {}\n",
        "\n",
        "        # Count entity types\n",
        "        for entity in self.entities.values():\n",
        "            entity_type_counts[entity.type] = entity_type_counts.get(entity.type, 0) + 1\n",
        "            confidence_distribution[entity.confidence] = confidence_distribution.get(entity.confidence, 0) + 1\n",
        "\n",
        "        # Count relation types\n",
        "        for relation in self.relations:\n",
        "            relation_type_counts[relation.relation_type] = relation_type_counts.get(relation.relation_type, 0) + 1\n",
        "\n",
        "        return {\n",
        "            'entity_types': entity_type_counts,\n",
        "            'relation_types': relation_type_counts,\n",
        "            'confidence_distribution': confidence_distribution\n",
        "        }\n",
        "\n",
        "    def save_graph_data(self, graph_data: Dict[str, Any], output_path: str = None) -> str:\n",
        "        \"\"\"Save extracted graph data to JSON file\"\"\"\n",
        "        if not output_path:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_path = f\"graph_data_{timestamp}.json\"\n",
        "\n",
        "        try:\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(graph_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"Graph data saved to: {output_path}\")\n",
        "            return output_path\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to save graph data: {e}\")\n",
        "\n",
        "    def generate_neo4j_queries(self, graph_data: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"Generate Neo4j Cypher queries for data insertion\"\"\"\n",
        "        queries = []\n",
        "\n",
        "        # Create unique constraints\n",
        "        entity_types = set(entity['type'] for entity in graph_data['entities'])\n",
        "        for entity_type in entity_types:\n",
        "            queries.append(f\"CREATE CONSTRAINT {entity_type.lower()}_id IF NOT EXISTS FOR (n:{entity_type}) REQUIRE n.id IS UNIQUE;\")\n",
        "\n",
        "        # Create entities\n",
        "        for entity in graph_data['entities']:\n",
        "            properties = entity['properties'].copy()\n",
        "            properties.update({\n",
        "                'source_sentence': entity.get('source_sentence', ''),\n",
        "                'source_section': entity.get('source_section', ''),\n",
        "                'confidence': entity.get('confidence', 'medium'),\n",
        "                'created_at': 'datetime()'\n",
        "            })\n",
        "\n",
        "            # Build property string\n",
        "            prop_parts = []\n",
        "            for key, value in properties.items():\n",
        "                if key == 'created_at':\n",
        "                    prop_parts.append(f\"{key}: {value}\")\n",
        "                else:\n",
        "                    escaped_value = str(value).replace(\"'\", \"\\\\'\")\n",
        "                    prop_parts.append(f\"{key}: '{escaped_value}'\")\n",
        "\n",
        "            prop_string = ', '.join(prop_parts)\n",
        "\n",
        "            query = f\"MERGE (n:{entity['type']} {{id: '{entity['id']}'}}) SET n += {{{prop_string}}};\"\n",
        "            queries.append(query)\n",
        "\n",
        "        # Create relations\n",
        "        for relation in graph_data['relations']:\n",
        "            properties = relation.get('properties', {}).copy()\n",
        "            properties['created_at'] = 'datetime()'\n",
        "            properties['confidence'] = relation.get('confidence', 'medium')\n",
        "\n",
        "            # Build property string for relation\n",
        "            prop_parts = []\n",
        "            for key, value in properties.items():\n",
        "                if key == 'created_at':\n",
        "                    prop_parts.append(f\"{key}: {value}\")\n",
        "                else:\n",
        "                    escaped_value = str(value).replace(\"'\", \"\\\\'\")\n",
        "                    prop_parts.append(f\"{key}: '{escaped_value}'\")\n",
        "\n",
        "            prop_string = ', '.join(prop_parts) if prop_parts else ''\n",
        "            rel_props = f\" {{{prop_string}}}\" if prop_string else \"\"\n",
        "\n",
        "            query = f\"\"\"MATCH (a {{id: '{relation['source_entity_id']}'}}), (b {{id: '{relation['target_entity_id']}'}})\n",
        "CREATE (a)-[:{relation['relation_type']}{rel_props}]->(b);\"\"\"\n",
        "            queries.append(query)\n",
        "\n",
        "        return queries\n",
        "\n",
        "    def print_summary(self, graph_data: Dict[str, Any]) -> None:\n",
        "        \"\"\"Print extraction summary\"\"\"\n",
        "        metadata = graph_data['metadata']\n",
        "        stats = graph_data['statistics']\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"BULA GRAPH EXTRACTION SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Source file: {metadata['source_file']}\")\n",
        "        print(f\"Extraction date: {metadata['extraction_date']}\")\n",
        "        print(f\"Total entities: {metadata['total_entities']}\")\n",
        "        print(f\"Total relations: {metadata['total_relations']}\")\n",
        "        print(f\"Main medication ID: {metadata['medication_id']}\")\n",
        "\n",
        "        print(f\"\\nEntity Types:\")\n",
        "        for entity_type, count in stats['entity_types'].items():\n",
        "            print(f\"  {entity_type}: {count}\")\n",
        "\n",
        "        print(f\"\\nRelation Types:\")\n",
        "        for relation_type, count in stats['relation_types'].items():\n",
        "            print(f\"  {relation_type}: {count}\")\n",
        "\n",
        "        print(f\"\\nConfidence Distribution:\")\n",
        "        for confidence, count in stats['confidence_distribution'].items():\n",
        "            print(f\"  {confidence}: {count}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    import sys\n",
        "\n",
        "    if len(sys.argv) != 2:\n",
        "        print(\"Usage: python bula_graph_extractor.py <json_file_path>\")\n",
        "        print(\"Example: python bula_graph_extractor.py bula_enhanced_analysis.json\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    json_file_path = sys.argv[1]\n",
        "\n",
        "    if not Path(json_file_path).exists():\n",
        "        print(f\"Error: File '{json_file_path}' not found\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        # Initialize extractor\n",
        "        extractor = BulaGraphExtractor()\n",
        "\n",
        "        # Extract graph data\n",
        "        graph_data = extractor.extract_from_json_file(json_file_path)\n",
        "\n",
        "        # Save graph data\n",
        "        output_file = extractor.save_graph_data(graph_data)\n",
        "\n",
        "        # Generate Neo4j queries\n",
        "        queries = extractor.generate_neo4j_queries(graph_data)\n",
        "        queries_file = output_file.replace('.json', '_neo4j_queries.cypher')\n",
        "        with open(queries_file, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(queries))\n",
        "        print(f\"Neo4j queries saved to: {queries_file}\")\n",
        "\n",
        "        # Print summary\n",
        "        extractor.print_summary(graph_data)\n",
        "\n",
        "        print(f\"\\nFiles created:\")\n",
        "        print(f\"  - Graph data: {output_file}\")\n",
        "        print(f\"  - Neo4j queries: {queries_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ds0OQkcppE4L",
        "outputId": "ed8fb5c2-d879-4e35-b58f-8e3be8f6a4e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: python bula_graph_extractor.py <json_file_path>\n",
            "Example: python bula_graph_extractor.py bula_enhanced_analysis.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "1",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}